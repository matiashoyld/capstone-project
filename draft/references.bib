@misc{kapoor2025prediction,
      title={Prediction of Item Difficulty for Reading Comprehension Items by Creation of Annotated Item Repository}, 
      author={Radhika Kapoor and Sang T. Truong and Nick Haber and Maria Araceli Ruiz-Primo and Benjamin W. Domingue},
      year={2025},
      eprint={2502.20663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.20663}, 
}

@inproceedings{benedetto2021transformers,
    title = {On the application of Transformers for estimating the difficulty of Multiple-Choice Questions from text},
    author = {Benedetto, Luca  and
      Aradelli, Giovanni  and
      Cremonesi, Paolo  and
      Cappelli, Andrea  and
      Giussani, Andrea  and
      Turrin, Roberto},
    editor = {Burstein, Jill  and
      Horbach, Andrea  and
      Kochmar, Ekaterina  and
      Laarmann-Quante, Ronja  and
      Leacock, Claudia  and
      Madnani, Nitin  and
      Pil{\'a}n, Ildik{\'o}  and
      Yannakoudakis, Helen  and
      Zesch, Torsten},
    booktitle = {Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications},
    month = apr,
    year = {2021},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2021.bea-1.16/},
    pages = {147--157},
    abstract = {Classical approaches to question calibration are either subjective or require newly created questions to be deployed before being calibrated. Recent works explored the possibility of estimating question difficulty from text, but did not experiment with the most recent NLP models, in particular Transformers. In this paper, we compare the performance of previous literature with Transformer models experimenting on a public and a private dataset. Our experimental results show that Transformers are capable of outperforming previously proposed models. Moreover, if an additional corpus of related documents is available, Transformers can leverage that information to further improve calibration accuracy. We characterized the dependence of the model performance on some properties of the questions, showing that it performs best on questions ending with a question mark and Multiple-Choice Questions (MCQs) with one correct choice.}
}

@inproceedings{gombert2024predicting,
    title = "Predicting Item Difficulty and Item Response Time with Scalar-mixed Transformer Encoder Models and Rational Network Regression Heads",
    author = "Gombert, Sebastian  and
      Menzel, Lukas  and
      Di Mitri, Daniele  and
      Drachsler, Hendrik",
    editor = {Kochmar, Ekaterina  and
      Bexte, Marie  and
      Burstein, Jill  and
      Horbach, Andrea  and
      Laarmann-Quante, Ronja  and
      Tack, Ana{\"i}s  and
      Yaneva, Victoria  and
      Yuan, Zheng},
    booktitle = "Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.bea-1.40/",
    pages = "483--492",
    abstract = "This paper describes a contribution to the BEA 2024 Shared Task on Automated Prediction of Item Difficulty and Response Time. The participants in this shared task are to develop models for predicting the difficulty and response time of multiple-choice items in the medical field. These items were taken from the United States Medical Licensing Examination{\textregistered} (USMLE{\textregistered}), a high-stakes medical exam. For this purpose, we evaluated multiple BERT-like pre-trained transformer encoder models, which we combined with Scalar Mixing and two custom 2-layer classification heads using learnable Rational Activations as an activation function, each for predicting one of the two variables of interest in a multi-task setup. Our best models placed first out of 43 for predicting item difficulty and fifth out of 34 for predicting Item Response Time."
}

@inproceedings{rudner2010implementing,
  author = {Rudner, Lawrence},
  year = {2010},
  month = {01},
  pages = {151-165},
  title = {Implementing the Graduate Management Admission Test Computerized Adaptive Test},
  isbn = {978-0-387-85459-5},
  doi = {10.1007/978-0-387-85461-8_8}
}

@article{impara1998teachers,
  author = {Impara, James C. and Plake, Barbara S.},
  title = {Teachers' Ability to Estimate Item Difficulty: A Test of the Assumptions in the Angoff Standard Setting Method},
  journal = {Journal of Educational Measurement},
  volume = {35},
  number = {1},
  pages = {69-81},
  doi = {https://doi.org/10.1111/j.1745-3984.1998.tb00528.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-3984.1998.tb00528.x},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-3984.1998.tb00528.x},
  abstract = {The Angoff (1971) standard setting method requires expert panelists to (a) conceptualize candidates who possess the qualifications of interest (e.g., the minimally qualified) and (b) estimate actual item performance for these candidates. Past and current research (Bejar, 1983; Shepard, 1994) suggests that estimating item performance is difficult for panelists. If panelists cannot perform this task, the validity of the standard based on these estimates is in question. This study tested the ability of 26 classroom teachers to estimate item performance for two groups of their students on a locally developed district-wide science test. Teachers were more accurate in estimating the performance of the total group than of the "borderline group," but in neither case was their accuracy level high. Implications of this finding for the validity of item performance estimates by panelists using the Angoff standard setting method are discussed.},
  year = {1998}
}

@article{rasch1960,
  title={Probabilistic models for some intelligence and attainment tests},
  author={Rasch, Georg},
  journal={Danish Institute for Educational Research},
  year={1960}
}

@article{alkhuzaey2023text,
  title={Text-based Question Difficulty Prediction: A Systematic Review of Automatic Approaches},
  author={AlKhuzaey, S. and Grasso, F. and Payne, T. R. and others},
  journal={International Journal of Artificial Intelligence in Education},
  volume={34},
  pages={862--914},
  year={2024},
  doi={10.1007/s40593-023-00362-1}
}

@article{choi2020predicting,
  author = {Inn-Chull Choi and Youngsun Moon},
  title = {Predicting the Difficulty of EFL Tests Based on Corpus Linguistic Features and Expert Judgment},
  journal = {Language Assessment Quarterly},
  volume = {17},
  number = {1},
  pages = {18--42},
  year = {2020},
  publisher = {Routledge},
  doi = {10.1080/15434303.2019.1674315},
  URL = {https://doi.org/10.1080/15434303.2019.1674315},
  eprint = {https://doi.org/10.1080/15434303.2019.1674315},
  abstract = { This study examines the relationships among various major factors that may affect the difficulty level of language tests in an attempt to enhance the robustness of item difficulty estimation, which constitutes a crucial factor ensuring the equivalency of high-stakes tests. The observed difficulties of the reading and listening sections of two EFL tests were compared using corpus linguistic features and expert judgments, i.e., native and nonnative speakers' perceived difficulty of the test items. The research findings are as follows: Some corpus features and the predicted difficulties demonstrated a moderate to high correlation with the test sections' observed difficulty. The native and nonnative speakers' predicted difficulties significantly explained the observed difficulty of the test sections, where the nonnative speakers' predicted difficulty explained a similar variance. When entered separately, the corpus features showed a stronger explanatory power than the predicted difficulties. The corpus features and predicted difficulty together accounted for the largest variance, which was more than half of the variance of the test sections. The current study suggests that corpus features and expert judgment capture different aspects of item difficulty and future research in this area needs to consider how these two can be combined for robust item difficulty estimation. }
}

@inproceedings{yaneva2019predicting,
  title = {Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam},
  author = {Ha, Le An  and
    Yaneva, Victoria  and
    Baldwin, Peter  and
    Mee, Janet},
  editor = {Yannakoudakis, Helen  and
    Kochmar, Ekaterina  and
    Leacock, Claudia  and
    Madnani, Nitin  and
    Pil{\'a}n, Ildik{\'o}  and
    Zesch, Torsten},
  booktitle = {Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  month = {8},
  year = {2019},
  address = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/W19-4402/},
  doi = {10.18653/v1/W19-4402},
  pages = {11--20},
  abstract = {Predicting the construct-relevant difficulty of Multiple-Choice Questions (MCQs) has the potential to reduce cost while maintaining the quality of high-stakes exams. In this paper, we propose a method for estimating the difficulty of MCQs from a high-stakes medical exam, where all questions were deliberately written to a common reading level. To accomplish this, we extract a large number of linguistic features and embedding types, as well as features quantifying the difficulty of the items for an automatic question-answering system. The results show that the proposed approach outperforms various baselines with a statistically significant difference. Best results were achieved when using the full feature set, where embeddings had the highest predictive power, followed by linguistic features. An ablation study of the various types of linguistic features suggested that information from all levels of linguistic processing contributes to predicting item difficulty, with features related to semantic ambiguity and the psycholinguistic properties of words having a slightly higher importance. Owing to its generic nature, the presented approach has the potential to generalize over other exams containing MCQs.}
}

@inproceedings{benedetto2020framework,
  title={Introducing a framework to assess newly created questions with Natural Language Processing}, 
  author={Luca Benedetto and Andrea Cappelli and Roberto Turrin and Paolo Cremonesi},
  year={2020},
  eprint={2004.13530},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2004.13530}, 
}

@article{bulut2023evaluating,
  author = {Hatice Cigdem Bulut and Okan Bulut and Serkan Arikan},
  title = {Evaluating group differences in online reading comprehension: The impact of item properties},
  journal = {International Journal of Testing},
  volume = {23},
  number = {1},
  pages = {10--33},
  year = {2023},
  publisher = {Routledge},
  doi = {10.1080/15305058.2022.2044821},
  URL = {https://doi.org/10.1080/15305058.2022.2044821},
  eprint = {https://doi.org/10.1080/15305058.2022.2044821},
  abstract = { This study examined group differences in online reading comprehension (ORC) using student data from the 2016 administration of the Progress in International Reading Literacy Study (ePIRLS). An explanatory item response modeling approach was used to explore the effects of item properties (i.e., item format, text complexity, and cognitive complexity), student characteristics (i.e., gender and language groups), and their interactions on dichotomous and polytomous item responses. The results showed that female students outperformed male students in ORC tasks and that the achievement difference between female and male students appeared to change text complexity increases. Similarly, the cognitive complexity of the items seems to play a significant role in explaining the gender gap in ORC performance. Students who never (or sometimes) spoke the test language at home particularly struggled with answering ORC tasks. The achievement gap between students who always (or almost always) spoke the test language at home and those who never (or sometimes) spoke the test language at home was larger for constructed-response items and items with higher cognitive complexity. Overall, the findings suggest that item properties could help understand performance differences between gender and language groups in ORC assessments. }
}

@article{hickendorff2013,
  author = {Marian Hickendorff},
  title = {The Language Factor in Elementary Mathematics Assessments: Computational Skills and Applied Problem Solving in a Multidimensional IRT Framework},
  journal = {Applied Measurement in Education},
  volume = {26},
  number = {4},
  pages = {253--278},
  year = {2013},
  publisher = {Routledge},
  doi = {10.1080/08957347.2013.824451},
  URL = { https://doi.org/10.1080/08957347.2013.824451  },
  eprint = {https://doi.org/10.1080/08957347.2013.824451  },
  abstract = { The results of an exploratory study into measurement of elementary mathematics ability are presented. The focus is on the abilities involved in solving standard computation problems on the one hand and problems presented in a realistic context on the other. The objectives were to assess to what extent these abilities are shared or distinct, and the extent to which students' language level plays a differential role in these abilities. Data from a sample of over 2,000 students from first, second, and third grade in the Netherlands were analyzed in a multidimensional item response theory (IRT) framework. The latent correlation between the two ability dimensions (computational skills and applied mathematics problem solving) ranged from .81 in grade 1 to .87 in grade 3, indicating that the ability dimensions are highly correlated but still distinct. Moreover, students' language level had differential effects on the two mathematical abilities: Effects were larger on applied problem solving than on computational skills. The implications of these findings for measurement practices in the field of elementary mathematics are discussed. }
}

@misc{benedetto2020r2de,
  title={R2DE: a NLP approach to estimating IRT parameters of newly generated questions}, 
  author={Luca Benedetto and Andrea Cappelli and Roberto Turrin and Paolo Cremonesi},
  year={2020},
  eprint={2001.07569},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2001.07569}, 
}

@article{lawrence2022reading,
  doi = {https://doi.org/10.1002/rrq.434},
  author = {Lawrence, Joshua F. and Knoph, Rebecca and McIlraith, Autumn and Kulesz, Paulina A. and Francis, David J.},
  title = {Reading Comprehension and Academic Vocabulary: Exploring Relations of Item Features and Reading Proficiency},
  journal = {Reading Research Quarterly},
  volume = {57},
  number = {2},
  pages = {669-690},
  keywords = {Vocabulary, Comprehension, Methodological perspectives, Academic Literacy / Literacies, Vocabulary - Selecting, Vocabulary - Word structure, 3-Early adolescence},
  doi = {https://doi.org/10.1002/rrq.434},
  url = {https://ila.onlinelibrary.wiley.com/doi/abs/10.1002/rrq.434},
  eprint = {https://ila.onlinelibrary.wiley.com/doi/pdf/10.1002/rrq.434},
  abstract = {Abstract General academic words are those which are typically learned through exposure to school texts and occur across disciplines. We examined academic vocabulary assessment data from a group of English-speaking middle school students (N = 1,747). We tested how word frequency, complexity, proximity, polysemy, and diversity related to students' knowledge of target words across ability levels. Our results affirm the strong relation between vocabulary and reading at the individual level. Strong readers were more likely to know the meanings of words than struggling readers were, regardless of the features of the academic words tested. Words with more meanings were easier for all students, on average. The relation between word frequency and item difficulty was stronger among better readers, whereas the relation between word complexity and item difficulty was stronger among less proficient readers. Our examination of academic words' characteristics and how these characteristics relate to word difficulty across reading performance has implications for instruction.},
  year = {2022}
}

@inproceedings{kurdi2016experimental,
  author = {Kurdi, Ghader and Parsia, Bijan and Sattler, Uli},
  year = {2017},
  month = {02},
  pages = {24-39},
  title = {An Experimental Evaluation of Automatically Generated Multiple Choice Questions from Ontologies},
  isbn = {978-3-319-54626-1},
  doi = {10.1007/978-3-319-54627-8_3}
}

@inproceedings{yaneva2018automatic,
  title = {Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept Embeddings and Information Retrieval},
  author = {Ha, Le An  and
    Yaneva, Victoria},
  editor = {Tetreault, Joel  and
    Burstein, Jill  and
    Kochmar, Ekaterina  and
    Leacock, Claudia  and
    Yannakoudakis, Helen},
  booktitle = {Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications},
  month = jun,
  year = {2018},
  address = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/W18-0548/},
  doi = {10.18653/v1/W18-0548},
  pages = {389--398},
  abstract = {Developing plausible distractors (wrong answer options) when writing multiple-choice questions has been described as one of the most challenging and time-consuming parts of the item-writing process. In this paper we propose a fully automatic method for generating distractor suggestions for multiple-choice questions used in high-stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with information retrieval methods. We frame the evaluation as a prediction task where we aim to "predict" the human-produced distractors used in large sets of medical questions, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item-writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human-produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts.}
}

@article{toyama2021makes,
  doi = {https://doi.org/10.1002/rrq.440},
  author = {Toyama, Yukie},
  title = {What Makes Reading Difficult? An Investigation of the Contributions of Passage, Task, and Reader Characteristics on Comprehension Performance},
  journal = {Reading Research Quarterly},
  volume = {56},
  number = {4},
  pages = {633-642},
  keywords = {Comprehension, Assessment, Methodological perspectives, Cognitive, Cognitive Processes, Comprehension (General), Text Features, 2-Childhood, 3-Early adolescence, 4-Adolescence},
  doi = {https://doi.org/10.1002/rrq.440},
  url = {https://ila.onlinelibrary.wiley.com/doi/abs/10.1002/rrq.440},
  eprint = {https://ila.onlinelibrary.wiley.com/doi/pdf/10.1002/rrq.440},
  abstract = {ABSTRACT In this study, I investigated the simultaneous effects of the reader, the text, and the task factors, and their interactions, on reading comprehension, using explanatory item response models. Analyses of a large data set from a commercially available online assessment system with a wide range of readers (n = 10,547) and passages (n = 48) uncovered factors that contribute to reading challenge in complex ways. Among the passage features, sentence length, word frequency, syntactic simplicity, and temporality were found to significantly affect comprehension difficulty. More importantly, these textual features were moderated by student general vocabulary and task type. In general, high-vocabulary readers benefited more from traditional textual affordances (e.g., shorter sentences, familiar words, simpler grammatical constructions) than low-vocabulary readers, especially when asked to recall localized information without accessing the passage. However, a reverse effect was found with temporality: Passages with more time markers helped low-vocabulary readers, whereas low-temporality passages helped high-vocabulary readers. Ultimately, understanding these complex interactions, as highlighted in the RAND Reading Study Group's heuristic model, will be key in supporting students with their comprehension development.},
  year = {2021}
}

@inproceedings{huang2017question,
  doi = {10.5555/3298239.3298437},
  author = {Huang, Zhenya and Liu, Qi and Chen, Enhong and Zhao, Hongke and Gao, Mingyong and Wei, Si and Su, Yu and Hu, Guoping},
  title = {Question difficulty prediction for READING problems in standard tests},
  year = {2017},
  publisher = {AAAI Press},
  abstract = {Standard tests aim to evaluate the performance of examinees using different tests with consistent difficulties. Thus, a critical demand is to predict the difficulty of each test question before the test is conducted. Existing studies are usually based on the judgments of education experts (e.g., teachers), which may be subjective and labor intensive. In this paper, we propose a novel Test-aware Attention-based Convolutional Neural Network (TACNN) framework to automatically solve this Question Difficulty Prediction (QDP) task for READING problems (a typical problem style in English tests) in standard tests. Specifically, given the abundant historical test logs and text materials of questions, we first design a CNN-based architecture to extract sentence representations for the questions. Then, we utilize an attention strategy to qualify the difficulty contribution of each sentence to questions. Considering the incomparability of question difficulties in different tests, we propose a test-dependent pairwise strategy for training TACNN and generating the difficulty prediction value. Extensive experiments on a real-world dataset not only show the effectiveness of TACNN, but also give interpretable insights to track the attention information for questions.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages = {1352–1359},
  numpages = {8},
  location = {San Francisco, California, USA},
  series = {AAAI'17}
}

@inproceedings{qiu2019question,
  author = {Qiu, Zhaopeng and Wu, Xian and Fan, Wei},
  year = {2019},
  month = {11},
  pages = {139-148},
  title = {Question Difficulty Prediction for Multiple Choice Problems in Medical Exams},
  isbn = {978-1-4503-6976-3},
  doi = {10.1145/3357384.3358013}
}

@inproceedings{xue2020predicting,
  title = {Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning},
  author = {Xue, Kang  and
    Yaneva, Victoria  and
    Runyon, Christopher  and
    Baldwin, Peter},
  editor = {Burstein, Jill  and
    Kochmar, Ekaterina  and
    Leacock, Claudia  and
    Madnani, Nitin  and
    Pil{\'a}n, Ildik{\'o}  and
    Yannakoudakis, Helen  and
    Zesch, Torsten},
  booktitle = {Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  month = jul,
  year = {2020},
  address = {Seattle, WA, USA {\textrightarrow} Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2020.bea-1.20/},
  doi = {10.18653/v1/2020.bea-1.20},
  pages = {193--197},
  abstract = {This paper investigates whether transfer learning can improve the prediction of the difficulty and response time parameters for 18,000 multiple-choice questions from a high-stakes medical exam. The type the signal that best predicts difficulty and response time is also explored, both in terms of representation abstraction and item component used as input (e.g., whole item, answer options only, etc.). The results indicate that, for our sample, transfer learning can improve the prediction of item difficulty when response time is used as an auxiliary task but not the other way around. In addition, difficulty was best predicted using signal from the item stem (the description of the clinical case), while all parts of the item were important for predicting the response time.}
}

@mastersthesis{aradelli2020transformers,
  title={Transformers for Question Difficulty Estimation from Text},
  author={Aradelli, Giovanni},
  school={Politecnico di Milano},
  year={2020},
  month={12},
  day={15},
  url={https://hdl.handle.net/10589/170785}
}

@article{laverghetta2023generating,
  title = {Generating Better Items for Cognitive Assessments Using Large Language Models},
  author = {Laverghetta Jr., Antonio  and
    Licato, John},
  editor = {Kochmar, Ekaterina  and
    Burstein, Jill  and
    Horbach, Andrea  and
    Laarmann-Quante, Ronja  and
    Madnani, Nitin  and
    Tack, Ana{\"i}s  and
    Yaneva, Victoria  and
    Yuan, Zheng  and
    Zesch, Torsten},
  booktitle = {Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
  month = jul,
  year = {2023},
  address = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2023.bea-1.34/},
  doi = {10.18653/v1/2023.bea-1.34},
  pages = {414--428},
  abstract = {Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items with minimal human intervention. Researchers have explored using large language models (LLMs) to generate new items with equivalent psychometric properties to human-written ones. But can LLMs generate items with improved psychometric properties, even when existing items have poor validity evidence? We investigate this using items from a natural language inference (NLI) dataset. We develop a novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence. Collectively, our results demonstrate the potential of employing LLMs to ease the item development process and suggest that the careful use of prompting may allow for iterative improvement of item quality.}
}

@article{liu2023improving,
  title={Improving Large Language Model Fine-tuning for Solving Math Problems},
  author={Yixin Liu and Avi Singh and C. Daniel Freeman and John D. Co-Reyes and Peter J. Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.10047},
  url={https://api.semanticscholar.org/CorpusID:264146593}
}

@article{didolkar2024metacognitive,
  title={Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving},
  author={Aniket Didolkar and Anirudh Goyal and Nan Rosemary Ke and Siyuan Guo and Michal Valko and Timothy Lillicrap and Danilo Jimenez Rezende and Yoshua Bengio and Michael Curtis Mozer and Sanjeev Arora},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.12205},
  url={https://api.semanticscholar.org/CorpusID:269921384}
}

@inbook{scaria2024automated,
   title={Automated Educational Question Generation at Different Bloom’s Skill Levels Using Large Language Models: Strategies and Evaluation},
   ISBN={9783031642999},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-031-64299-9_12},
   DOI={10.1007/978-3-031-64299-9_12},
   booktitle={Artificial Intelligence in Education},
   publisher={Springer Nature Switzerland},
   author={Scaria, Nicy and Dharani Chenna, Suma and Subramani, Deepak},
   year={2024},
   pages={165–179} }

@inproceedings{sadihin2024proposalml,
  title={[Proposal-{ML}] Mining Misconception in Mathematics},
  author={Bryan Constantine Sadihin and Hector Rodriguez Rodriguez and Matteo Jiahao Chen},
  booktitle={Submitted to Tsinghua University Course: Advanced Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=CE85qdNSlp},
  note={under review}
}