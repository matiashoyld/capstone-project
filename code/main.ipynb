{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zapien Difficulty‑Prediction Pipeline\n",
        "\n",
        "This notebook implements an end-to-end pipeline for predicting question difficulty in educational assessments using Item Response Theory (IRT) and neural networks.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The pipeline follows these main steps:\n",
        "1. Calculate empirical difficulty of questions using 1-PL IRT modeling\n",
        "2. Extract features from question text and multiple-choice options\n",
        "3. Generate text embeddings for questions and options\n",
        "4. Train a neural network to predict student performance\n",
        "5. Evaluate the model on hold-out data\n",
        "6. Compare predicted difficulty with empirical difficulty estimates\n",
        "\n",
        "All orchestration lives **inside this notebook**; module files expose only\n",
        "small helpers. By scrolling top‑to‑bottom you will see every data\n",
        "transformation step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Load libraries and modules\n",
        "\n",
        "The pipeline uses several custom modules for different stages of processing:\n",
        "- `irt`: Item Response Theory modeling for calculating question difficulty\n",
        "- `features`: Feature extraction from question text and options\n",
        "- `embeddings`: Text embedding generation using pretrained models\n",
        "- `utils`: Helper functions for text formatting and preprocessing\n",
        "- `modeling_data`: Dataset preparation for neural network training\n",
        "- `neural_net`: Neural network architecture and training functions\n",
        "- `evaluation`: Model evaluation and metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard Python libraries\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- External pipeline modules ---\n",
        "# IRT difficulty estimation\n",
        "from modules.irt import estimate_irt_2pl_params\n",
        "\n",
        "# Feature extraction\n",
        "from modules.features import extract_text_features, calculate_option_features\n",
        "from modules.embeddings import generate_text_embeddings\n",
        "from modules.utils import format_question_text\n",
        "\n",
        "# Data preparation and model training\n",
        "from modules.modeling_data import (\n",
        "    stratified_question_split_3way,\n",
        "    prepare_nn_datasets,\n",
        "    save_preprocessors,\n",
        ")\n",
        "from modules.neural_net import create_nn_model, train_nn_model, save_nn_model\n",
        "\n",
        "# Evaluation\n",
        "from modules.evaluation import (\n",
        "    make_dataset,\n",
        "    evaluate_model,\n",
        "    dump_json,\n",
        "    prediction_matrix,\n",
        "    difficulty_from_predictions,\n",
        "    compare_difficulty,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Define constants\n",
        "\n",
        "These constants define column names, model parameters, and configuration settings used throughout the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Core column names for student response data ---\n",
        "USER_ID_COL = \"user_id\"              # Identifier for students/users\n",
        "QUESTION_ID_COL = \"question_id\"      # Identifier for questions\n",
        "CORRECTNESS_COL = \"is_correct\"       # Binary indicator of response correctness\n",
        "\n",
        "# --- Question content columns ---\n",
        "QUESTION_TEXT_COL = \"question_title\"  # Text of the question prompt\n",
        "OPTION_COLS = [\"option_a\", \"option_b\", \"option_c\", \"option_d\", \"option_e\"]  # Multiple choice options\n",
        "CORRECT_OPTION_COL = \"correct_option_letter\"  # Letter of the correct option (A-E)\n",
        "FORMATTED_TEXT_COL = \"formatted_question_text\"  # Processed question text\n",
        "IRT_DIFFICULTY_COL = \"difficulty\"  # IRT-based difficulty parameter\n",
        "\n",
        "# --- Embedding model configuration ---\n",
        "EMBEDDING_MODEL = \"nomic-ai/modernbert-embed-base\"  # Text embedding model\n",
        "EMBEDDING_BATCH_SIZE = 32  # Batch size for generating embeddings\n",
        "\n",
        "# --- Dataset split configuration ---\n",
        "TEST_SPLIT_SIZE = 0.1     # Proportion of data for final evaluation\n",
        "VALIDATION_SPLIT_SIZE = 0.2  # Proportion of data for validation during training\n",
        "RANDOM_SEED = 42          # Seed for reproducibility\n",
        "\n",
        "# --- Numerical features for neural network input ---\n",
        "# These features capture various aspects of question complexity and structure\n",
        "NUMERICAL_FEATURE_COLS = [\n",
        "    # Text complexity metrics\n",
        "    \"question_word_count\", \"question_char_count\", \"question_avg_word_length\",\n",
        "    \"question_digit_count\", \"question_special_char_count\",\n",
        "    \"question_mathematical_symbols\", \"question_latex_expressions\",\n",
        "    \n",
        "    # Option-related metrics\n",
        "    \"jaccard_similarity_std\", \"avg_option_length\", \"avg_option_word_count\",\n",
        "    \n",
        "    # Pedagogical metadata\n",
        "    \"avg_steps\", \"level\", \"num_misconceptions\", \"has_image\",\n",
        "    \n",
        "    # Target variable (used for both feature and prediction)\n",
        "    # IRT_DIFFICULTY_COL,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Define paths\n",
        "\n",
        "Set up file paths for data input/output and create a timestamped directory for results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results will be saved to: /Users/matias/Projects/capstone-new/results/20250506_130810\n"
          ]
        }
      ],
      "source": [
        "# --- Path configuration ---\n",
        "# Root directory is one level up from current working directory\n",
        "ROOT_DIR = os.path.abspath(os.path.dirname(os.getcwd()))\n",
        "\n",
        "# Input data location\n",
        "DATA_DIR = \"data/zapien\"\n",
        "ANSWERS_FILE_PATH = os.path.join(ROOT_DIR, DATA_DIR, \"answers.csv\")\n",
        "QUESTIONS_FILE_PATH = os.path.join(ROOT_DIR, DATA_DIR, \"questions.csv\")\n",
        "\n",
        "# Output directory for results\n",
        "RESULTS_DIR = os.path.join(ROOT_DIR, \"results\")\n",
        "\n",
        "# Create a timestamped directory for this run's outputs\n",
        "current_run_dir = os.path.join(RESULTS_DIR, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "os.makedirs(current_run_dir, exist_ok=True)\n",
        "print(\"Results will be saved to:\", current_run_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Load raw data\n",
        "\n",
        "Load two key datasets:\n",
        "1. `answers_df`: Student response data (who answered what and whether they were correct)\n",
        "2. `questions_df`: Question content and metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student answers dataset: 269171 rows, 16 columns\n",
            "Question content dataset: 4696 rows, 51 columns\n",
            "\n",
            "Sample student answers:\n",
            "Additional question features dataset: 4483 rows, 19 columns\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>Answer_Length_Variance</th>\n",
              "      <th>Correct_Distractor_CosineSim_Mean</th>\n",
              "      <th>Distractor_Embedding_Distance_Mean</th>\n",
              "      <th>Extreme_Wording_Option_Count</th>\n",
              "      <th>Has_Abstract_Symbols</th>\n",
              "      <th>Has_NoneAll_Option</th>\n",
              "      <th>Knowledge_Dimension</th>\n",
              "      <th>LLM_Distractor_Plausibility_Max</th>\n",
              "      <th>LLM_Distractor_Plausibility_Mean</th>\n",
              "      <th>Mathematical_Notation_Density</th>\n",
              "      <th>Max_Expression_Nesting_Depth</th>\n",
              "      <th>Most_Complex_Number_Type</th>\n",
              "      <th>Option_Length_Outlier_Flag</th>\n",
              "      <th>Problem_Archetype</th>\n",
              "      <th>Question_Answer_Info_Gap</th>\n",
              "      <th>Ratio_Abstract_Concrete_Symbols</th>\n",
              "      <th>RealWorld_Context_Flag</th>\n",
              "      <th>Units_Check</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28193</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.473637</td>\n",
              "      <td>0.517494</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.583333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual Definition/Understanding</td>\n",
              "      <td>3.333333333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21906</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.805628</td>\n",
              "      <td>0.240420</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>3.416667</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Geometric Reasoning/Proof</td>\n",
              "      <td>3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15904</td>\n",
              "      <td>0.40000</td>\n",
              "      <td>0.691133</td>\n",
              "      <td>0.471909</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.916667</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Formula Application</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16039</td>\n",
              "      <td>9.87117</td>\n",
              "      <td>0.547879</td>\n",
              "      <td>0.579021</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.916667</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual Definition/Understanding</td>\n",
              "      <td>3</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16141</td>\n",
              "      <td>3.60000</td>\n",
              "      <td>0.978145</td>\n",
              "      <td>0.037294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conceptual</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Word Problem - Calculation</td>\n",
              "      <td>2</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_id  Answer_Length_Variance  Correct_Distractor_CosineSim_Mean  \\\n",
              "0        28193                 0.00000                           0.473637   \n",
              "1        21906                 0.00000                           0.805628   \n",
              "2        15904                 0.40000                           0.691133   \n",
              "3        16039                 9.87117                           0.547879   \n",
              "4        16141                 3.60000                           0.978145   \n",
              "\n",
              "   Distractor_Embedding_Distance_Mean  Extreme_Wording_Option_Count  \\\n",
              "0                            0.517494                             0   \n",
              "1                            0.240420                             0   \n",
              "2                            0.471909                             0   \n",
              "3                            0.579021                             0   \n",
              "4                            0.037294                             0   \n",
              "\n",
              "   Has_Abstract_Symbols  Has_NoneAll_Option Knowledge_Dimension  \\\n",
              "0                     1                   0          Conceptual   \n",
              "1                     1                   0          Conceptual   \n",
              "2                     0                   0          Conceptual   \n",
              "3                     1                   0          Conceptual   \n",
              "4                     0                   0          Conceptual   \n",
              "\n",
              "   LLM_Distractor_Plausibility_Max  LLM_Distractor_Plausibility_Mean  \\\n",
              "0                         5.000000                          3.583333   \n",
              "1                         4.666667                          3.416667   \n",
              "2                         5.000000                          2.916667   \n",
              "3                         5.000000                          3.916667   \n",
              "4                         5.000000                          3.500000   \n",
              "\n",
              "   Mathematical_Notation_Density  Max_Expression_Nesting_Depth  \\\n",
              "0                       0.250000                             2   \n",
              "1                       0.250000                             0   \n",
              "2                       0.647059                             0   \n",
              "3                       0.520000                             1   \n",
              "4                       0.714286                             0   \n",
              "\n",
              "   Most_Complex_Number_Type  Option_Length_Outlier_Flag  \\\n",
              "0                         1                           0   \n",
              "1                         3                           0   \n",
              "2                         2                           0   \n",
              "3                         1                           0   \n",
              "4                         2                           0   \n",
              "\n",
              "                     Problem_Archetype Question_Answer_Info_Gap  \\\n",
              "0  Conceptual Definition/Understanding              3.333333333   \n",
              "1            Geometric Reasoning/Proof                        3   \n",
              "2                  Formula Application                        3   \n",
              "3  Conceptual Definition/Understanding                        3   \n",
              "4           Word Problem - Calculation                        2   \n",
              "\n",
              "   Ratio_Abstract_Concrete_Symbols  RealWorld_Context_Flag  Units_Check  \n",
              "0                         0.500000                       0            0  \n",
              "1                         0.666667                       0            0  \n",
              "2                         0.000000                       0            0  \n",
              "3                         0.333333                       0            0  \n",
              "4                         0.200000                       0            0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load student answers (response data)\n",
        "student_answers_df = pd.read_csv(os.path.join(DATA_DIR, ANSWERS_FILE_PATH))\n",
        "\n",
        "# Load question content and metadata\n",
        "question_content_df = pd.read_csv(os.path.join(DATA_DIR, QUESTIONS_FILE_PATH))\n",
        "\n",
        "# Display dataset dimensions to verify loading\n",
        "print(f\"Student answers dataset: {student_answers_df.shape[0]} rows, {student_answers_df.shape[1]} columns\")\n",
        "print(f\"Question content dataset: {question_content_df.shape[0]} rows, {question_content_df.shape[1]} columns\")\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "print(\"\\nSample student answers:\")\n",
        "student_answers_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original student answers shape: (269171, 16)\n",
            "Original unique questions: 9769\n",
            "Filtering questions with 0%/100% correctness or < 10 responses...\n",
            "Removed 20632 responses belonging to 5275 questions due to filtering criteria.\n",
            "Filtered student answers shape: (248539, 16)\n",
            "Remaining unique questions: 4494\n"
          ]
        }
      ],
      "source": [
        "# --- Filter Questions Based on Response Patterns ---\n",
        "print(f\"Original student answers shape: {student_answers_df.shape}\")\n",
        "initial_question_count = student_answers_df[QUESTION_ID_COL].nunique() # Get initial unique question count\n",
        "print(f\"Original unique questions: {initial_question_count}\")\n",
        "print(\"Filtering questions with 0%/100% correctness or < 10 responses...\")\n",
        "\n",
        "# Calculate stats per question (mean correctness and count)\n",
        "question_stats = student_answers_df.groupby(QUESTION_ID_COL)[CORRECTNESS_COL].agg(['mean', 'count'])\n",
        "\n",
        "# Identify questions to remove:\n",
        "# 1. Perfect scores (mean 0 or 1)\n",
        "# 2. Insufficient responses (count < 10)\n",
        "qids_to_remove = question_stats[\n",
        "    (question_stats['mean'] == 0) |\n",
        "    (question_stats['mean'] == 1) |\n",
        "    (question_stats['count'] < 10)\n",
        "].index\n",
        "\n",
        "# Filter the student_answers_df\n",
        "initial_responses = len(student_answers_df)\n",
        "final_question_count = initial_question_count # Default if nothing removed\n",
        "if not qids_to_remove.empty:\n",
        "    student_answers_df = student_answers_df[~student_answers_df[QUESTION_ID_COL].isin(qids_to_remove)]\n",
        "    removed_count = initial_responses - len(student_answers_df)\n",
        "    final_question_count = student_answers_df[QUESTION_ID_COL].nunique() # Get final unique question count\n",
        "    print(f\"Removed {removed_count} responses belonging to {len(qids_to_remove)} questions due to filtering criteria.\")\n",
        "    print(f\"Filtered student answers shape: {student_answers_df.shape}\")\n",
        "    print(f\"Remaining unique questions: {final_question_count}\")\n",
        "else:\n",
        "    print(\"No questions met the filtering criteria for removal.\")\n",
        "    print(f\"Remaining unique questions: {initial_question_count}\") # Same as initial\n",
        "\n",
        "\n",
        "# Optional: Check if the DataFrame is empty after filtering\n",
        "if student_answers_df.empty:\n",
        "    raise ValueError(\"Response DataFrame is empty after filtering questions. Cannot proceed.\")\n",
        "\n",
        "# --- End Filtering ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Calculate Question Difficulty Using Item Response Theory\n",
        "\n",
        "Item Response Theory (IRT) provides a framework for estimating latent traits like question difficulty and student ability based on observed response patterns. \n",
        "\n",
        "Here we use a 1-Parameter Logistic (1PL) model, also known as the Rasch model, which assumes:\n",
        "- Each question has a single difficulty parameter\n",
        "- All questions have equal discrimination\n",
        "- No guessing parameter is included\n",
        "\n",
        "The model estimates difficulty values where higher values indicate more difficult questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training IRT 2PL (PyTorch): 100%|██████████| 200/200 [00:02<00:00, 85.89it/s, loss=0.4388, bce=0.4388]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question difficulty (2PL) summary statistics:\n",
            "count    4494.000000\n",
            "mean       -0.194739\n",
            "std         0.560646\n",
            "min        -1.380479\n",
            "25%        -0.630784\n",
            "50%        -0.257883\n",
            "75%         0.154857\n",
            "max         1.766039\n",
            "Name: difficulty, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Estimate question difficulty using IRT 1PL (Rasch) model\n",
        "# This function fits a model to student response patterns and returns\n",
        "# a dataframe with question_id and corresponding difficulty parameters\n",
        "question_params_df = estimate_irt_2pl_params(\n",
        "    response_df = student_answers_df,\n",
        "    user_col = USER_ID_COL,\n",
        "    question_col = QUESTION_ID_COL,\n",
        "    correctness_col = CORRECTNESS_COL,\n",
        ")\n",
        "\n",
        "# Save the parameters (difficulty and discrimination)\n",
        "question_params_df.to_csv(os.path.join(current_run_dir, \"01_irt_2pl_params.csv\"), index=False) # New filename\n",
        "\n",
        "# Display summary statistics of estimated difficulties\n",
        "print(\"Question difficulty (2PL) summary statistics:\")\n",
        "print(question_params_df[IRT_DIFFICULTY_COL].describe()) # Still use difficulty column\n",
        "\n",
        "# Keep only difficulty for merging if needed, or adjust merge step\n",
        "question_difficulty_df = question_params_df[[QUESTION_ID_COL, IRT_DIFFICULTY_COL]].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract Question Features\n",
        "\n",
        "We engineer two types of features from the question content:\n",
        "\n",
        "1. **Text-based features**: Complexity metrics from the question text (word count, character count, etc.)\n",
        "2. **Option-based features**: Metrics related to multiple-choice options (option similarity, length, etc.)\n",
        "\n",
        "These features capture linguistic complexity and structural patterns that may correlate with question difficulty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Questions dataset after merging additional features: 4696 rows, 79 columns\n",
            "Sample of extracted question features:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question_word_count</th>\n",
              "      <th>question_avg_word_length</th>\n",
              "      <th>jaccard_similarity_std</th>\n",
              "      <th>avg_option_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28193</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.777778</td>\n",
              "      <td>5.551115e-17</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21906</td>\n",
              "      <td>22.0</td>\n",
              "      <td>3.545455</td>\n",
              "      <td>5.551115e-17</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15904</td>\n",
              "      <td>23.0</td>\n",
              "      <td>4.695652</td>\n",
              "      <td>5.551115e-17</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16039</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2.781250</td>\n",
              "      <td>1.401058e-01</td>\n",
              "      <td>21.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16141</td>\n",
              "      <td>29.0</td>\n",
              "      <td>3.344828</td>\n",
              "      <td>1.959592e-01</td>\n",
              "      <td>21.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_id  question_word_count  question_avg_word_length  \\\n",
              "0        28193                  9.0                  4.777778   \n",
              "1        21906                 22.0                  3.545455   \n",
              "2        15904                 23.0                  4.695652   \n",
              "3        16039                 32.0                  2.781250   \n",
              "4        16141                 29.0                  3.344828   \n",
              "\n",
              "   jaccard_similarity_std  avg_option_length  \n",
              "0            5.551115e-17                6.0  \n",
              "1            5.551115e-17               11.0  \n",
              "2            5.551115e-17                6.2  \n",
              "3            1.401058e-01               21.4  \n",
              "4            1.959592e-01               21.2  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract text-based features from question titles\n",
        "# These include word count, character count, average word length, etc.\n",
        "text_features_df = question_content_df[QUESTION_TEXT_COL].apply(\n",
        "    lambda text: pd.Series(extract_text_features(text))\n",
        ").add_prefix(\"question_\")\n",
        "\n",
        "# Extract features related to multiple-choice options\n",
        "# These include similarity between options, average length, etc.\n",
        "option_features_list = question_content_df.apply(\n",
        "    lambda row: calculate_option_features(row, OPTION_COLS), \n",
        "    axis=1\n",
        ").tolist()\n",
        "option_features_df = pd.DataFrame(option_features_list, index=question_content_df.index)\n",
        "\n",
        "# Combine all features with the original question data\n",
        "questions_with_features_df = pd.concat(\n",
        "    [question_content_df, text_features_df, option_features_df], \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the enhanced questions dataset\n",
        "questions_with_features_df.to_csv(\n",
        "    os.path.join(current_run_dir, \"02_question_features.csv\"), \n",
        "    index=False\n",
        ")\n",
        "\n",
        "# Display a sample of the extracted features\n",
        "print(\"Sample of extracted question features:\")\n",
        "questions_with_features_df[[QUESTION_ID_COL, 'question_word_count', 'question_avg_word_length', \n",
        "                             'jaccard_similarity_std', 'avg_option_length']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Text Embeddings\n",
        "\n",
        "Text embeddings represent natural language in dense vector spaces, capturing semantic meaning and relationships. We generate embeddings for:\n",
        "\n",
        "1. The complete formatted question text (including options)\n",
        "2. Each individual multiple-choice option\n",
        "\n",
        "These embeddings will serve as inputs to our neural network model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Format question text\n",
        "\n",
        "First, we format the question content into a unified text representation that combines the question prompt and options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample formatted question text:\n",
            "Question: Which of the following functions corresponds to `2^-x`?\n",
            "Wrong Answer: `p(x)`\n",
            "Wrong Answer: `n(x)`\n",
            "Wrong Answer: `h(x)`\n",
            "Correct Answer: `m(x)`\n",
            "Wrong Answer: `g(x)`\n"
          ]
        }
      ],
      "source": [
        "# Format each question into a standardized text representation\n",
        "# This combines the question title and all options into a single text string\n",
        "questions_with_features_df[FORMATTED_TEXT_COL] = questions_with_features_df.apply(\n",
        "    lambda row: format_question_text(\n",
        "        row,\n",
        "        title_col = QUESTION_TEXT_COL,\n",
        "        option_cols = OPTION_COLS,\n",
        "        correct_option_col = CORRECT_OPTION_COL,\n",
        "    ),\n",
        "    axis = 1,\n",
        ")\n",
        "\n",
        "# Display a sample of the formatted question text\n",
        "print(\"Sample formatted question text:\")\n",
        "print(questions_with_features_df[FORMATTED_TEXT_COL].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Generate embeddings for questions and options\n",
        "\n",
        "Now we generate embeddings using a pretrained language model. This is a computationally intensive step that transforms text into numerical vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for formatted question text...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 147/147 [01:11<00:00,  2.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for option_a...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 147/147 [00:11<00:00, 12.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for option_b...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 147/147 [00:09<00:00, 16.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for option_c...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 147/147 [00:08<00:00, 16.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for option_d...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 147/147 [00:08<00:00, 17.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for option_e...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 147/147 [00:08<00:00, 18.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings saved to /Users/matias/Projects/capstone-new/results/20250506_130810/03_embeddings.pkl\n",
            "Embedding dimension: 768\n"
          ]
        }
      ],
      "source": [
        "# Initialize a structure to hold all embeddings\n",
        "question_embeddings = {\n",
        "    \"question_ids\": questions_with_features_df[QUESTION_ID_COL].tolist(),\n",
        "    \"formatted_embeddings\": {},           # Will hold embeddings for complete formatted questions\n",
        "    \"option_embeddings\": defaultdict(dict),  # Will hold embeddings for each option\n",
        "}\n",
        "\n",
        "# Generate embeddings for the formatted question text\n",
        "print(\"Generating embeddings for formatted question text...\")\n",
        "question_embeddings[\"formatted_embeddings\"] = generate_text_embeddings(\n",
        "    data_df = questions_with_features_df,\n",
        "    text_col = FORMATTED_TEXT_COL,\n",
        "    id_col = QUESTION_ID_COL,\n",
        "    model_name = EMBEDDING_MODEL,\n",
        "    batch_size = EMBEDDING_BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Generate embeddings for each multiple-choice option\n",
        "for option_col in OPTION_COLS:\n",
        "    print(f\"Generating embeddings for {option_col}...\")\n",
        "    question_embeddings[\"option_embeddings\"][option_col] = generate_text_embeddings(\n",
        "        data_df = questions_with_features_df,\n",
        "        text_col = option_col,\n",
        "        id_col = QUESTION_ID_COL,\n",
        "        model_name = EMBEDDING_MODEL,\n",
        "        batch_size = EMBEDDING_BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "# Save the embeddings to a pickle file\n",
        "embedding_file_path = os.path.join(current_run_dir, \"03_embeddings.pkl\")\n",
        "with open(embedding_file_path, \"wb\") as f:\n",
        "    pickle.dump(question_embeddings, f)\n",
        "\n",
        "print(f\"Embeddings saved to {embedding_file_path}\")\n",
        "\n",
        "# Display embedding dimension to verify\n",
        "embedding_dimension = next(iter(question_embeddings[\"formatted_embeddings\"].values())).shape[0]\n",
        "print(f\"Embedding dimension: {embedding_dimension}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Split Data into Train/Validation/Test Sets\n",
        "\n",
        "To ensure robust model evaluation, we divide our data into three sets:\n",
        "\n",
        "1. **Training set**: Used to train the model\n",
        "2. **Validation set**: Used for hyperparameter tuning and early stopping\n",
        "3. **Test set**: Used for final evaluation (hold-out data)\n",
        "\n",
        "We use stratified sampling to maintain similar distributions of correctness and difficulty across all sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete merged dataset: 247822 rows, 96 columns\n"
          ]
        }
      ],
      "source": [
        "# Merge all the necessary data for splitting\n",
        "# This includes student responses, question features, and difficulty estimates\n",
        "complete_dataset_df = student_answers_df.merge(\n",
        "    questions_with_features_df, \n",
        "    on=QUESTION_ID_COL\n",
        ").merge(\n",
        "    question_difficulty_df[[QUESTION_ID_COL, IRT_DIFFICULTY_COL]], \n",
        "    on=QUESTION_ID_COL\n",
        ")\n",
        "\n",
        "print(f\"Complete merged dataset: {complete_dataset_df.shape[0]} rows, {complete_dataset_df.shape[1]} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: 3137 questions\n",
            "Validation set: 897 questions\n",
            "Test set: 449 questions\n"
          ]
        }
      ],
      "source": [
        "# Perform stratified 3-way split of questions\n",
        "# We stratify by correctness and difficulty to ensure balanced distributions\n",
        "train_question_ids, validation_question_ids, test_question_ids = stratified_question_split_3way(\n",
        "    df = complete_dataset_df,\n",
        "    question_col = QUESTION_ID_COL,\n",
        "    stratify_cols = [CORRECTNESS_COL, IRT_DIFFICULTY_COL],  # Stratify by both correctness and difficulty\n",
        "    test_size = TEST_SPLIT_SIZE,\n",
        "    val_size = VALIDATION_SPLIT_SIZE,\n",
        "    random_state = RANDOM_SEED,\n",
        "    n_bins = 3,\n",
        ")\n",
        "\n",
        "# Save the test set question IDs for later evaluation\n",
        "pd.DataFrame({QUESTION_ID_COL: test_question_ids}).to_csv(\n",
        "    os.path.join(current_run_dir, \"holdout_ids.csv\"), \n",
        "    index=False\n",
        ")\n",
        "\n",
        "# Print split sizes\n",
        "print(f\"Training set: {len(train_question_ids)} questions\")\n",
        "print(f\"Validation set: {len(validation_question_ids)} questions\")\n",
        "print(f\"Test set: {len(test_question_ids)} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Prepare Neural Network Datasets\n",
        "\n",
        "Now we transform our data into the format required for neural network training. This involves:\n",
        "\n",
        "1. Scaling numerical features to zero mean and unit variance\n",
        "2. Encoding categorical features (like user IDs)\n",
        "3. Looking up embeddings for questions\n",
        "4. Packaging everything into NumPy arrays for TensorFlow/Keras\n",
        "\n",
        "The resulting datasets will have multiple input components (numerical features, user IDs, and text embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "prepare_nn_datasets() got an unexpected keyword argument 'new_categorical_cols'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m embedding_dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(question_embeddings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformatted_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Prepare datasets for neural network training\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This function handles scaling, encoding, and packaging of all inputs\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_dataset, train_labels, validation_dataset, validation_labels, feature_preprocessors \u001b[38;5;241m=\u001b[39m prepare_nn_datasets(\n\u001b[1;32m      7\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m complete_dataset_df,\n\u001b[1;32m      8\u001b[0m     combined_embeddings \u001b[38;5;241m=\u001b[39m question_embeddings,\n\u001b[1;32m      9\u001b[0m     train_q_ids \u001b[38;5;241m=\u001b[39m train_question_ids,\n\u001b[1;32m     10\u001b[0m     val_q_ids \u001b[38;5;241m=\u001b[39m validation_question_ids,\n\u001b[1;32m     11\u001b[0m     user_col \u001b[38;5;241m=\u001b[39m USER_ID_COL,\n\u001b[1;32m     12\u001b[0m     question_col \u001b[38;5;241m=\u001b[39m QUESTION_ID_COL,\n\u001b[1;32m     13\u001b[0m     correctness_col \u001b[38;5;241m=\u001b[39m CORRECTNESS_COL,\n\u001b[1;32m     14\u001b[0m     numerical_feature_cols \u001b[38;5;241m=\u001b[39m NUMERICAL_FEATURE_COLS,\n\u001b[1;32m     15\u001b[0m     embedding_dim \u001b[38;5;241m=\u001b[39m embedding_dimension,\n\u001b[1;32m     16\u001b[0m     new_categorical_cols \u001b[38;5;241m=\u001b[39m CATEGORICAL_FEATURE_COLS_NEW\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Save the preprocessors for later use with test data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m save_preprocessors(feature_preprocessors, current_run_dir)\n",
            "\u001b[0;31mTypeError\u001b[0m: prepare_nn_datasets() got an unexpected keyword argument 'new_categorical_cols'"
          ]
        }
      ],
      "source": [
        "# Get the embedding dimension from our precomputed embeddings\n",
        "embedding_dimension = next(iter(question_embeddings[\"formatted_embeddings\"].values())).shape[0]\n",
        "\n",
        "# Prepare datasets for neural network training\n",
        "# This function handles scaling, encoding, and packaging of all inputs\n",
        "train_dataset, train_labels, validation_dataset, validation_labels, feature_preprocessors = prepare_nn_datasets(\n",
        "    merged_df = complete_dataset_df,\n",
        "    combined_embeddings = question_embeddings,\n",
        "    train_q_ids = train_question_ids,\n",
        "    val_q_ids = validation_question_ids,\n",
        "    user_col = USER_ID_COL,\n",
        "    question_col = QUESTION_ID_COL,\n",
        "    correctness_col = CORRECTNESS_COL,\n",
        "    numerical_feature_cols = NUMERICAL_FEATURE_COLS,\n",
        "    embedding_dim = embedding_dimension,\n",
        ")\n",
        "\n",
        "# Save the preprocessors for later use with test data\n",
        "save_preprocessors(feature_preprocessors, current_run_dir)\n",
        "\n",
        "# Print dataset shapes to verify\n",
        "print(\"Training data shapes:\")\n",
        "for key, value in train_dataset.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "print(f\"Training labels: {train_labels.shape}\")\n",
        "\n",
        "print(\"\\nValidation data shapes:\")\n",
        "for key, value in validation_dataset.items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "print(f\"Validation labels: {validation_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build and Train Neural Network Model\n",
        "\n",
        "Now we build and train a neural network model to predict student performance on questions. The model has multiple input pathways:\n",
        "\n",
        "1. **User pathway**: Captures student ability using embeddings\n",
        "2. **Numerical features pathway**: Processes extracted question features\n",
        "3. **Text embedding pathway**: Handles semantic meaning from question text\n",
        "\n",
        "These pathways are combined in a multi-layer perceptron (MLP) architecture to predict the probability of a correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the neural network model\n",
        "# This creates a multi-input architecture combining user info, numerical features, and text embeddings\n",
        "model = create_nn_model(\n",
        "    user_vocab_size = feature_preprocessors[\"user_vocab_size\"],  # Number of unique users\n",
        "    numerical_feature_size = train_dataset[\"numerical_input\"].shape[1],  # Number of numerical features\n",
        "    embedding_dim = embedding_dimension,  # Dimension of text embeddings\n",
        ")\n",
        "\n",
        "# Display model summary to understand architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model with early stopping on validation loss\n",
        "print(\"Training neural network model...\")\n",
        "training_history = train_nn_model(\n",
        "    model,\n",
        "    train_dataset, train_labels,\n",
        "    validation_dataset, validation_labels,\n",
        "    epochs = 50,  # Maximum number of training epochs\n",
        "    batch_size = 1024,  # Number of samples per gradient update\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "save_nn_model(model, current_run_dir)\n",
        "print(f\"Model saved to {current_run_dir}\")\n",
        "\n",
        "# Plot training history to visualize learning progress\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_history.history['loss'], label='Training Loss')\n",
        "plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curves')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(training_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Model on Hold-out Test Data\n",
        "\n",
        "Now we evaluate our trained model on the hold-out test set. This gives us an unbiased estimate of model performance on new, unseen questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the hold-out test question IDs\n",
        "holdout_question_ids = pd.read_csv(\n",
        "    os.path.join(current_run_dir, \"holdout_ids.csv\")\n",
        ")[QUESTION_ID_COL].tolist()\n",
        "\n",
        "# Prepare the test dataset using the same preprocessing transformations\n",
        "test_dataset, test_labels = make_dataset(\n",
        "    df = complete_dataset_df,\n",
        "    q_ids = holdout_question_ids,\n",
        "    preprocessors = feature_preprocessors,\n",
        "    combined_embeddings = question_embeddings,\n",
        "    user_col = USER_ID_COL,\n",
        "    question_col = QUESTION_ID_COL,\n",
        "    correctness_col = CORRECTNESS_COL,\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "print(\"Evaluating model on hold-out test data...\")\n",
        "test_metrics = evaluate_model(model, test_dataset, test_labels)\n",
        "\n",
        "# Save the metrics to a JSON file\n",
        "dump_json(test_metrics, os.path.join(current_run_dir, \"holdout_metrics.json\"))\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Test set performance metrics:\")\n",
        "for metric, value in test_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Prediction-based IRT Analysis\n",
        "\n",
        "Finally, we use our trained model to estimate question difficulty parameters and compare them with the original IRT-based estimates. This analysis helps us understand if our neural model captures the same difficulty characteristics as traditional IRT.\n",
        "\n",
        "The process involves:\n",
        "1. Generating predictions for all user × question combinations in the test set\n",
        "2. Using these predictions to fit a new IRT model\n",
        "3. Comparing the resulting difficulty parameters with the original ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a probability matrix of predictions\n",
        "# This contains predicted probabilities for every user × question combination\n",
        "print(\"Generating prediction probability matrix...\")\n",
        "probability_matrix = prediction_matrix(\n",
        "    data_df = complete_dataset_df,\n",
        "    q_ids = holdout_question_ids,\n",
        "    preprocessors = feature_preprocessors,\n",
        "    model = model,\n",
        "    combined_embeddings = question_embeddings,\n",
        "    user_col = USER_ID_COL,\n",
        "    question_col = QUESTION_ID_COL,\n",
        ")\n",
        "\n",
        "# Save the probability matrix\n",
        "probability_matrix_path = os.path.join(current_run_dir, \"04_prediction_probabilities.csv\")\n",
        "probability_matrix.to_csv(probability_matrix_path)\n",
        "print(f\"Saved prediction probability matrix to {probability_matrix_path}\")\n",
        "\n",
        "# Fit an IRT model (2PL) to the predicted probabilities to derive new difficulty estimates\n",
        "print(\"Estimating difficulty parameters from model predictions (2PL)...\")\n",
        "predicted_params_df = difficulty_from_predictions(probability_matrix) # Returns difficulty & discrimination\n",
        "\n",
        "# Save the predicted parameters (difficulty and discrimination)\n",
        "predicted_params_path = os.path.join(current_run_dir, \"05_predicted_2pl_params.csv\") # New filename\n",
        "predicted_params_df.to_csv(predicted_params_path, index=False)\n",
        "print(f\"Saved predicted 2PL parameters to {predicted_params_path}\")\n",
        "\n",
        "# Load the original difficulty estimates for comparison\n",
        "# Make sure this path points to the file saved in *this* run\n",
        "original_params_df = pd.read_csv(os.path.join(current_run_dir, \"01_irt_2pl_params.csv\")) # Load original 2PL params\n",
        "\n",
        "# Compare the original and predicted difficulty parameters\n",
        "# The compare_difficulty function now handles the _orig/_pred suffixes automatically\n",
        "difficulty_comparison_metrics = compare_difficulty(original_params_df, predicted_params_df)\n",
        "\n",
        "# Save the comparison metrics\n",
        "dump_json(difficulty_comparison_metrics, os.path.join(current_run_dir, \"prediction_irt_metrics.json\"))\n",
        "\n",
        "# Display the comparison metrics\n",
        "print(\"Difficulty comparison metrics:\")\n",
        "for metric, value in difficulty_comparison_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Create a scatter plot comparing original and predicted difficulties\n",
        "# Make sure to merge based on the correct column names after suffixes\n",
        "comparison_df = original_params_df.merge(\n",
        "    predicted_params_df,\n",
        "    on=QUESTION_ID_COL,\n",
        "    suffixes=('_orig', '_pred') # compare_difficulty now handles this merge\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(\n",
        "    comparison_df[f\"{IRT_DIFFICULTY_COL}_orig\"], # Use correct suffix _orig\n",
        "    comparison_df[f\"{IRT_DIFFICULTY_COL}_pred\"], # Use correct suffix _pred\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.xlabel('Original IRT Difficulty (2PL)') # Updated label\n",
        "plt.ylabel('Predicted Difficulty (2PL)')  # Updated label\n",
        "plt.title('Comparison of Original vs. Predicted Question Difficulty (2PL)') # Updated title\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add a diagonal line representing perfect correlation\n",
        "# Use correct suffixes _orig and _pred here too\n",
        "min_val = min(comparison_df[f\"{IRT_DIFFICULTY_COL}_orig\"].min(), \n",
        "              comparison_df[f\"{IRT_DIFFICULTY_COL}_pred\"].min())\n",
        "max_val = max(comparison_df[f\"{IRT_DIFFICULTY_COL}_orig\"].max(), \n",
        "              comparison_df[f\"{IRT_DIFFICULTY_COL}_pred\"].max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(current_run_dir, \"difficulty_comparison_2pl.png\"), dpi=300) # New plot filename\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
