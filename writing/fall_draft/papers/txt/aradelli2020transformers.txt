Politecnico di Milano
School of Industrial and Information Engineering
Master of Science in Computer Science and Engineering
Department of Electronics, Information and Bioengineering
Transformers for Question Diï¬ƒculty
Estimation from Text
Supervisor
Prof. Paolo Cremonesi
Co-Supervisor
Dott. Ing. Luca Benedetto
Candidate
Giovanni Aradelli â€“ 920885
Academic Year 2019 â€“ 2020

--- Page Break ---



--- Page Break ---

Sommario
La calibrazione delle domande, ovvero la stima della loro diï¬ƒcoltÃ , Ã¨ una componente
molto importante dellâ€™educazione. Infatti, il livello di conoscenza degli studenti puÃ²
essere stimato dalla correttezza delle loro risposte alle domande dellâ€™esame e dalla
loro diï¬ƒcoltÃ . Una stima accurata della diï¬ƒcoltÃ  delle domande puÃ² anche essere
sfruttata per fornire agli studenti esercizi adatti al loro livello di abilitÃ . Gli approcci
tradizionali alla calibrazione delle domande sono la calibrazione manuale e il pre-test.
Nella calibrazione manuale, uno o piÃ¹ esperti assegnano a ciascuna domanda un
valore numerico che ne rappresenta la diï¬ƒcoltÃ , e questo Ã¨ intrinsecamente soggettivo.
Nel pre-test, le domande vengono somministrate agli studenti in un vero esame e
successivamente la diï¬ƒcoltÃ  Ã¨ stimata a partire dalla correttezza delle loro risposte. Il
pre-test introduce un lungo ritardo tra il momento della generazione della domanda
e il momento in cui puÃ² essere utilizzata per valutare gli studenti. Ricerche recenti
hanno cercato di risolvere questo problema stimando la diï¬ƒcoltÃ  delle domande
usando solo le loro informazioni testuali, sfruttando tecniche di Natural Language
Processing ( NLP)come modelli neurali o bag of words . Lâ€™idea alla base di ciÃ² Ã¨
ridurre (o eliminare) la necessitÃ  di calibrazione manuale e di pre-test, stimando
la diï¬ƒcoltÃ  delle domande dal loro testo, che Ã¨ immediatamente disponibile dopo
la creazione della domanda. I modelli linguistici pre-addestrati, in particolare i
Transformers, hanno portato a notevoli miglioramenti in diverse aree del NLP, ma
ï¬nora nessuno studio ha esplorato il loro utilizzo per la calibrazione delle domande.
In questo lavoro, eseguiamo uno studio su come i modelli Transformers (in particolare,
BERT e DistilBERT) si confrontano con lo stato dellâ€™arte attuale nella stima della
diï¬ƒcoltÃ  dal testo e proponiamo un modello che Ã¨ in grado di migliorare il precedente
stato dellâ€™arte. Il nostro modello Ã¨ addestrato utilizzando il testo e la diï¬ƒcoltÃ  delle
domande, ma puÃ² opzionalmente sfruttare un corpus aggiuntivo di documenti per
migliorare le prestazioni. Test eï¬€ettuati su due diversi set di dati, uno pubblico e
uno privato, mostrano che il nostro modello riduce la radice del valore quadratico
medio (in inglese Root Mean Square Error , RMSE) degli studi precedenti ï¬no al 6,5%
e conferma la nostra intuizione sullâ€™eï¬ƒcacia dei modelli basati su Transformers per
stimare la diï¬ƒcoltÃ  dal testo delle domande. Inoltre, analizziamo quali caratteristiche
delle domande possono inï¬‚uire sullâ€™errore di predizione del modello.
iii

--- Page Break ---



--- Page Break ---

Abstract
Question Diï¬ƒculty Estimation ( QDE), a process which is also referred to as question
calibration, is a very important task in education. Indeed, the knowledge level of
students, also called skill, can be estimated from the correctness of their answers
to exam questions and their diï¬ƒculty. An accurate estimation of question diï¬ƒculty
can also be leveraged to provide students with exercises suitable for their skill level.
Conventional approaches to question calibration are manual calibration and pretesting.
In manual calibration, one or more domain experts assign to each question a numerical
value representing the diï¬ƒculty, and this is intrinsically subjective. In pretesting,
questions are administered to students in a real test scenario, and then the diï¬ƒculty
is estimated from the correctness of their answers. Pretesting introduces a long
delay between the time of question generation and when the question can be used
to score students. Recent research tried to overcome this issue by estimating the
diï¬ƒcultyofquestionsusingonlytheirtextualinformation, exploitingNaturalLanguage
Processing ( NLP) techniques such as neural models or bag of words. The idea behind
this is to reduce (or eliminate) the need for manual calibration and pretesting by
estimating the diï¬ƒculty of questions from their text, which is immediately available at
themomentofquestioncreation. Pre-trainedlanguagemodels, especiallyTransformers,
have led to impressive gains on several NLP tasks, but no previous work has explored
their use for question calibration. In this work, we perform a study of how Transformer
models (speciï¬cally, BERT and DistilBERT) compare with the current state of the art
in the task of QDEfrom text, and propose a model which is capable of outperforming
previous research. Our model is trained on the text of questions and their diï¬ƒculty,
but can optionally take advantage of an additional corpus of domain-related documents
to improve performance. Tests on two diï¬€erent real-world datasets, one public and
one private, show that our model reduces the Root Mean Square Error ( RMSE) of
previous baselines by up to 6.5% and conï¬rms our intuition about the eï¬€ectiveness of
Transformer-based models for QDEfrom text. Furthermore, we carry out an analysis
on which characteristics of the questions (such as length of the text and presence of
numbers) can inï¬‚uence the prediction error.
v

--- Page Break ---



--- Page Break ---

Contents
Sommario iii
Abstract v
List of Figures x
List of Tables xi
1 Introduction 1
2 Background 3
2.1 Questions Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.1.1 Classical Test Theory . . . . . . . . . . . . . . . . . . . . . . . 3
2.1.2 Item Response Theory . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Traditional approaches to Natural Language Processing . . . . . . . . 7
2.2.1 Text Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2.2 Encoding Text . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.3 Term Frequencyâ€“Inverse Document Frequency (TF-IDF) . . . 10
2.2.4 Machine Learning Models . . . . . . . . . . . . . . . . . . . . 11
2.3 Deep Learning in Natural Language Processing . . . . . . . . . . . . 11
2.3.1 Feed Forward Neural Networks . . . . . . . . . . . . . . . . . 12
2.3.2 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . 14
2.3.3 Preventing Neural Networks from overï¬tting . . . . . . . . . . 17
2.3.4 Encoder-Decoder architecture . . . . . . . . . . . . . . . . . . 18
2.3.5 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.6 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.3.7 Pre-trained Models & Transfer Learning . . . . . . . . . . . . 21
3 Related Works 25
3.1 Knowledge Tracing (KT) . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.2 NLP for Diï¬ƒculty Prediction . . . . . . . . . . . . . . . . . . . . . . 26
3.3 Domain-speciï¬c Pre-training . . . . . . . . . . . . . . . . . . . . . . . 29
4 Models 31
4.1 BERT and distilBERT . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.2 Further pre-training on MLM . . . . . . . . . . . . . . . . . . . . . . 32
4.3 Fine-tuning on Question Diï¬ƒculty Estimation . . . . . . . . . . . . . 34
5 Experimental Datasets 37
vii

--- Page Break ---

Contents
5.1 ASSISTments Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 37
5.1.1 Interactions data . . . . . . . . . . . . . . . . . . . . . . . . . 37
5.1.2 Questions data . . . . . . . . . . . . . . . . . . . . . . . . . . 43
5.2 Cloud Academy Dataset . . . . . . . . . . . . . . . . . . . . . . . . . 45
5.2.1 Interactions data . . . . . . . . . . . . . . . . . . . . . . . . . 45
5.2.2 Questions data . . . . . . . . . . . . . . . . . . . . . . . . . . 47
5.2.3 Lectures data . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
6 Experimental Setup 51
6.1 IRT Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
6.2 Pre-training on MLM . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
6.3 Fine-tuning on Question Diï¬ƒculty Estimation . . . . . . . . . . . . . 54
7 Results 57
7.1 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
7.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.2.1 Majority . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.2.2 R2DE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.2.3 ELMo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
7.3.1 ASSISTments . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
7.3.2 Cloud Academy . . . . . . . . . . . . . . . . . . . . . . . . . . 68
8 Conclusion 77
Acronyms 79
References 81
viii

--- Page Break ---

List of Figures
Figure 2.1 Eï¬€ects of the diï¬ƒculty on the Item Response Function (IRF). 6
Figure2.2Eï¬€ects of the discrimination on the Item Response Function ( IRF).6
Figure 2.3 Perceptron. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Figure2.4A Feed Forward Neural Network ( FFNN) with 5 neurons in the
input layer, 3 neurons in the hidden layer and 1 neuron in the output
layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Figure 2.5 Elman neural network architecture. . . . . . . . . . . . . . . . 14
Figure2.6Diï¬€erencesbetweenRectiï¬edLinearUnit( ReLU)andleaky- ReLU.15
Figure 2.7 Long short-term memory (LSTM) structure. . . . . . . . . . . 16
Figure 2.8 seq2seq training process . . . . . . . . . . . . . . . . . . . . . 19
Figure 2.9 The Transformer - model architecture [61]. . . . . . . . . . . . 21
Figure3.1Structure of R2DE, from the input question to the estimated
latent traits. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Figure4.1The two diï¬€erent approaches: the dotted line represents the
approach which performs only ï¬ne-tuning for QDEfrom text, the
continuous line is the approach with the additional pre-training on
Masked Language Modeling (MLM). . . . . . . . . . . . . . . . . . . 31
Figure 4.2 Additional pre-training on Masked Language Modeling (MLM). 33
Figure 4.3 Fine-tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
Figure 4.4 Fine-tuning model architecture of BERT in detail. . . . . . . . 35
Figure5.1Distribution of days between the ï¬rst and last interaction on
ASSISTments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Figure 5.2 ASSISTments, distribution of items per correctness. . . . . . . 42
Figure 5.3 ASSISTments, distribution of students per correctness. . . . . 43
Figure 5.4 Cloud Academy, distribution of questions per correctness. . . . 47
Figure 5.5 Cloud Academy, distribution of students per correctness. . . . 47
Figure 6.1 Experimental setup. . . . . . . . . . . . . . . . . . . . . . . . 51
Figure 6.2 Fitting loss per number of interactions. . . . . . . . . . . . . . 52
Figure 6.3 Distribution of items per IRT diï¬ƒculty. . . . . . . . . . . . . . 53
Figure 7.1 Structure of the ELMo-based model presented in [70]. . . . . . 59
Figure 7.2 ASSISTments, model loss over training epochs. . . . . . . . . 62
Figure 7.3 Test set, distribution of predicted diï¬ƒculties. . . . . . . . . . . 63
Figure 7.4 Distribution of the target diï¬ƒculties and BERT predictions. . 64
Figure7.5ASSISTments, error depending on the input length and true
diï¬ƒculty. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
ix

--- Page Break ---

List of Figures
Figure7.6ASSISTments, error depending on percentage of digits in the
input text and true diï¬ƒculty. . . . . . . . . . . . . . . . . . . . . . . 66
Figure 7.7 Cloud Academy, model loss over training epochs. . . . . . . . 71
Figure 7.8 Test set, distribution of predicted diï¬ƒculties. . . . . . . . . . . 72
Figure 7.9 Distribution of the target diï¬ƒculties and BERT predictions. . 73
Figure7.10Cloud Academy, error depending on the input length and true
diï¬ƒculty. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
x

--- Page Break ---

List of Tables
Table 2.1 BERT models comparison. . . . . . . . . . . . . . . . . . . . . 22
Table2.2Inference time of a full pass of General Language Understanding
Evaluation ( GLUE) task STS-B (sentimental analysis) on CPU with a
batch size of 1 [53]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Table 4.1 BERT and distilBERT size comparison. . . . . . . . . . . . . . 32
Table 5.1 Distribution of scores. . . . . . . . . . . . . . . . . . . . . . . . 38
Table 5.2 Types of problems. . . . . . . . . . . . . . . . . . . . . . . . . . 39
Table 5.3 Main and scaï¬€olding problems. . . . . . . . . . . . . . . . . . . 39
Table 5.4 Interactions dataset dimensionality. . . . . . . . . . . . . . . . 41
Table 5.5 Interactions per item after pre-processing. . . . . . . . . . . . . 42
Table 5.6 Examples of unusable problems. . . . . . . . . . . . . . . . . . 44
Table 5.7 Questions dataset dimensionality through pre-processing steps. 44
Table 5.8 Distribution of problems per length. . . . . . . . . . . . . . . . 45
Table 5.9 Distribution of scores. . . . . . . . . . . . . . . . . . . . . . . . 46
Table 5.10 Interactions per item after pre-processing. . . . . . . . . . . . . 46
Table 5.11 Distribution of questions per length. . . . . . . . . . . . . . . . 48
Table 5.12 Distribution of questions per number of possible choices. . . . . 48
Table 5.13 Distribution of sentences per length. . . . . . . . . . . . . . . . 49
Table 6.1 Datasets size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Table 7.1 ASSISTments results. . . . . . . . . . . . . . . . . . . . . . . . 60
Table 7.2 ASSISTments, train and test errors. . . . . . . . . . . . . . . . 62
Table 7.3 Models performance per question with and without digits. . . . 67
Table 7.4 Models performance per question type. . . . . . . . . . . . . . . 67
Table 7.5 Models performance per question with and without â€œ?â€. . . . . 68
Table 7.6 Cloud Academy, results of BERT and DistilBERT. . . . . . . . 69
Table 7.7 Cloud Academy results. . . . . . . . . . . . . . . . . . . . . . . 70
Table 7.8 Train and test errors. . . . . . . . . . . . . . . . . . . . . . . . 71
Table 7.9 Models performance per question type. . . . . . . . . . . . . . . 75
Table 7.10 Models performance per digits. . . . . . . . . . . . . . . . . . . 75
Table 7.11 Models performance per number of correct choices. . . . . . . . 76
xi

--- Page Break ---



--- Page Break ---

Chapter 1
Introduction
The task of modeling student knowledge over time is deï¬ned as Knowledge Tracing
(KT). An accurate estimation of studentsâ€™ skill levels can be leveraged by teachers to
provide tailored content and understand if there are students struggling and, therefore,
in need of further support. Also, it can be leveraged to perform personalized exercise
recommendations. The growth of online education platforms has led to the availability
of a large number of exercises. Rather than wasting their time on exercises that are
too easy or too hard, students could practice more with exercises appropriate to their
skill level. Studentâ€™s skill is usually estimated using the correctness of their answers
and the diï¬ƒculty of each question they answered to. Questions, also called items, need
to be calibrated before being used in a real exam. Calibration consists in estimating
some latent (i.e., non-observable) characteristics of questions, such as their diï¬ƒculty.
Question Diï¬ƒculty Estimation ( QDE) is essential to give students questions that are
not too diï¬ƒcult or too easy so that they can accurately identify the studentâ€™s skill
level. All examinees would answer wrongly to an item that is too diï¬ƒcult, giving no
information about their knowledge. The same happens with too easy items, which
would be answered correctly by everyone regardless of their skill level.
The standard methodologies used to estimate the diï¬ƒculty of newly created
questions are manual calibration and pre-testing with real students. In manual
calibration, it is necessary the intervention of an expert who manually selects numerical
values representing the diï¬ƒculty of the questions; thus, the evaluation may be biased
and is intrinsically subjective [31]. Also, it is time-consuming and non-scalable. In
pre-testing, questions are administered to a group of students in a real test scenario.
The diï¬ƒculty of the questions under pre-testing is then estimated from the correctness
of studentsâ€™ answers. Questions under pre-testing are not used for scoring, and they
should be indistinguishable from the others. This method leads to an accurate and
reliable estimation but introduces a long delay before the newly generated questions
can be used in a real exam.
In the literature, there is a recent interest in developing a model able to automat-
ically perform QDEfrom text. The growth of this interest is due to the fact that
such a model might eliminate or at least reduce the need for pre-testing and manual
labeling. Automatic diï¬ƒculty assessment can support the creation of new questions,
especially with the increasing availability of algorithms for automatic question genera-
tion, helping in real-time to discard too easy or too diï¬ƒcult ones. Recent models for
QDEfrom text explored the use of Natural Language Processing ( NLP) techniques
1

--- Page Break ---

Chapter 1. Introduction
such asTF-IDF [6] or ELMo embeddings [72], but none of them makes use of the
latestNLPapproaches (e.g., Transformer models). Moreover, most previous models
are trained on questionsâ€™ text, but some models also require additional resources (such
as lecture notes) to extract other useful information. This limits the utilization in
those contexts where such additional resources are not available.
We explored the extent to which using Transformers language models can be
beneï¬cial for QDEfrom text and propose a model that outperforms previous studies.
Our research focuses on two pre-trained models: BERT and DistilBERT. We ï¬ne-
tuned the two models on the task of diï¬ƒculty estimation from text, using only
the text of the questions, but also experimented with the possibility of leveraging
an additional corpus of domain-speciï¬c texts to improve their performance. We
experimented on two diï¬€erent datasets, one private from Cloud Academy1and one
public from ASSISTments2. Results show that the proposed model outperforms all
previous baselines. Our model reduces the Root Mean Square Error ( RMSE) by up to
4.7% with respect to previous approaches when trained only on questionsâ€™ text. The
improvement is even more signiï¬cant, up to 6.5%, if our model is further pre-trained on
an additional corpus of text related to the same domain of the questions. Furthermore,
we propose an analysis of which characteristics of the question may inï¬‚uence the
model performance. The code is publicly available for future research3.
The rest of this document is organized as follows:
Chapter 2 Background, it provides fundamental theoretical notions.
Chapter 3 Related Works, it collects various research works present in the
literature related to our work.
Chapter 4 Models, it describes our approach to perform Question Diï¬ƒculty
Estimation ( QDE), giving an high-level view of the model architecture and
training.
Chapter 5 Experimental Datasets, it presents the datasets and the pre-
processing.
Chapter 6 Experimental Setup, it provides information about the setup used
to perform the various experiments.
Chapter 7 Results, it shows and analyses the results of the experiments
performed on the datasets.
Chapter 8 Conclusion, it gives the ï¬nal considerations about the experiments
and proposes a direction for future works.
1https://cloudacademy.com/
2https://new.assistments.org/
3https://github.com/aradelli/transformers-for-qde
2

--- Page Break ---

Chapter 2
Background
This chapter has the purpose of establishing some common ground by introducing the
theoretical foundations used for the development of the proposed model. Section 2.1
introduces the concept of diï¬ƒculty of a question, presenting two popular psychometry
frameworks. Then, Section 2.2 introduces a classic approach to text mining and
Section 2.3 presents the most modern NLPtechniques based on Transformers models.
2.1 Questions Calibration
2.1.1 Classical Test Theory
Classical Test Theory ( CTT) is a theory that predicts outcomes of psychological
testing, such as the diï¬ƒculty of items or the ability of test-takers. The goal of CTT,
also called â€œtrue score theoryâ€, is to improve the reliability and validity of tests. The
term â€œclassicalâ€ refers to the time when this framework was developed, and it is also
in contrast with modern psychometric theories such as Item Response Theory. CTT
has been the most used in the last century and has the advantage of being simple to
compute and understand compared to Item Response Theory (IRT).
Classical Test Theory assumes that each individual has associated a true score
T, which would be possible to observe if there was no error in the estimate. For a
person taking a test, his true score is the expected value of the observed scores over
an inï¬nitely long run of repeated independent administrations of the same test [7].
Doing an inï¬nite number of observations is impossible; therefore, the observed score
Xis used. The observed score Xis made up of the sum of the true score Tand an
ErrorE; errors are assumed to be normally distributed with mean equal to zero. The
(linear) model describing CTT is the following:
X=T+E (2.1)
WhereTandEare two unobservable (or latent) variables.
The major assumptions [24] underline the CTT are:
3

--- Page Break ---

Chapter 2. Background
T,Eare not correlated.
Eis normally distributed with zero mean.
The errors of diï¬€erent tests are not correlated.
A reliability coeï¬ƒcient can provide an estimate of the level of concordance between
observed and true scores. The reliability of test scores 2
XTis deï¬ned as follows:
2
XT=2
T
2
X=2
T
2
T+2
E(2.2)
Where2
Tis the true score variance, 2
Xis the observed score variance, and 2
Eis
the error variance.
The reliability is zero when all variation in the observed scores is due to measure-
ment error. The maximum value (i.e., 2
XT= 1) is when there is no measurement
error. However, the true scores of test-takers are not observable, thus the reliability
is estimated indirectly using parallel-tests. Two tests are considered parallel if they
have the same observed variance in the population of examinees, and each student
has the same true score on both tests.
The concept of diï¬ƒculty of an item in CTTis expressed by the p-value. Theprefers
to probability and is the fraction of correct responses in the considered population.
Usually, the p-valueis typically referred to as item diï¬ƒculty or correctness . It should
be noted that the higher the p-value, the easier the item is. In the same way, we can
also deï¬ne the wrongness as1 p-value.
The property of an item to discriminate between high ability examinees and low
ability examinees is called discrimination . If an item is dichotomously scored (i.e., the
score is 0 or 1), the discrimination estimate is computed as a point-biserial correlation.
CTThas several shortcomings that have led to the development of other models.
The major limitation of CTTcan be summarized as a circular dependency: (a) the
person statistic is (item) sample dependent, and (b) the item statistics are (examinee)
sample dependent [18]. On the other hand, CTThas weak theoretical assumptions,
which make it easy to apply in many testing situations.
2.1.2 Item Response Theory
Item Response Theory ( IRT)â€”also named latent traits theoryâ€”is a family of models
used to perform abilities assessments. Important educational tests, such as the
Graduate Management Admission Test ( GMAT), useIRTto improve measurement
accuracy and reliability [51]. However, IRTis not used only in the educational context:
the latent trait can be a behavioral characteristic, such as customer satisfaction or
others. This theory was developed around the mid-1900s but was not widely used
until the end of the century. A property that has made IRTwidely used is the
4

--- Page Break ---

2.1. Questions Calibration
invariance property: itemsâ€™ latent traits do not depend on the ability distribution of
test-takers [25].
InIRT, each student is supposed to have associated one or more latent traits, also
called skills or abilities. The same is true for questions (also called items) that have
one or more latent traits associated with them.
IRTmodels can be unidimensional or multidimensional (MIRT): in the ï¬rst case,
each student is modeled as having only one skill, while the second one introduces
multiple skills referring to diï¬€erent topics (e.g., mathematics and science) and multiple
diï¬ƒculties for each question.
IRTmodels can be grouped in three main categories depending in the number of
questionsâ€™ parameters which are deï¬ned:
One-Parameter Logistic ( 1PL), also known as Rasch Model [50], which deï¬nes
a diï¬ƒculty for each item.
Two-Parameter Logistic ( 2PL), which deï¬nes a diï¬ƒculty and a discrimination
for each item.
Three-Parameter Logistic ( 3PL), which deï¬nes a diï¬ƒculty, a discrimination,
and a guess factor for each item.
Diï¬€erent IRTmodels can be classiï¬ed based on the score that can be assigned to
studentsâ€™ answers. In the case the score of an item is only correct or wrong, i.e. item
dichotomous, we have Dichotomous IRT. These models are very common since they
can model Multiple Choice Questions (MCQ) or true-false questions.
Given a question iwith diï¬ƒculty bi, discrimination aiand a guess factor ci,
IRTdeï¬nes the Item Response Function ( IRF) (also know as Item Characteristic
Curve (ICC)), which represents the probability that a student with a skill level theta
() correctly answers the question. The formula of the item response function is as
follows:
Pi() =ci+ (1 ci)1
1 +eai( bi)(2.3)
Thediï¬ƒcultyparameteraï¬€ectsthelocationofthelogisticcurvealongthehorizontal
axis: as the diï¬ƒculty increases, a student with a given skill level is less likely to answer
the question correctly. Figure 2.1 shows three items with diï¬€erent diï¬ƒculty.
5

--- Page Break ---

Chapter 2. Background
4
 3
 2
 1
 0 1 2 3 4
Ability ( )
0.00.20.40.60.81.0Probability of correct answerb1 = -1.5
b2 = 0
b3 = 3
Figure 2.1. Eï¬€ects of the diï¬ƒculty on the Item Response Function (IRF).
The discrimination aï¬€ects the slope of the logistic curve: as the discrimination
increases, the steepness increases. Figure 2.2 shows three items with diï¬€erent discrim-
ination. The question with discrimination a3= 3, manages to discriminate with good
precision a student with an ability level ( ) lower than 1or greater than 1, while
the question with discrimination a1= 0:5does not provide much information because
students with diï¬€erent skill levels have a similar probability of correctly answering
the question. A question with a large discrimination value can diï¬€erentiate better
between diï¬€erent skills level of students, and this is the reason why the discrimination
can be considered as a proxy of the â€œqualityâ€ of a question.
4
 3
 2
 1
 0 1 2 3 4
Ability ( )
0.00.20.40.60.81.0Probability of correct answera1 = 0.5
a2 = 1
a3 = 3
Figure 2.2. Eï¬€ects of the discrimination on the Item Response Function (IRF).
6

--- Page Break ---

2.2. Traditional approaches to Natural Language Processing
The guess factor takes into consideration the fact that an examinee could pick
the correct answer by chance, even without having the required knowledge level. An
example where this may happen is in Multiple Choice Questions (MCQ).
Parameter estimation methods
In the literature, we can ï¬nd four techniques for the estimation of item response models:
joint maximum likelihood, conditional maximum likelihood, marginal maximum
likelihood, and Bayesian estimation with Markov chain Monte Carlo. All of them
rely heavily on the assumption that individuals are independent from each other
and that the item responses of a given individual are independent, given that an
individualâ€™s skill level [33]. The applicability of the aforementioned methods depends
on the number of parameters to ï¬t and whether we need to estimate both studentsâ€™
and itemsâ€™ latent traits or only one of the two.
It is important to point out that the quality of the estimated parameters, regardless
of the method, is inï¬‚uenced by two factors [52]: i) test length (i.e., how many items
are in the test); ii) sample size (i.e., how many diï¬€erent interactions for each item).
2.2 TraditionalapproachestoNaturalLanguagePro-
cessing
Natural Language Processing ( NLP) is a ï¬eld of artiï¬cial intelligence that deals
with the interaction between computers and humans using natural language. It is
an interdisciplinary ï¬eld that began around 1950, combining linguistic, computer
science, and artiï¬cial intelligence. With the advent of big data, we have seen an
exponential increase of data in the form of text and the need of processing them in
an automated way. Dealing with language is very tricky: it is inherently ambiguous,
words can assume various meanings depending on the context, and they can acquire
new meanings over time; these and many other things make it a challenging task.
Tasks in NLPfrequently involve speech recognition, natural language understanding,
and natural language generation.
2.2.1 Text Preprocessing
Text preprocessing is traditionally a necessary step for NLPtasks. It changes text into
a more digestible form, below are some of the most common steps of this phase [62].
Tokenization
Tokenization consists in splitting a text into words, phrases, or other meaningful parts,
namely â€œtokensâ€. Tokens can be either words, characters, or subwords. Typically, the
segmentation is carried out considering only alphabetic or alphanumeric characters
that are delimited by non-alphanumeric characters (e.g., punctuations, whitespace).
As a simple example of word-level tokenization, we can consider the sentence:
7

--- Page Break ---

Chapter 2. Background
This is an example of tokenization
The result is a list of tokens, where each token is a word:
[This,is,an,example,of,tokenization ]
While it is the most straightforward way to separate texts in smaller chunks, it can
cause problems when you have a vast corpus: it usually yields a huge vocabulary (the
set of all unique tokens used), especially in morphologically rich languages. Another
issue in tokenization is also how to deal with unknown words; usually, a special token
is reserved for these words.
A better way to perform tokenization is Byte Pair Encoding ( BPE) [55]. The
sentence of before tokenized using BPE:
[This,is,an,example,of,token,ization]
WordPiece [54] is another subword tokenization algorithm very similar to BPE.
Stop word removal
Stop words are common words (e.g., prepositions, articles) that do not contribute
much from a semantic perspective to the content or meaning of a document; for this
reason, they should be eliminated from a text. Moreover, removing stop words reduces
the dimensionality of the input space. The most frequent words in text documents are
articles, prepositions, and pronouns that do not give the meaning of the documents.
Very frequent words in the considered corpus, called corpus speciï¬c words, can be
eliminated. These words are not necessarily very common in the language under
consideration.
Stemming
This method is used to identify the root of a word. The purpose of this method is to
remove various suï¬ƒxes, to reduce the number of words, to have accurately matching
stems, to save time and memory space. It is used as an approximate way for grouping
words with a similar basic meaning together. For example, the words correlate,
correlated, correlating, correlations, all can be stemmed to the stem â€œcorrelateâ€. An
algorithm that performs this step is called stemmer. One common implementation is
the Porter stemmer [66].
8

--- Page Break ---

2.2. Traditional approaches to Natural Language Processing
Others
Other steps that can be done are converting all letters to lower cases, converting
numbers into words or removing them, removing punctuations, removing white
spaces, and expanding abbreviations. All this is done to prepare the text in a better
manageable format for the machine, also reducing the dimensionality.
2.2.2 Encoding Text
Once the text has been tokenized, the tokens obtained must, however, be transformed
into numbers to be feed as input to machine learning models. A naive way to do this
is to assign a diï¬€erent number for each token. For example, using this approach on
the sentence â€œencoding text is a serious thingâ€, the encoding would be:
[encoding,text,is,a,serious,thing] => [5, 2, 3, 1, 6]
Another very similar approach is to use a one-hot encoded representation for all
the words in our vocabulary.
encoding => [1, 0, 0, 0, 0, 0]
text => [0, 1, 0, 0, 0, 0]
is => [0, 0, 1, 0, 0, 0]
a => [0, 0, 0, 1, 0, 0]
serious => [0, 0, 0, 0, 1, 0]
thing => [0, 0, 0, 0, 0, 1]
As we can observe, this representation is sparse since most of the elements are
zero. The two techniques presented before are not good encoders for several reasons:
they do not capture meaning, semantic relationships, similarities between words, and
do not take into consideration the context in which a word appears.
Word embedding is the common name for a set of language modeling and feature
learning techniques in NLPwhere words or phrases from the vocabulary are mapped
to vectors of real numbers. Embedding refers to any technique mapping a word (or
phrase) from its original high-dimensional input space (the body of all words) to a
lower-dimensional numerical vector space. The approaches used for mapping can be
divided into two groups:
1. Frequency-based Embedding;
2. Prediction-based Embedding.
9

--- Page Break ---

Chapter 2. Background
In the ï¬rst category, there are techniques that exploit the frequency of a wordâ€™s
occurrence to extract embeddings. One of these is called TF-IDF, which will be
covered in more detail later.
The second includes various techniques, for the most part, probabilistic models
and neural-based models. They learn embeddings by predicting a word based on
the words present in the context. One of the most relevant neural-based embedding
technique is Word2Vec, developed by a team of researchers at Google [42] in 2013 and
has become the de-facto standard for developing pre-trained word embedding.
2.2.3 TermFrequencyâ€“InverseDocumentFrequency(TF-IDF)
TF-IDF is a statistical measure that is meant to indicate how relevant a word is to
a document in a collection or corpus. It is used as a weighting factor in searches
of information retrieval and text mining applications. The TF-IDF value increases
proportionally to the number of times a word appears in the document and is
counterbalanced by the number of documents in the corpus that contain the considered
word, which helps to compensate for the fact that some words appear more frequently
in general (such as stop words). TF-IDF is calculated with the following formula:
TF IDF (t;d;D ) =tf(t;d)idf(t;D) (2.4)
Wheretindicates the terms, dindicates each document, and Dindicates the
collection of documents.
The ï¬rst term of the formula 2.4, tf(t,d), is called term frequency and represents
the number of times each word appears in each document. The second part idf(t,D)
is called inverse document frequency and represents how frequent or unfrequent a
word is in the entire document set: The closer it is to zero, the more common a word
is. It is calculated as indicated below:
idf(t;D) =log|D|
1 +|fd2D:t2dg|(2.5)
UsingTF-IDF, we can extract features from text and use them for a task as
classiï¬cation or regression. However, the dimensionality (size of the feature set) in
TF-IDF for textual data is the size of the vocabulary across the entire dataset, leading
to a huge computation on weighting all these terms [78]. Usually, not all the features
are used, but it is built a vocabulary that only considers the top features ordered by
term frequency across the corpus.
10

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
2.2.4 Machine Learning Models
Once the text has been transformed into vectors, these can be used for statistical
analysis or as input to machine learning models. Machine learning is used in NLP in
both unsupervised and supervised ways. An example of an unsupervised application
is Latent Dirichlet Allocation ( LDA) topic modeling, whose goal is to identify topics
in a set of documents. Supervised learning is widely applied in several tasks, such
as document classiï¬cation and regression, using diï¬€erent models, including neural
network, random forest, and Support-Vector Machines ( SVM). Random forest is a
supervised learning algorithm that can perform both classiï¬cation and regression. It
was proposed in 1995 [29], and in the following years, it has been improved in several
ways. It is composed of an ensemble of decision trees. The prediction of a decision
tree follows several branches of â€œif-thenâ€ decision splits similar to the branches of a
tree. The endpoint is called a leaf, and it represents the ï¬nal result: a predicted class
(classiï¬cation) or a value (regression). At each branch, the feature threshold that
best split the samples locally is found. A single decision tree usually tends to overï¬t
as its depth increases; random forests reduce the variance and bias by combining
various decision tree models. Random forests train several decision trees on various
subsamples of the dataset and various subsamples of the available features.
The principal hyperparameters in random forests are:
The number of decision trees in the forest usually called estimators.
The max depth of a tree: as the longest path between the root node and the
leaf node.
The minimum number of samples required to split each node.
The minimum number of samples required for each leaf.
The number of features to consider when looking for the best split.
2.3 Deep Learning in Natural Language Processing
The classical machine learning pipeline is typically done using a set of hand-crafted
features that are then fed to a trainable model. A priori knowledge of the data is
required to decide how and which features to extract. One of this approachâ€™s issues is
that the selection of the features may require the need for an expert or may be diï¬ƒcult
to perform and hence suboptimal. Moreover, these extracted features are usually
applicable only to one speciï¬c problem and not transferable. Deep Learning is a
categoryofmachinelearningalgorithmsinwhichthefeatureextractoristrainedtoo, to
learn the best representation of the data so that the features are tailored to the speciï¬c
problem it is required to solve. It uses multiple hidden layers of neurons to extract
low, mid, and high-level features. Deep Learning assumes that it is possible to learn a
hierarchy of descriptors with increasing abstraction. Neural-based models, especially
deep learning ones, have achieved superior results on various language-related tasks
as compared to traditional machine learning models.
11

--- Page Break ---

Chapter 2. Background
2.3.1 Feed Forward Neural Networks
Artiï¬cial neural networks are systems inspired by the biological neurons which allow
humans to learn. The brainâ€™s computational model is distributed among simple
units called neurons; it is intrinsically parallel and redundant and thus fault-tolerant.
The neuron has a main connection with the rest of the network, called the axon,
and other minor connections called dendrites. Neurons are connected through the
synapses, which are the terminations of the dendrites, and neurons exchange charge
through them. The impact on the receiver is diï¬€erent between synapses, so they have
diï¬€erent weights. A neuron cumulates charges, and when it exceeds a threshold level
for the membrane potential, it gets released through the axon (ï¬ring). Hence, the
computation is a highly nonlinear phenomenon because the neuron is almost inactive
until it suddenly spikes.
The basic component of an artiï¬cial neural network, called perceptron, was created
in 1958 by Frank Rosenblatt. The perceptron is a binary linear classiï¬er which
applies a threshold function on the linear combination of the input features. Figure
2.3 describes the components of a perceptron: inputs Xn= [x1;x2;:::;xn], weights
Wn= [w1;w2;:::;wn], the biasb(the threshold) and the activation function f.
x2w2fyx1w1
x3w3b
Figure 2.3. Perceptron.
The function describing the percepetron is the following:
y=f(nX
i=1wixi b) =f(nX
i=0wixi) (2.6)
This formula describes the output of the perceptron; the bias bcan be inglobed
into the summation, also becoming a parameter to be learned.
The training process of a single perceptron occurs through hebbian learning,
introduced by Donald Hebb in his 1949 book The Organization of Behavior [28].
Hebbian learning is summarized in the following rules:
12

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
wk+1
i=wk
i+ wi (2.7)
wk
i=tkxk
i (2.8)
Where:
: learning rate
xi:ithinput at time k
tk: desidered output at time k
It is easily demonstrated that a single perceptron cannot learn a nonlinear separable
space; for example, it cannot learn a XOR function. Hence the idea of combining
several perceptrons together.
The combination of perceptrons in connected layers resulted in a Multi Layer
Perceptron ( MLP), which belongs to the class of Feed Forward Neural Network
(FFNN). InFFNN, the ï¬‚ow of information moves only forward from the input
nodes to the output nodes. There are no cycles or loops in the network. A general
architecture consists of at least three layers of parallel neurons: an input layer, a
hidden layer with a nonlinear activation function at each neuron, and an output layer.
The dimensionality of the input and output layer depends on the problem, while the
shape of the hidden layer, in terms of the number of neurons per hidden layer and the
number of hidden layers, is a design parameter.
Input
layerHidden
layerOutput
layer
x1
x2
x3
x4
x5Ouput
Figure 2.4. AFFNNwith 5 neurons in the input layer, 3 neurons in the hidden layer and 1
neuron in the output layer.
13

--- Page Break ---

Chapter 2. Background
Hebbian learning is not applicable to train this kind of network since we have more
than one neuron. The most commonly used algorithm for FFNNtraining is called
backpropagation. It computes the gradient of the loss function with respect to the
networkâ€™s weights for a sample or a group of samples (batch). Then, it updates the
weights to minimize loss using gradient descent or other variants such as stochastic
gradient descent.
2.3.2 Recurrent Neural Networks
Recurrent Neural Networks ( RNNs) are derived from feedforward neural networks
and, diï¬€erently from FFNN, are able to handle dynamic data, using an internal state
as a memory to process variable-length sequences of inputs. The reason why RNNare
called recurrent neural networks is that the previous outputs are used as inputs for the
current sequence; in this way, computation takes into account historical information.
RNNmodels are commonly used in the ï¬elds of natural language processing and
speech recognition.
Context
layerOutput
Figure 2.5. Elman neural network architecture.
In Figure 2.5 is represented the structure of an Elman network, a category of RNN.
The main diï¬€erence over the FFNNis the context layer, which receives input from,
and returns values to, the hidden layer: the output produced at time taï¬€ects the
parameter available at time t+ 1.
14

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
The training of these kinds of networks is done using an algorithm called back-
propagation through time, which is an extension of the standard backpropagation
algorithm. It performs: i) Unfolding of the RNNforUtime steps, obtaining a FFNN;
ii) backpropagation is applied to the new network.
Vanishing gradient
The vanishing and exploding gradient phenomena, where the backpropagated error
decreases or increases exponentially, are often encountered in the context of RNNs.
The reason why it happens is the multiplicative gradient that can be exponentially
decreasing/increasing to the number of layers. This problem is relevant, especially
inRNN, because they have a large number of layers after the unfolding: the more
the learning process goes into the past, the more the gradient will be close to zero,
independently of the error (vanishing gradient). This is caused by some activation
functions, such as sigmoid or hyperbolic tangent, which have a derivative smaller than
1. By repeatedly multiplying them during the training process, the gradient will go to
zero.
One possible solution to solve the vanishing problem is to use an activation function
called ReLU deï¬ned as follows:
f(x) =ReLU (x) =max(0;x) (2.9)
It has a derivative equal to 1 for x>0and equal to 0 for x<0. In the former
case, the gradient is backpropagated as it is, while in the latter, no gradient is
backpropagated from that point backward.
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.0
x0246810f(x)ReLU
leaky-ReLU
Figure 2.6. Diï¬€erences between ReLU and leaky-ReLU.
15

--- Page Break ---

Chapter 2. Background
The use of ReLUhas a disadvantage: if the weights learned are such that the x
is negative for the entire domain of inputs, the neuron never learns. This is known
as the dying ReLUproblem. The dying ReLUproblem can be tackled by using a
modiï¬ed version of the activation function, called leaky ReLU deï¬ned as follows:
f(x) =max(0:01x;x) (2.10)
LSTM
Another solution to deal with vanishing gradient is to use Long short-term memory
(LSTM) networks [30]: particular types of RNNthat can learn very long sequences.
They have a cell state ct 1, which allows the information to go through it unchanged.
The LSTM can remove or add information to the cell state if needed through structures
called gates. Gates allow to let information through if needed and are composed of a
sigmoid neural net layer and a pointwise multiplication operation. There are three
diï¬€erent gates: the Forget gate decides what information should be thrown away or
kept from the previous step; the Input gate decides what information is relevant to
add from the current step; the Output gate determines what the next hidden state
should be.
The same problem that usually happens to RNNhappens with LSTMas well,
that is when sentences are too long LSTMdo not work well. The reason is that the
probability of keeping the context from a word that is distant from the current word
being processed decreases exponentially with the distance from it. Another issue with
RNN, andLSTM, is that it is diï¬ƒcult to parallelize the processing of sequences since
you have to process word by word.
 Tanh +
 Tanhct 1Cell
ht 1Hidden
xt InputctNext cell
htNext hiddenht Output
Figure 2.7. LSTM structure.
16

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
2.3.3 Preventing Neural Networks from overï¬tting
The training of neural network models is based on a ï¬nite set of training data. During
the training, the modelâ€™s objective is to perform well in predicting the training setâ€™s
observations. In general, however, a machine learning schemeâ€™s goal is to produce a
model that generalizes, that is, that predicts previously unseen samples. Overï¬tting
occurs when a model ï¬ts the training data well while having low performance on
the testing data. Some of the most used techniques to reduce overï¬tting in neural
networks, also referred to as regularization techniques, are described below.
Dropout
Dropout is based on a simple idea. During training, some neurons are randomly
switched oï¬€ or â€œdropped outâ€ with a certain probability preventing neurons from
co-adapting too much. Co-adaptation is when some connections have more predictive
capability than the others. While training, dropout trains diï¬€erent subnetworks,
similar to an ensemble. Dropout can be applied to any hidden layers in the network,
as well to the input layer, but it is not used on the output layer. The dropout is then
not used during the testing (or inference).
Early stopping
The model tries to minimize the loss function on the training data by tuning the
parameters. Overï¬tting networks show a monotone training error trend, but at some
point, they loose generalization. At each epoch, the loss is calculated on a diï¬€erent
data split, called validation set. When the loss of the validation set stops decreasing,
the training is stopped. However, when the validation loss starts to rise, the training
is not stopped immediately but continues for a number (the â€œpatienceâ€) of epochs to
avoid local minima. This simple strategy of stopping early based on the validation set
loss is called Early Stopping.
L1 & L2 regularization
A way to avoid overï¬tting is to regularize the cost function of the neural network. A
complex model is prone to overï¬tting, so to penalize complexity is possible to add to
the loss function another term known as the regularization term.
L1 regularization, also known as Lasso Regularization is deï¬ned as follows:
NX
n=1jwij (2.11)
Whereis a hyperparameter that determines the regularizationâ€™s importance
compared to the loss, and ware the weights. L1 regularization pushes weights to be
17

--- Page Break ---

Chapter 2. Background
zero, producing sparse models, i.e., models where unnecessary features donâ€™t contribute
to predictions.
L2 regularization, also known as Lasso Regularization is deï¬ned as follows:
NX
n=1w2
i (2.12)
L2 doesnâ€™t push the values to be exactly zero, but only close to zero.
2.3.4 Encoder-Decoder architecture
The encoder-decoder architecture is a very general design. The encoder is a neural
network that maps an input space, for example, a sentence, to a latent space. The
decoder is the complementary function that creates a map from the (encoderâ€™s) latent
space to another target space. A task like machine translation is done by mapping
the input space (e.g., a sequence of tokens in English) to another output space (e.g.,
a sequence of tokens in French) and linking them through a shared latent space.
An autoencoder is a particular case of an encoder-decoder where the target is equal
to the input. The encoder and the decoder are trained together. The loss function
is based on computing the delta between the actual and reconstructed input. The
optimizer will try to train both encoder and decoder to lower this reconstruction loss.
This process is completely unsupervised, and no label is needed. Once trained, we
can keep only the encoder part and extract meaningful information from the input.
The encoder-decoder approach was successfully used in machine-translation, and it
is called Sequence to Sequence Learning ( seq2seq) [59]. Originally based on multiple
layers of LSTMto map the input sequence to a ï¬xed vector, and then others LSTM
layers to decode the target sequence from the latent vector. However, a single ï¬xed-size
hidden state becomes an information bottleneck, resulting in performance degradation
when dealing with long sentences. The idea to solve this problem is to introduce a
mechanism called attention, which allows using all the encoderâ€™s hidden states as a
source of information.
18

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
Figure 2.8. seq2seq training process.1
2.3.5 Attention
Attention is a concept that refers to how we, as humans, actively process certain
information in the environment. An example is when we focus on diï¬€erent regions of an
image or when we correlate words in a sentence. Researchers have tried to reproduce
this mechanism to better memorize long source sentences in neural machine translation
(NMT), and have become a fundamental part of sequence modeling, allowing modeling
of dependencies without concern about their distance in the input or output sequences.
It is usually presented combined with seq2seqmodels to explain how it works. This is
because attention has initially been introduced as a solution to address the problem
of capturing long dependence in seq2seqmodels. Attention is successfully applied in
NLP, but also in computer vision: a visual attention mechanism was proposed in 2015
by Xu et al. [68].
Diï¬€erent models of attention exist, and can be grouped into three main categories:
Self-Attention (also called intra-attention).
Global/Local attention.
Hard/Soft attention.
Self-attention relates to diï¬€erent positions of a single sequence to compute a
representation of the same sequence.
Xu et al. [68] have proposed the concept of Hard and soft attention; they present
an attention-based model that automatically learns to describe the content of images
based on this attention. In hard attention, some part of the image is selectively
ignored, while in soft attention, all the image is used, but in an aggregated and
reweighted form.
1https://medium.com/@Aj.Cheng/seq2seq-18a0730d1d77
19

--- Page Break ---

Chapter 2. Background
Global and local attention were introduced in [40]. It is a concept very similar to
hard and soft attention; the global one always attends to all sources of information,
and local uses only a subset at a time. An improvement is that local attention is
diï¬€erentiable everywhere, while this is not true in hard attention.
In the beginning, the attention mechanism was applied to RNN, but this does not
allow for processing inputs in parallel.
2.3.6 Transformers
The Transformer is a deep learning model introduced the ï¬rst time in 2017 in the
paperAttention is all you need [61] and mostly used in NLPtasks. Transformers
are created to handle sequential data, such as natural language, without requiring
the sequential data to be processed in a given order, unlike RNNs. This fact allows
parallelization, reducing training time compared to RNNs, making easier the training
on a large corpus of data. Moreover, Transformers models easily manage long-range
dependencies, thanks to the attention mechanism.
The Transformer is based on an encoder-decoder architecture, as shows Figure
2.9 where the encoder is the block on the left and the decoder on the right. There
are multiple identical encoder-decoder blocks stacked on top of each other. Both the
encoder stack and the decoder stack have the same number of units. The Transformer
is composed of:
Scaled Dot-Product Attention.
Multi-Head Attention.
Position-wise Feed-Forward Networks.
Embeddings and Softmax.
Positional Encoding.
The Transformer relies on self-attention mechanism, more precisely on Scaled
Dot-Product Attention. The encoded representation of the input is a set of key-value
pairs, (K;V ), both of dimension n(the input sequence length), where the keys and
values are the encoder hidden states. In the decoder, the previous output is compressed
into a query ( Qof dimension m) and the next output is produced by mapping this
query and the set of keys and values. The output is a weighted sum of the values,
where the weight assigned to each value is calculated as the dot-product of the query
with all the keys:
Attention (Q;K;V ) =softmax (QKT
pn)V (2.13)
The attention is calculated multiple times, therefore referred to as Multi-head
attention: â€œMulti-head attention allows the model to jointly attend to information from
20

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
diï¬€erent representation subspaces at diï¬€erent positionsâ€ [61]. Then each Multi-head
attention layer is followed by a fully connected layer.
Figure 2.9. The Transformer - model architecture [61].
Another aspect to clarify is how to give information about the order of tokens in
the input sequence since it diï¬€erent compared to RNNs. A position-dependent vector
is added to each word-embedding, called positional encoding, that in the original
paper uses sinusoidal functions.
2.3.7 Pre-trained Models & Transfer Learning
Transfer learning refers to a technique that allows the storing of knowledge obtained
while solving a task and utilizing it for a diï¬€erent but related task. In some domain,
it is challenging to construct a large-scale well-annotated dataset, or to train a model
on a large amount of data could be very expensive. Transfer learning relaxes the
hypothesis that training data must be independent and identically distributed with
the test data [60]. It addresses the problem of lack of training data in a new domain,
21

--- Page Break ---

Chapter 2. Background
transferring knowledge from a pre-existing data pool, and prevents the training of the
model from scratch.
Transfer learning is common in the real world: learning to ride a motorbike might
help to ride a car, or learning a new language might help you learn another. In deep
learning, common transfer learning cases are in image vision and NLP ï¬elds.
An example of a pre-trained deep neural network in computer vision is the
ResNet [27]. It is trained to classify more than a million images. One common source
used to do the pre-training is the ImageNet dataset [14], a large dataset consisting of
1.4M images and 1000 classes. Next, it is possible to exploit the model to do a more
speciï¬c task.
Transfer learning is also being widely used in the ï¬eld of NLP, in the form of
pre-trained language models. These models are, in most cases, trained on a large
corpus of text in an unsupervised manner.
Bidirectional Encoder Representations from Transformers (BERT)
BERTis a deep learning model, developed by Google in 2018, that has given state-
of-the-art results on eleven NLPtasks such as GLUE[63] and Stanford Q/A dataset
(SQuAD) [49].
BERT is a multi-layer bidirectional Transformer. Two versions are presented in
the paper [15]; their size is summarized in the following table.
Model name# parameters
(millions)# layers hidden size attention heads
BERT-large 180 24 1024 16
BERT-base 110 12 768 12
Table 2.1. BERT models comparison.
BERT is pre-trained on two tasks:
1. MLM : 15% of the words in each input sequence is replaced with a [MASK]token.
The goal is to train the model to predict the original value of the masked words,
given the context provided by the other, non-masked, words in the sequence. Of
these 15% tokens: 80% are replaced with a [MASK]token, 10% with a random
word, and 10% use the original word.
2.Next Sentence Prediction ( NSP): The goal here is to teach the model to
understand the relationship between two sentences. The model receives pairs
of sentences as input and learns to predict if the second sentence in the pair
is the following sentence or not in the original document. 50% of the inputs
are a pair in which the second sentence is the following sentence in the original
document, while in the other 50%, a random sentence from the corpus is chosen
as the second sentence.
22

--- Page Break ---

2.3. Deep Learning in Natural Language Processing
There are two steps in BERT: pre-training and ï¬ne-tuning. During the pre-training
part, the model is trained in an unsupervised way on a large corpus. In ï¬ne-tuning,
the pre-trained parameters are used as initialization, and all of the parameters are
ï¬ne-tuned using labeled data from the downstream tasks.
The pre-training of the model is based on a large corpus constituted of the
BooksCorpus (800M words) and English Wikipedia (2,500M words). The tokenizer
used is WordPiece with a 30,000 token vocabulary. BERT relies on randomly masking
and predicting tokens. The original BERT implementation performed masking once
during data preprocessing, also called static masking.
For a given token, its input representation is constructed by three embeddings.
The token embeddings are the vocabulary IDs for each of the tokens, the segment
embeddings is a binary class to distinguish between sentence A and B (0 or 1), and
position embeddings represents the position of each token in the sequence.
The special tokens present are:
[CLS]:theï¬rsttokenofthesequence, itisusedwhendoingsequenceclassiï¬cation
since it provides a representation of all the sentence.
[SEP]: the separator token, which is used when building a sequence from multiple
sequences.
[PAD]: the token used for padding, in the case of sequences of diï¬€erent lengths.
[MASK]: the token used when training this model on the task of masked language
modeling.
BERT model has an excellent performance in all natural language understanding
tasks, but it lacks in language generation. Another limitation is the maximum sequence
length is limited to 512. To deal with longer sequences, a model called Transformer-XL
has been developed [13].
DistilBERT
DistilBERT is a pre-trained model based on BERT[53], developed by Hugginface2. As
claimed by the authors, it has 40% fewer parameters than BERT-base-uncased, runs
60% faster while preserving over 95% of BERTâ€™s performances as measured on the
GLUEbenchmark. Thetechniqueusedtoobtainthislightermodeliscalleddistillation.
Knowledge distillation or teacher-student learning is a compression technique in which
a small model is trained to replicate a larger modelâ€™s behavior.
The number of layers is reduced by taking one layer out of two, leveraging the
common hidden size between student and teacher. Compared to the original BERT,
in DistilBERT have been removed the token-type embeddings and the pooler (used
for the next sentence classiï¬cation task) since the next sentence prediction objective
is eliminated. Unlike what was done in the implementation of BERT, A dynamic
masking has been used: each time a sequence is fed to the Transformer will have
diï¬€erent [MASK]tokens.
2https://huggingface.co/
23

--- Page Break ---

Chapter 2. Background
Model name# parameters
(millions)inference time
(seconds)
ELMo 180 895
BERT-base 110 668
distilBERT 66 410
Table 2.2. Inference time of a full pass of GLUEtask STS-B (sentimental analysis) on CPU
with a batch size of 1 [53].
As we can see from Table 2.2, a lighter model like DistilBERT is cheaper to
pre-train while keeping good capabilities, being useful for on-device computations in
a proof-of-concept experiment.
24

--- Page Break ---

Chapter 3
Related Works
This chapter provides an overview of the literature relevant to this work. Section
3.1 introduces research about Knowledge Tracing ( KT), to motivate the choice of
Item Response Theory ( IRT) as the technique to model studentsâ€™ skills. Section 3.2
describes works focusing on questions diï¬ƒculty estimation from text. Lastly, Section
3.3 illustrates various domain-speciï¬c pre-trained models, since our approach is also
based on a pre-traning on Masked Language Modeling (MLM).
3.1 Knowledge Tracing (KT)
KTis the task of modeling studentsâ€™ knowledge over time, allowing us to predict
whether students will give correct answers to questions. Estimating studentsâ€™ knowl-
edge allows teachers to understand better their studentsâ€™ attainment levels, which
can help improve the quality of learning material and provide personalized support.
Recent surveys report [2,45] that KTis most commonly performed using logistic
models [8,9,65] (e.g., dynamic IRT) or neural networks. The primary information
used for learner modeling is the history of the scores (i.e., correctness) of studentsâ€™
answers. Still, there are other potentially useful sources of information, such as the
history of attempts, response times, question topics, and the use of hints.
Bayesian Knowledge Tracing ( BKT)â€”introduced for the ï¬rst time in [12]â€”models
a learnerâ€™s latent knowledge state as a set of binary variables (known/unknown),
each of which represents understanding or non-understanding of a single concept (i.e.,
topic). The process of learning is modeled by a discrete transition from an unknown
to a known state. Many assumptions adopted by the BKT model, as the binary
knowledge state, are considered too simplistic. The skill mastery process is complex,
and a simple binary variable might not be enough to model it.
Recent works tried to build KTmodels using neural networks, introduced for the
ï¬rst time in [47]. These models, often referred to as Deep Knowledge Tracing (DKT),
seemtooutperformlogisticmodelsinpredictingthecorrectnessofunobservedstudentsâ€™
answers [1,11,76,77], but not all works agree on this point [16,41,67,75]. However,
the quality of KTis not merely based on the prediction of studentsâ€™ performance. As
it is often the case with neural models, DKT models lack explainability; some works
have been done to give a better understanding of the model [37,74]. The ï¬rst work
introduces a concept of probabilistic skill similarity, while the latter leverages a hybrid
25

--- Page Break ---

Chapter 3. Related Works
system composed of neural network and IRT. However, they do not reach the same
intuitiveness as logistic models.
The comparison of diï¬€erent models for the KT is of relevance to our work. In
fact, through these models, we estimate the diï¬ƒculty of the questions, that is our
ground truth. Logistics models are still used in both literature and industry; plus,
they are easy to interpret. For this reason, we decided to use a 1PL IRT model, which
associates a skill level for each student and a diï¬ƒculty level to each question.
3.2 NLP for Diï¬ƒculty Prediction
In the literature, there are works [17,21,73] related to the prediction of the readability
of questions alone. Diï¬ƒculty and readability are not the same concepts; however, the
latter can be used as a feature for estimating the diï¬ƒculty.
Huang et al. [31] propose a technique to predict the diï¬ƒculty in Standard Tests
such as TOELF or SAT, called Test-aware Attention-based Convolutional Neural
Network. Text encoding is done through Word2Vec; then, there is a CNN layer,
an attention layer, and ï¬nally, a prediction layer. In reading tests, the answers to
questions can be inferred from the given passages. The task in these cases is diï¬€erent
because the answer can be deduced from the text of the question, as the goal is to
measure how diï¬ƒcult it is to infer the response from the passage.
Some works have been done to assess the diï¬ƒculty of automatically generated
questions, using bag of words and measuring the degree of similarity between the key
(correct choices) and the distractors (incorrect choices) [3,35,72].
The estimate of the diï¬ƒculty of a question also involved the area of Community
Question Answering ( CQA). Research in this area was carried out by Liu et al.,
which propose in [38] a competition-based model for estimating question diï¬ƒculty by
leveraging pairwise comparisons between questions and users. Their dataset comes
from Stack Overï¬‚ow1and includes questions with diï¬€erent topics (programming,
mathematics, andEnglish). Themodelproposedsigniï¬cantlyoutperformstheprevious
PageRank-based approach. Although their model does not use text as an input feature,
they demonstrate that diï¬€erent words or tags in the question descriptions indicate
question diï¬ƒculty levels, opening the possibility of predicting question diï¬ƒculty from
text.
Online Judge ( OJ) systems are designed for self-directed learning without interven-
tions of teachers. These systems are widespread in the programming domain, oï¬€ering a
real-time automatic assessment of usersâ€™ solutions. Platforms such as Codeforces2oï¬€er
diï¬€erent programming problems divided by diï¬ƒculty levels. Categorizing problems
based on diï¬ƒculty is crucial to help a novice user to approach programming and
providing them recommendations. Research has been carried out in this ï¬eld, both
for topic modeling [32,79] and prediction of diï¬ƒculty [32], but the latter does not use
text as a feature.
In the next lines, we describe models for question calibration in education. In
order to compare our model with others present in the literature, we ï¬rst need to
1https://stackoverflow.com/
2https://codeforces.com/
26

--- Page Break ---

3.2. NLP for Diï¬ƒculty Prediction
analyze the concept of diï¬ƒculty, i.e., the ground truth, used in various works. Three
types can be identiï¬ed:
1. CTT diï¬ƒculty [48,70,71];
2. IRT diï¬ƒculty [6,48];
3. Category-based diï¬ƒculty [19].
In most of the papers, the diï¬ƒculty of a question is estimated using CTT; therefore,
thecorrectness (i.e.,p-value) or thewrongness is used. Using the p-valueas diï¬ƒculty,
while simple to calculate, is less accurate than IRT, as it assumes that all students
have the same skill level. The diï¬ƒculty is not the only latent traits that have been
tried to predict; indeed, Benedetto et al. [6] proposed a model for the prediction
of both diï¬ƒculty and discrimination. In order to estimate the discrimination, it is
necessary to ï¬t a 2PLmodel, which requires more interactions than a 1PLmodel.
However, requiring a large number of interactions per question leads to a reduction
in the dataset for QDEfrom text, as questions with few interactions are discarded.
A large dataset of questions usually allows for obtaining better performance and
reliability.
Figure 3.1. Structure of R2DE, from the input question to the estimated latent traits.
27

--- Page Break ---

Chapter 3. Related Works
Benedetto et al. proposed R2DE [6], a regressor model that estimates IRTdiï¬ƒculty
and discrimination of MCQby looking at the text of the item and the text of the
possible choices. It is trained and validated on a large scale dataset coming from an
e-learning platform. As Figure 3.1 shows, the model uses TF-IDF to extract features
from the text, a simple method but supported by the fact that keywords can indicate
the diï¬ƒculty of a question. The best model reported uses random forests as regressor
and shows how the input question plus correct choice(s) and question plus all choices
lead to comparable results, while the question stem alone leads to lower performance.
The model was improved in a follow-up work [5], introducing new readability and
linguistic features.
In the literature, there is a particular interest in diï¬ƒculty prediction in the medical
ï¬eld. Qiu et al. in Question Diï¬ƒculty Prediction for Multiple Choice Problems in
Medical Exams [48] propose a â€œDocument enhanced Attention based neural Network
(DAN)â€ framework to predict the diï¬ƒculty of multiple-choice problems in medical
exams. The results are compared using two diï¬ƒculties: one deï¬ned as in IRTand one
described as the proportion of incorrect answers, i.e. the wrongness (1 p-value). One
note is that they considered questions with at least ten interactions. This number of
interactions might not be suï¬ƒcient to obtain a valid ground of truth, especially for a
1PL model. The encoding is done using Bi-LSTM, and then there are three major
steps:
1.the question stem and possible choices are enriched using relevant medical
documents;
2.the question diï¬ƒculty is divided into two components: the hardness for recalling
the knowledge assessed by the question and the confusion degree to exclude
distractors; for each part, there is an attention layer;
3. the two components are combined to predict the total diï¬ƒculty.
The model shows excellent performance compared to the baselines used. Still,
this solution relies on a large quantity of unstructured medical materials: around two
million published papers and 500 textbooks. The additional corpus is used to enrich
questions and to recall the knowledge assessed by the question.
Yaneva et al. in Predicting the Diï¬ƒculty of Multiple Choice Questions in a
High-stakes Medical Exam [71] propose a model to perform QDEfrom text in the
medical ï¬eld. They used a real-world dataset from a high-stakes medical licensing
exam United States Medical Licensing Examination, utilizing p-valueas ground truth
diï¬ƒculty. Diï¬€erent classes of features were developed: linguistic features (e.g., lexical,
syntactic, semantic), Information Retrieval ( IR) features, and two embeddings (ELMo
and Word2Vec). These embeddings were obtained using around 20 million medical
abstracts. Best results were achieved when combining all these features, showing a
statistically signiï¬cant improvement over the majority baseline.
Using a dataset of the same domain, Xue et al. [70] use transfer learning to predict
the diï¬ƒculty. In particular, an ELMo network was trained ï¬rstly for the prediction of
response time, then used for diï¬ƒculty prediction, and vice versa. They utilized three
diï¬€erent ELMo conï¬gurations (small, middle, and original) and various combinations
of inputs: stem only, options only, or a combination of both. The results indicate
28

--- Page Break ---

3.3. Domain-speciï¬c Pre-training
that transfer learning can be applied to improve the prediction of questions diï¬ƒculty
when response time is used as pretraining but not vice-versa. Also, diï¬ƒculty was
best predicted using only the item stem, and the only baseline used is the majority
baseline.
Fang et al. [19] propose a novel Bayesian inference-based Exercise Diï¬ƒculty
Prediction (BEDP) framework to predict the diï¬ƒculty of visual-textual exercises. The
representation of images is obtained from a Residual Network (ResNet), while texts are
embedded using BERT. Then a Bayesian inference-based Softmax Regression classiï¬er
to predict the diï¬ƒculty of the exercise. Two datasets were used, one containing
mathematics exercises and the other medicine exercises. The medical dataset was
manually labeled. In both cases, the diï¬ƒculty is not a continuous value but is
represented in categories.
The models used in the literature are validated using private datasets. Questions
used in real tests are not publishable for obvious reasons, except when no longer
used. Private datasets and the fact that the concept of diï¬ƒculty is not unique makes
it challenging to compare results between models. From this point of view, CQA
provides a large pool of public questions. The interest of recent years in Question
Answering has also increased the availability of MCQdatasets, such as the datasets
oï¬€ered by AI23. These, however, do not have a label that represents the diï¬ƒculty,
but they could be used for a transfer learning or unsupervised training approach.
The model proposed in this thesis investigates the use of Transformers models
to perform Question Diï¬ƒculty Estimation ( QDE) from text, using IRTas ground
truth diï¬ƒculty. Transformers-based models have achieved brilliant performances in
many NLP tasks. For this reason, we decided to apply them for the estimation of
the diï¬ƒculty since the previous models use less modern techniques, such as TF-IDF
or ELMo. Furthermore, we propose a solution that does not depend on additional
textual material but can leverage it if available. Indeed we explored two diï¬€erent
approaches: the ï¬rst uses only the text of the questions, the second also uses external
textual information to improve the performance. Our model is evaluated on two
datasets coming from diï¬€erent sources, one public and one private.
3.3 Domain-speciï¬c Pre-training
Pre-trained models, such as BERT, are trained on a general domain corpus and can
be then ï¬ne-tuned on diï¬€erent downstream tasks. However, if the downstream task
concerns a diï¬€erent domain with a diï¬€erent vocabulary, the results might not be
optimal. Two solutions to overcome this problem are: i) pre-training from scratch
the Transformer model with a new vocabulary; ii) pre-training starting from the
weights of the original model using the same vocabulary (referred to as continual
pre-training [23]).
The scientiï¬c community is moving to share models pre-trained in diï¬€erent domains
sincethesearecostlytotrain. Thefollowingexamplesshowdomain-speciï¬cpre-trained
models. SCIBERT [4], is pre-trained on a large multi-domain corpus of scientiï¬c
publications (3.17B tokens) to improve performance on downstream scientiï¬c tasks.
They did two experiments: i) training BERT starting from its original vocabulary and
3https://allenai.org/data
29

--- Page Break ---

Chapter 3. Related Works
weights; ii) training BERT from scratch with a new vocabulary built on the new corpus.
Both the models performed better than the original BERT, showing that while an
in-domain vocabulary is helpful, their model beneï¬ts most from the scientiï¬c corpus
pre-training. BioBERT [36] is another BERT-base model pre-trained on large-scale
biomedical corpora. They show that pre-training BERT on biomedical corpora helps
it to understand biomedical texts. Even if they used a huge corpus (18B words), they
adopted the original vocabulary of BERT-base for two reasons: (i) compatibility of
BioBERT with BERT; (ii) with the WordPiece tokenization, any new biomedical
words can still be represented by subwords that are in the vocabulary of the original
BERT. A novel work [23] from Microsoft research team shows that domain-speciï¬c
pre-training from scratch can signiï¬cantly outperform continual pre-training models
like BioBERT. Sun et al. [58]. report that Within-task pre-training, i.e., when the
pre-training corpus is the target taskâ€™s training data, improves the performance on
the tasked task.
Our additional corpus is 100 times less than the one used in BERT and DistilBERT.
For this reason, we could not train the model from scratch. However, using a new
vocabulary is not essential to have an increase in performance over the original.
30

--- Page Break ---

Chapter 4
Models
This chapter describes how the two pre-trained Transformers models, BERT and
DistilBERT, are trained with two diï¬€erent approaches. Section 4.1 illustrates the
diï¬€erences between the two pre-trained language models, Section 4.2 describes the
additional pre-training on Masked Language Modeling ( MLM), and Section 4.3 the
ï¬ne-tuning on Question Diï¬ƒculty Estimation (QDE) from text.
QDEconsists in predicting the diï¬ƒculty of a question from text. The task
performed is a regression: the input is a sequence of words representing the question,
and the target is a real number representing the diï¬ƒculty. We apply pre-trained
language models to this regression task by adding a linear regressor layer. Then, we
ï¬ne-tune the parameters of the Transformers model and the regressor layer jointly.
In addition to that, we experiment with the possibility of further pre-training the
pre-trained Language models on a corpus speciï¬c to the ï¬ne-tuning task domain. The
pre-trained weights are then used to perform the ï¬ne-tuning on the regression task.
Figure 4.1 shows our general approach; the dotted line is the solution that does not
leverage the further pre-training, while the continuous line is the one that requires it.
PRE-TRAINED MODEL
FINE-TUNING (REGRESSION)FURTHER
PRE-TRAINING (MLM)
Figure 4.1. The two diï¬€erent approaches: the dotted line represents the approach which
performs only ï¬ne-tuning for QDEfrom text, the continuous line is the approach
with the additional pre-training on MLM.
31

--- Page Break ---

Chapter 4. Models
4.1 BERT and distilBERT
BERT and DistilBERT are two very similar pre-trained models; in fact, DistilBERT
is a Transformer model trained by distilling BERT-base , with which it shares almost
all the architecture. Table 4.1 shows the size of the two models. DistilBERT has
fewer parameters, which makes it lighter and faster to train. As we can see, the
diï¬€erence lies in the number of layers, which leads to a smaller number of parameters
for DistilBERT.
Model name# parameters
(millions)# layers hidden size attention heads
BERT-base 110 12 768 12
distilBERT 66 6 768 12
Table 4.1. BERT and distilBERT size comparison.
BERT and distilBERT are pre-trained on a general corpus of 3,300 million words:
the Toronto BooksCorpus (800M words) and English Wikipedia (2,500M words).
The Wikipedia texts are pre-processed to ignore lists, tables, and headers. Both the
models address the problem of out-of-vocabulary words using a variant of Byte Pair
Encoding ( BPE) [55] named WordPiece [54]. The vocabulary of the two pre-trained
models is identical and has the same pre-speciï¬ed size of 30,522 tokens. Moreover,
both models use the same special tokens ( [PAD],[MASK],[CLS],[SEP]). We consider
the uncased version.
The maximum sequence length supported by the models is 512. For our experi-
ments, we set it to 128 or to 256, depending on the input. Only the ï¬rst 128 or 256 are
considered for longer sequences, while shorter ones are padded with the special token
[PAD]. This design choice is based on the analysis of our datasets, as the questions
are short.
4.2 Further pre-training on MLM
The original BERT is trained on two objectives: Masked Language Modeling ( MLM)
and Next Sentence Prediction ( NSP), while DistilBERT only on the ï¬rst. We decide to
pre-train both models only on MLM, for consistency and also because NSPimproves
only slightly downstream task performance [39].
32

--- Page Break ---

4.2. Further pre-training on MLM
0.14
Pre-trained	T ransformer	modelClassification	layer
[CLS] this is[MASK] simple example [SEP]... ... 0.01 0.07 0.13 Softmax	outputsa Predicted	token
Figure 4.2. Additional pre-training on Masked Language Modeling (MLM).
InMLMpre-training, one of the input words is substituted by the special token
[MASK], and the model is asked to predict (using the contextual embedding of the
surrounding tokens) the word that was masked. Figure 4.2 shows a high-level view of
masked training. The [MASK]token is predicted by feeding the embedding output (the
ï¬nal hidden vectors corresponding to the mask tokens) into a fully connected layer.
The number of output neurons is equal to the number of tokens in the vocabulary;
each tokenâ€™s probability is calculated with softmax.
Our corpus is 100 times smaller than the one used in BERT. For this reason, we
cannot train the model from scratch. We use the original weights as initialization
forMLMpre-training and the same original vocabulary as BERT. Creating a new
vocabulary based on our corpus would require training from scratch. Before training,
we mask each sequence as in the original BERT implementation: 15% of the sequenceâ€™s
tokens are masked; of those, 80% is masked with [MASK], 10% is set to a random
token, and 10% is just left as it is. The special tokens are never masked nor used as
random tokens.
The following shows an example of masking:
Input = [CLS] this is an [MASK] of masking [SEP]
Label = example
After passing through the dense layer, the ï¬nal hidden vectors corresponding to
the[MASK]tokens are fed into an output softmax over the vocabulary. The MLM
33

--- Page Break ---

Chapter 4. Models
objective is a cross-entropy loss on predicting the masked tokens. The cross-entropy
loss is deï¬ned as follows:
loss=KX
k=1(yklog(^yk)) (4.1)
Where ^yis the predicted probability for the class k,ybinary indicator (0 or 1) if
class labelkis the correct classiï¬cation. The number of classes is equal to the size of
the vocabulary.
4.3 Fine-tuning on Question Diï¬ƒculty Estimation
The ï¬ne-tuning of BERT and DistilBERT is straightforward since the self-attention
mechanism in the Transformer allows modeling many downstream tasks. Figure 4.3
shows a high-level view of the ï¬ne-tuning process of a Transformer model.
Pre-trained	T ransformer	modelRegression	layer
[CLS] find the radius in cm [SEP]-	1.2 Predicted	value
Figure 4.3. Fine-tuning.
It is done as follows. We obtain a ï¬xed-dimensional representation of the input
sequence using the ï¬nal hidden state (i.e., the output of the Transformer) of the [CLS]
34

--- Page Break ---

4.3. Fine-tuning on Question Diï¬ƒculty Estimation
token, denoted as C2IRH, that represent the sequence. Then we add a linear layer,
whose parameters matrix has dimension 2IRKH, whereKis the number of output
neurons (K= 1, for us). Finally, the prediction is the output O=linear (CWT). In
our case, since we are performing regression, the activation function is linear.
The pre-trained model and the linear layer are linked as described in the BERT
paper: â€œThe ï¬rst token of every sequence is always a special classiï¬cation token
([CLS]). The ï¬nal hidden state corresponding to this token is used as the aggregate
sequence representation for classiï¬cation tasks.â€ [15]. The [CLS]token is the best
representation when we ï¬ne-tune all the parameters of the model, like in this case.
If we wanted to use BERT as a feature-based extractor (i.e., when the Transformer
model is frozen), this would not be valid anymore. In fact, the authors show that
the best choice is to concatenate the token representations from the top four hidden
layers. The same observations are valid for DistilBERT.
[CLS] this is the input [SEP]LA YER	1LA YER	12
H	=	768L	=	12
SEQUENCE	LENGTH	=	128LA YER	2LA YER	1 1
...
Output
...
Figure 4.4. Fine-tuning model architecture of BERT in detail.
Figure 4.4 shows more in detail the ï¬ne-tuning of our model, speciï¬cally the BERT
version. As we can see, the embedding of the last hidden layer of [CLS]is fed to
a linear regressor layer. The only diï¬€erence in the DistilBERT version is that the
number of Transformer blocks (L) is 6 instead of 12.
For ï¬ne-tuning, we use Mean Squared Error ( MSE) as loss function. MSEis a
widely used loss in regression cases, and it is diï¬€erentiable. The choice of MSEas loss
function is justiï¬able from a probabilistic point of view as a Maximum Likelihood
35

--- Page Break ---

Chapter 4. Models
Estimation ( MLE) procedure (under the assumption data are sampled from a normal
distribution) [22].
It is calculated as the average of the squared diï¬€erences between the predicted ^y
and observed yvalues:
MSE =1
nnX
i=1(yi ^yi)2(4.2)
36

--- Page Break ---

Chapter 5
Experimental Datasets
This chapter introduces the two data collections used for the experiments as well all pre-
processing done to create the ï¬nal dataset. Section 5.1 presents the publicly available
data collection provided by ASSISTments1, while Section 5.2 the CloudAcademy2
data collection.
Both data collections are composed of two datasets: i) the Interactions (I) dataset
and ii) the Questions (Q) dataset. The Interactions dataset stores studentsâ€™ answers
to questions. Each row provides (at least): the user id, the question id, the correctness
of the answer, and the interactionâ€™s timestamp. The Interactions dataset is used to
estimate (with IRT) the diï¬ƒculty of each question, which will be used as ground
truth while training the model that performs diï¬ƒculty estimation from text. The
Questions dataset contains the textual information about the items. For each question,
it provides the question id, the text, and, for CloudAcademy data, the text of the
possible choices.
An additional dataset referred to as Lectures, is provided by Cloud Academy. It
contains transcripts of the IT technologies lectures related to the questions and can
be leveraged by our model to estimate the diï¬ƒculty of questions more accurately.
5.1 ASSISTments Dataset
ASSISTments is an online intelligent tutoring system, developed by the Worcester
Polytechnic Institute, that assists students and teachers. The platform provides
teachers and administrators with contents from open educational resources but also the
possibility to build and share their own questions. Students can complete assignments
using the ASSISTments web-platform, receiving instant feedback. Teachers also
receive reports on studentsâ€™ progress and class performance. This platform is a generic
system that can be applied to any subject, although it oï¬€ers mostly math content.
5.1.1 Interactions data
TheInteractions dataset was published in [20] and contains data related to the school
year 2012-2013. This dataset has been used in several works, such as [56,64,69],
1https://new.assistments.org/
2https://cloudacademy.com/
37

--- Page Break ---

Chapter 5. Experimental Datasets
mostly about KT.
This dataset contains 6,123,270 interactions between users and questions. Each
interaction has 35 attributes; the most relevant to our work are described below:
problem_id : indicates the id of the problem.
user_id: indicates the unique identiï¬er of the student.
start_time : is a timestamp when a student start a problem.
start_time : is a timestamp when a student submits the answer to a problem.
problem_type : identiï¬es the typology of the problem.
original: identiï¬es if a problem is the main problem or a scaï¬€olding.
correct: correctness of the answer.
template_id : identiï¬es a template; questions from the same template are very
similar.
skill: indicates the skill associated with the problem.
correct Thecorrectattribute indicates the correctness of the studentâ€™s answer. It
is equal to 1 if the student answers correctly on the ï¬rst attempt, otherwise, it is
0. Decimal values are calculated as a partial credit based on the number of hints
and attempts needed to solve. Table 5.1 shows the distribution of correct values; the
overall correctness is 67.64%, and there are few partial scores.
Correct Count Percentage
1 4,141,564 67.64%
0 1,976,383 32.28%
Decimal values 5,323 0.08%
Total 6,123,270 100%
Table 5.1. Distribution of scores.
problem_type As Table 5.2 shows, there are six diï¬€erent types of problems:
algebra: math evaluated string (text box);
choose_1 : MCQ with one correct choice (radio buttons);
ï¬ll_in: simple string-compared answer (text box);
38

--- Page Break ---

5.1. ASSISTments Dataset
open_response : open response question (text box);
choose_n : MCQ with multiple correct choices (radio buttons);
rank: rank multiple objects.
99% of the interactions are originated by the ï¬rst three categories.
Problem Type Count Percentage
algebra 3,500,688 57.17%
choose_1 1,847,657 30.17%
ï¬ll_in_1 742,960 12.13%
open_response 17,642 0.29%
choose_n 11,597 0.19%
rank 2,726 0.05%
Total 6,123,270 100%
Table 5.2. Types of problems.
original Some problems can be broken down into simpler subproblems called
â€œscaï¬€oldingâ€. When a student cannot solve the main problem, he can decide to solve
the scaï¬€oldings. The originalattribute identiï¬es a main problem ( original= 1) from
a scaï¬€olding ( original = 0). If a problem has no scaï¬€olding, it is marked as a main
problem. The main problem and its associated scaï¬€olding have a diï¬€erent problem_id ,
but same template_id . As Figure 5.3 shows the interactions are mostly with main
problems; looking at the number of problems: 84% of the problems are main, while
16% are scaï¬€olding.
Original Count Percentage
main problem 5,819,737 95%
scaï¬€olding problem 303,533 5%
Total 6,123,270 100%
Table 5.3. Main and scaï¬€olding problems.
39

--- Page Break ---

Chapter 5. Experimental Datasets
start_time and end_time Each student-problem interaction has two associated
timestamps: start_time indicates the beginning, end_time indicates the end. The ï¬rst
interaction of the dataset is dated 2012-09-01, while the last one is dated 2013-08-31.
The days passed from the ï¬rst to the last interaction of each student were computed.
The average is 73 days with a standard deviation of 99 days. Figure 5.1 shows the
distribution of the days passed on the platform; as we can notice, there is a peak of
students who have been on the platform only a few days.
0 50 100 150 200 250 300 350
Days0.000.010.020.030.040.050.06Densitystudents
Figure 5.1. Distribution of days between the ï¬rst and last interaction on ASSISTments.
skillThe attribute skillrefers to the skill of a problem, but in 72% of the cases, its
value isnull. In total, there are 179 diï¬€erent skills.
template_id Eachproblem_id hasassociatedonlyone template_id . Atemplate_id ,
on the other hand, can have several problems associated with it, which are very similar
to each other. In order to create a large number of problems, it is common for content
creators to make a few templates and substitute diï¬€erent numbers and keywords in
the problems and answers. An example of two diï¬€erent problems generated from the
same template is the following:
If a new jacket sells for $34 , ï¬nd the total cost if you were charged 5% sales tax.
If a new shirt sells for $36 , ï¬nd the total cost if you were charged 6% sales tax.
A note should be made about scaï¬€olding problems: although they have the same
template as the main problem, they are diï¬€erent. The choice of the identiï¬er to
use to distinguish the problems, template_id orproblem_id , is important. Problems
40

--- Page Break ---

5.1. ASSISTments Dataset
generated by the same template_id have very similar text and are created to have
the same level of diï¬ƒculty.
We wanted to check if the diï¬ƒculty between problems generated by the same
templateissimilar, andweveriï¬editbycomparingthecorrectnessasanapproximation
of the diï¬ƒculty. First, we calculated the correctness of original problems with at
least 50 interactions to get a more accurate estimate. Then we calculate the standard
deviation of correctness for each template with at least 10 problems. Finally, we
computed the mean of all the standard deviations resulting in 0.078, which represents
how the correctness varies for problems of the same template. This result is small,
considering a range of correctness that is between 0 and 1. Besides, we also compared
it with the standard deviation between diï¬€erent templates. First, we kept original
problems with at least 50 interactions. Then we calculated the correctness per template
and then the standard deviation of correctness between diï¬€erent template, that is
0.194. As we can see, the standard deviation for problems of the same template is very
small compared to the standard deviation between diï¬€erent templates. Supported by
these reasons, it was decided to use the template_id as item identiï¬er.
Pre-processing We used the Interactions dataset to obtain each item diï¬ƒculty
usingIRT, which is the ground truth. It must be suitably pre-processed to get an
accurate estimate, and we proceeded as follows. For each student-item pair, we
considered only the ï¬rst answer in chronological order (i.e., the ï¬rst attempt) because
if a student has already seen a question, he will be more advantaged to answer correctly,
inï¬‚uencing the IRTestimate. We kept only items with at least 50 interactions. The
more interactions an item has, the more accurate is the estimate of diï¬ƒculty. However,
considering a minimum threshold of interactions reduces the number of items. A low
number of items means a small dataset that is used to train and test our model. This
has some disadvantages, such as increased overï¬tting. Therefore, we tried to ï¬nd a
fair trade-oï¬€ between the goodness of the estimate and the number of items.
Thecorrectlabel also allows partial credits, which is why it has been converted,
as suggested by ASSISTments, to a binary variable using the formula: 1 =correct,
<1 =incorrect . Table 5.4 compares the dimensionality of the dataset before and
after pre-processing.
Raw dataset After pre-processing
# interactions 6,123,270 2,820,051
# users 46,674 43,868
# problems 179,999 55,178
# templates 96,403 18,659
Table 5.4. Interactions dataset dimensionality.
41

--- Page Break ---

Chapter 5. Experimental Datasets
# interactions iPercentage of items
50 <=i<= 100 68.03%
100 <i<= 200 19.54%
200 <i<= 500 8.15%
i> 500 4.29%
Total 100%
Table 5.5. Interactions per item after pre-processing.
Table 5.5 shows the distribution of the number of interactions per question. On
average, each question has 151 interactions (i.e., student responses) with a standard
deviation of 303. On average, each student answered 64 diï¬€erent questions with a
standard deviation of 113.
Figure 5.2 and 5.3 show the distribution of items and students per correctness after
the pre-processing. In the ï¬rst case, the distribution follows a log-normal shape with
a negative skew. In the second case, the distribution follows a gaussian shape; there
are peaks for values 0 and 1, which can be caused by students with few interactions.
0.00 0.25 0.50 0.75 1.00
Correctness0.00.51.01.52.0Densityitems
Figure 5.2. ASSISTments, distribution of items per correctness.
42

--- Page Break ---

5.1. ASSISTments Dataset
0.00 0.25 0.50 0.75 1.00
Correctness0.00.51.01.52.0Densitystudents
Figure 5.3. ASSISTments, distribution of students per correctness.
5.1.2 Questions data
TheQuestions dataset of ASSISTments contains the textual information of questions
(also referred to as problems). Some research works [43,44,57] have used this dataset
to predict the skill associated with each question. The attributes relevant to our work
are:
problem_id : indicates the id of the problem.
body: contains the textual information of the problem.
The dataset is composed of 179,950 problem_id . Several texts are duplicates: 138,084
bodyover 179,950 are unique. Other information, such as the attributes of the
Interaction data, can be obtained merging this dataset with the Interaction dataset
onproblem_id . Question texts do not always provide complete information about
the request. In fact, they sometimes refer to images, tables, or graphics that are not
present in the text: 9.2% of the items in the raw dataset refers to an external image.
Furthermore, in the case of MCQ problem typology, only the stem is present and not
the choices.
First of all, the texts of problems that are not in the Interactions dataset are
excluded (i.e., all those with an insuï¬ƒcient number of interactions are excluded). Then
we performed a speciï¬c data pre-processing divided into two steps: i) text cleaning, ii)
text elimination. In the ï¬rst step, the texts were only transformed and not completely
removed. HTML tags, URLs, newlines have been removed. There are texts with
references to the exercise number or external books (e.g., â€œPage 2â€, â€œQuestion #2â€).
Because they are not textual information and might increase the risk of overï¬tting,
we tried to remove them. The following is an example of text before and after the
cleaning step:
Raw string:
<p>Convert <span style="color: #45818e;">0.2</span> into a <strong id="p:w5"><span
43

--- Page Break ---

Chapter 5. Experimental Datasets
style="color: #674ea7;">fraction</span></strong>.&nbsp; You must simplify your
answer to lowest terms.<br id="yct810" /></p>
n<p>&nbsp;</p>
Pre-processed string:
Convert 0.2 into a fraction. You must simplify your answer to lowest terms.
Subsequently, we manually searched unsuitable problem patterns. We identiï¬ed
three categories of these problems: i) system messages (e.g., info on whether the
question is correct or not, how many attempts are available); ii) problems referring
to external books; iii) problems where all the text of the questions is in the image.
Table 5.6 shows some examples of unusable problems.
Text Problem type
â€œSorry, that is incorrect. Letâ€™s go to the next question!â€ choose_1
â€œSubmit your answer from the textbook.â€ algebra
â€œEarth Science QUESTION 10â€ choose_1
â€œProblem 6â€ ï¬ll_in_1
Table 5.6. Examples of unusable problems.
We removed problems with less than two words and also problems with the
same text but with a diï¬€erent template since most of the times are unusable. Each
template_id has several very similar problem texts associated with it, as described
above. We decided to keep a text for each template_id . This is done to increase
generalization; the presence of very similar texts in training and in the test would
falsify the results since the same template problems have a similar diï¬ƒculty. Another
option is to split test and train with diï¬€erent template_id . However, some template_id
have several dozen texts associated, while others are one. This would have created
problems during the training and evaluation of the results. Patikorn et al. in [44]
used a dataset from the same platform ASSISTments, and they showed that keeping
problems from the same template is the cause of overï¬tting in the skill-tagging problem.
Their solution was to keep one problem per template_id .
Raw Merged Cleaned Final
# unique texts 138,084 58,320 52,156 11,393
# template_id 125,264 18,883 12,993 11,393
# problem_id 179,950 47,898 41,254 11,393
Table 5.7. Questions dataset dimensionality through pre-processing steps.
44

--- Page Break ---

5.2. Cloud Academy Dataset
Table 5.7 shows the dimensionality of the dataset through the various steps: i)
raw: the raw dataset; ii) merged: after keeping items with at least 50 interactions;
iii)cleaned: after the text pre-processing; iv) ï¬nal: after keeping only one text per
template. The ï¬nal dataset used for the prediction of diï¬ƒculty from text is composed
of 11,393 items.
Length (# words) Percentage of items After pre-processing
len < 2 5.31% 0%
2 < len <= 10 44.54% 10.66%
10 < len <= 50 37.15% 74.38%
50 < len <= 100 9.67% 13.97%
len > 100 3.33% 0.99%
Total 100% 100%
Table 5.8. Distribution of problems per length.
Table 5.8 shows the distribution of Questions dataset before and after pre-
processing.
5.2 Cloud Academy Dataset
Cloud Academy is an e-learning provider oï¬€ering online courses about IT technologies.
The dataset used in our experiments is a sub-sample of their data collection, containing
only questions about cloud technologies (e.g., AWS3, GCP4, Azure5). Diï¬€erently
from ASSISTments, there is no concept of â€œtemplateâ€â€”thus, all the questions are
uniqueâ€”and all the questions are MCQ. In addition to the text of the questions, the
text of the possible choices is available as well, information that is not available in
the ASSISTments dataset.
5.2.1 Interactions data
TheInteractions dataset contains 7,323,502 interactions between users and questions,
involving 24,696 users and 13,603 questions. Each interaction is characterized by a
user id, an item id, a correct label, and a timestamp. Table 5.9 shows the distribution
of correct values, the overall correctness in the raw dataset is 66.51%.
3https://aws.amazon.com/
4https://cloud.google.com/
5https://azure.microsoft.com/
45

--- Page Break ---

Chapter 5. Experimental Datasets
Correct Count Percentage
1 4,870,727 66.51%
0 2,452,775 33.49%
Total 7,323,502 100%
Table 5.9. Distribution of scores.
As done with the ASSISTments dataset, for each student-item pair, only the ï¬rst
answer in chronological order is considered (i.e., the ï¬rst attempt), and only items
with at least 50 interactions are considered.
# interactions iPercentage of items
50 <=i<= 100 36.75%
100 <i<= 200 20.99%
200 <i<= 500 23.60%
i> 500 18.66%
Total 100%
Table 5.10. Interactions per item after pre-processing.
Table 5.10 shows the distribution of the number of interactions per question. On
average, each question has 304 interactions (i.e., student responses) with a standard
deviation of 365. On average, each student answered 114 diï¬€erent questions with a
standard deviation of 161.
Figure 5.4 and 5.5 show the distribution of items and students per correctness
after pre-processing. In both cases, the distribution follows a gaussian shape, but
in the case of students, there are peaks for values 0 and 1 which can be caused by
students with few interactions.
46

--- Page Break ---

5.2. Cloud Academy Dataset
0.00 0.25 0.50 0.75 1.00
Correctness0.00.51.01.52.0Densityitems
Figure 5.4. Cloud Academy, distribution of questions per correctness.
0.00 0.25 0.50 0.75 1.00
Correctness0.00.51.01.52.0Densitystudents
Figure 5.5. Cloud Academy, distribution of students per correctness.
5.2.2 Questions data
Thequestiondatasetrequiredlesspre-processingthanthatofASSISTments. Questions
texts with less than 50 interactions have been removed. Subsequently, the HTML
tags were removed.
47

--- Page Break ---

Chapter 5. Experimental Datasets
Length (# words) Percentage of items
2 < len <= 10 10.66%
10 < len <= 50 74.38%
50 < len <= 100 13.97%
len > 100 0.99%
Total 100%
Table 5.11. Distribution of questions per length.
It is essential to analyze the number of words in the question stem and the possible
choices. This is useful for sizing the model input. Table 5.11 shows the distribution
of the number of words in the questions, where only the stem of the question is
considered.
# of choices Percentage of items
4 83.24%
5 13.00%
6 3.42%
> 6 0.34%
Total 100%
Table 5.12. Distribution of questions per number of possible choices.
All the questions in the dataset are of type MCQ; the number of possible choices is
shown in Table 5.12. The texts of the possible answers are also available; on average,
a choice contains 6.8 words.
5.2.3 Lectures data
Additionally, we also consider an additional datasetâ€”referred to as Lecturesâ€”provided
by Cloud Academy, which contains the transcripts of some of the online lectures about
cloud technologies. Pre-trained models, such as BERT, are usually trained in a general
domain. However, to give the model a better understanding of a speciï¬c domain, it is
possible to do an additional pre-training. This pre-training is done in an unsupervised
manner and allows to improve performance, as shown in [58]. The paper refers to
â€œwithin-task pre-trainingâ€ when BERT is further pre-trained on the same data used for
the downstream task training, and to â€œin-domain pre-trainingâ€ when the pre-training
data is obtained from the same domain of the target task.
48

--- Page Break ---

5.2. Cloud Academy Dataset
The lecture dataset is composed of 2,826,126 words. This dataset is merged with
theQuestion dataset (containing all the questions, except those used for model testing)
that is composed of 401,912 words. The total corpus has 3,228,038 words. Then
the corpus is divided into sentences, based on punctuation (full stop, question mark,
exclamation mark) for a total of 159,563 sequences, with an average of 20 words per
sequence. Table 5.13 shows the distribution of sentences per length.
Length (# words) Percentage of items
len <= 10 23.24%
10 < len <= 50 72.79%
50 < len <= 100 3.70%
len > 100 0.27%
Total 100%
Table 5.13. Distribution of sentences per length.
49

--- Page Break ---



--- Page Break ---

Chapter 6
Experimental Setup
This chapter introduces the experimental setup. Section 6.1 describes how we estimate
our target variable (i.e., the diï¬ƒculty of the questions). Section 6.2 describes the
setup for the pre-training on Masked Language Modeling ( MLM) and Section 6.3
illustrates the setup related to the ï¬ne-tuning on Question Diï¬ƒculty Estimation.
As shown in Figure 6.1, training is performed in two steps: i) a 1PL IRT model is
ï¬tted to estimate the diï¬ƒculty of each question; ii) the IRTestimated diï¬ƒculty is
used as target label to train the Transformers-based model, which gets the text as
input.
USER	ID ITEM	ID CORRECT TIMEST AMP
ID TEXT CHOICES T ARGETIR T	ESTIMA TION
MODULEGROUND	TRUTH
IR T	DIFFICUL TY
MODEL	FOR
DIFFICUL TY
ESTIMA TION
FROM	TEXT Q TRAIN
Q TESTESTIMA TED
DIFFICUL TYInteractions	Dataset	(I)
Questions	Dataset	(Q)
Lectur es	Dataset	(L)
Figure 6.1. Experimental setup.
The ï¬rst step involves the estimation with IRTof diï¬ƒculty, starting from the
Interactions dataset. Then the IRTdiï¬ƒculties, which are considered as ground truth,
are associated with the Question dataset. It is split into two sets with a proportion
80:20 for training (Q TRAIN) and testing (Q TEST) our Transformers-based model. A
portion of the training set (10%), called validation set, is reserved for doing the
51

--- Page Break ---

Chapter 6. Experimental Setup
hyperparameters tuning. Furthermore, the proposed model can optionally exploit an
additional text dataset (i.e., the Lectures dataset) to further pre-train the pre-trained
language models on the task of MLM and improve performance.
The ï¬nal objective of our model is to estimate the diï¬ƒculty of a question without
using the Interaction dataset, but only the textual information. This allows calibrating
questions without having any interactions. Additionally, the code is publicly available1.
6.1 IRT Estimation
Theestimationofgroundtruthdiï¬ƒcultiesismadeusinga 1PLIRT model. Todoso, we
use apythonlibrary called pyirt2. It implements the EM (Expectation-Maximization)
algorithm for computing the maximum likelihood estimates of parameters in IRT
models, that is described in [26]. The inputs it uses are a unique student identiï¬er, a
unique item identiï¬er, a timestamp of the interaction, and a binary score value. The
range of diï¬ƒculty is set as [ 5; 5], while discrimination is ï¬xed to 1.
The choice to use a one-parameter model is dictated by the fact that more complex
models would have required many interactions. We set a minimum number of samples
per question equal to 50. We are aware that a more accurate estimate would require
over 50 interactions per item, but raising this threshold would reduce the number of
questions at our disposal too much. However, we remind that the average number of
interactions per item is much higher than the minimum threshold.
For each dataset, we assess the overall quality of the Interactions dataset in two
ways: i) using the loss of pyirt; ii) comparing the estimated diï¬ƒculty from diï¬€erent
subsets of interactions. The pyirtlibrary has an internal loss, based on likelihood,
that indicates the goodness of ï¬tting (the smaller, the better); we evaluate how that
loss varies as the number of interactions increases for a group of items.
25 50 75 100 125 150 175 200
# interactions0.65
0.64
0.63
0.62Loss
(a) ASSISTments dataset.
25 50 75 100 125 150 175 200
# interactions0.63
0.62
0.61
0.60Loss
 (b) Cloud Academy dataset.
Figure 6.2. Fitting loss per number of interactions.
1https://github.com/aradelli/transformers-for-qde
2https://github.com/17zuoye/pyirt
52

--- Page Break ---

6.1. IRT Estimation
Figure 6.2 shows the trend of the loss as the number of interactions increases. We
select items with 200 interactions and then calculate the diï¬ƒculty using the ï¬rst 20,
30, ..., 200 to see how the estimate improves. The behavior of the curves on both
graphs is similar. As we can see, with more than 50 interactions, the steepness of the
curves begins to decrease.
We also assess the stability of the diï¬ƒculty of diï¬€erent samples of interactions.
For each question, we create two sub-sets consisting of 50 diï¬€erent interactions. After
estimating the diï¬ƒculty using IRTwith both subsets, we compare, on average, how
much the estimated diï¬ƒculty for each question has varied.
The ASSISTments Interactions dataset has an average number of interactions per
item of 151, with a minimum of 50. Students interacted with 64 diï¬€erent items on
average. The ï¬nal loss that measures the goodness of ï¬t of the model to our data,
calculated by pyirtis 0.62. On average, the diï¬ƒculty varied by 6.46% between the
two diï¬€erent subsets.
The Cloud Academy Interactions dataset has an average number of interactions
per item of 304, with a minimum of 50. Students interacted with 115 diï¬€erent items
on average. The ï¬nal loss that measures the goodness of ï¬t of the model to our
data, calculated by pyirtis 0.58. The diï¬ƒculty varied by 5.29% between two diï¬€erent
subsets.
-5 -2.5 0 2.5 5
IRT difficulty0.00.10.20.30.4Densityitems
(a) ASSISTments dataset.
-5 -2.5 0 2.5 5
IRT difficulty0.00.10.20.30.4Densityitems (b) Cloud Academy dataset.
Figure 6.3. Distribution of items per IRT diï¬ƒculty.
Figure 6.3 shows the distributions of the estimated diï¬ƒculty for the ASSISTments
and Cloud Academy datasets. We can observe in both cases a Gaussian distribution,
with an average of zero. In the ASSISTments distribution, we notice outliers, i.e.,
items with diï¬ƒculty equal to 5 or -5. Such outliers are due to questions which are
always answered correctly or wrongly by the students; we believe that there might be
two reasons for that: i) some questions are probably too diï¬ƒcult or too easy for the
students taking the exams, and ii) the dataset most likely contains some unusable
problems (e.g., system logs), even after the data-cleaning we performed.
53

--- Page Break ---

Chapter 6. Experimental Setup
6.2 Pre-training on MLM
Pre-trainingisdoneinanunsupervisedmanneronMaskedLanguageModeling( MLM).
As a starting point we use the weights and vocabulary of bert-base-uncased and
distilbert-base-uncased models that are available in the transformers library from
Hugging Face3. For both models, the experimental setup in this phase is the same.
This pre-training requires an additional corpus, i.e., the Lectures dataset described
in chapter 5, which is available only for Cloud Academy. All data is used to train
the model. Each sequence is statically masked before the training, as described in
Chapter 4
We implement our model using Python. We use in particular these libraries:
i)Transformers by Hugging Face4for the availability of DistilBERT and BERT
architectures, weights, and tokenizers; ii) TensorFlow5andKeras6are used to wrap
the model provided by Hugging Face and train it for our task. As concerning the
hardware, the training is done on the Google Cloud Colab7platform using a Tensor
Processing Unit ( TPU).TPUis an accelerator, developed by Google, specialized in
training deep neural networks.
We train several models with a diï¬€erent number of epochs: 4, 12, 24, 36. The
original BERT has been trained from scratch, with a number of epochs equal to 40.
In our case, we are not starting from scratch, but we use for the initialization the
weights of the pre-trained models, thus we assume an ideal number of epochs lower
than 40. As optimizer, we use Adam [34]: an adaptive learning rate optimization
algorithm designed for training deep neural networks.
We experiment with the following hyperparameters:
Sequence length =128;
Batch size =256;
Adam learning rate =1e-5;
Number of epochs =4, 12, 24, 36;
Dropout =0.1.
6.3 Fine-tuning on Question Diï¬ƒculty Estimation
The ï¬ne-tuning task receives as input the texts of the questions and targets the
diï¬ƒculty estimated by IRT. For both datasets, we use the original conï¬guration of
bert-base-uncased anddistilbert-base-uncased . Also, in some of the experiments, we
use the pre-trained weights on MLMdescribed in the previous section for the Cloud
Academy dataset.
3https://huggingface.co/transformers/pretrained_models.html
4https://huggingface.co/transformers/
5https://www.tensorflow.org/
6https://keras.io/
7https://colab.research.google.com
54

--- Page Break ---

6.3. Fine-tuning on Question Diï¬ƒculty Estimation
The ASSISTments dataset provides only the stem of MCQ. The Cloud Academy
dataset provides both the stem and the possible answers allowing us to test several
input combinations:
question only : stem;
question +correct: stem [SEP] correct choice;
question +all: stem [SEP] choice_1 [SEP] ... [SEP] choice_n.
Both datasets are split in the following way: 80% training and 20% testing. Then,
the training set is further divided into 90% training and 10% validation. The validation
set is used for the choice of hyperparameters and for early stopping. Table 6.1 shows
the dimension of the splits of the datasets used for ï¬ne-tuning on QDE.
Dataset # train # validation # test
ASSISTments 8109 901 2253
Cloud Academy 4530 504 1259
Table 6.1. Datasets size.
The training is done on the Google Cloud Colab platform using a NVIDIARTeslaR
V100 with 16GB of memory.
To reduce overï¬tting, we use dropout and early stopping. The dropout is applied
to the regressor layer that we add on top of the Transformer. Moreover, the dropout
is applied inside the Transformer for all fully connected layers in the embeddings and
encoder. With early stopping, the loss on the validation set is used to determine when
overï¬tting begins and to select the best epoch. The optimizer is Adam, to which we
only tune the learning rate.
We experiment with the following hyperparameters:
Sequence length =128, 256;
Batch size =16, 32, 64;
Learning rate =1e-5, 2e-5, 3-5;
Patience early stopping =10 epochs;
Dropout regressor layer =0.1, 0.2, 0.3, 0.4, 0.5;
Dropout internal =0.1, 0.2, 0.3, 0.4, 0.5.
55

--- Page Break ---



--- Page Break ---

Chapter 7
Results
This chapter presents the results of the experiments on Question Diï¬ƒculty Estimation
(QDE) from text on the two experimental datasets. Section 7.1 introduces the metrics
that have been used. Section 7.2 brieï¬‚y presents the literature models used as baselines.
Lastly, Section 7.3 presents a quantitative and qualitative analysis of the results on
the ASSISTments dataset (in Section 7.3.1) and on the Cloud Academy dataset (in
Section 7.3.2).
7.1 Metrics
TheQDEtask consists in estimating a continuous numerical value that, in our case,
is between 5and+5. Being a regression task, we evaluate the models using two
standard regression metrics: the Mean Absolute Error ( MAE) and the Root Mean
Square Error (RMSE).
TheMAEis the average over the samples of the absolute values of the diï¬€erences
between the predictions ^biand the corresponding observations bi(in our case, the
diï¬ƒculties predicted by the model and the IRTdiï¬ƒculties). The MAEis a linear
score metric, which means that all the single diï¬€erences are weighted equally in the
average. It is deï¬ned as follows:
MAE =1
nnX
i=1(jbi ^bij) (7.1)
TheRMSEis a quadratic scoring metric that measures the average magnitude of
the error. It is deï¬ned as follows:
RMSE =vuut1
nnX
i=1(bi ^bi)2 (7.2)
57

--- Page Break ---

Chapter 7. Results
The errors are squared before being averaged; consequently, the RMSEgives a
relatively high weight to large errors (e.g., an error of 10 is 100 times worse than an
error of 1). The RMSEis always larger than or equal to the MAE; the greater the
diï¬€erence between the two metrics, the greater the variance in the sampleâ€™s errors. If
theRMSEis equal to the MAE, it means that all the errors are of the same magnitude.
Both metrics are always non-negative, and smaller values indicate better performance.
7.2 Baselines
This section introduces the models used as baseline while experimenting with the
Transformer models: the majority baseline (Section 7.2.1), R2DE (Section 7.2.2),
and ELMo (Section 7.2.3). We choose R2DE and ELMo because i) they are the
only models that do not necessarily require an additional dataset of domain-related
documents (in addition to the texts of the questions) and ii) they are the only two
models for which we had access to the code.
7.2.1 Majority
It is important to start by comparing the results with a very simple model. We use the
majority baseline, which predicts the same diï¬ƒculty for each test question, regardless
of its text. The diï¬ƒculty is obtained as the average diï¬ƒculty of the training questions.
7.2.2 R2DE
R2DE, which stands for Regressor for Diï¬ƒculty and Discrimination Estimation, is a
model proposed in [6], and the code is available1. The model can estimate both the
diï¬ƒculty and the discrimination of questions from text. Since we do not focus on
discrimination estimation in this work, we consider only the part of the model that
performs diï¬ƒculty estimation. This does not aï¬€ect the performance of the model, as
it is made of two parallel components that are trained separately for diï¬ƒculty and
discrimination.
The ï¬rst steps of text preprocessing are the following: i) stop words removal, ii)
punctuation removal, and iii) stemming. The features are then extracted from the
input text of each item using TF-IDF, a frequency-based technique. Only the top N
features are considered; this is done by sorting the features according to their number
of occurrences in the corpus and keeping only the Nmost frequent ones. The number
of featuresNis a hyperparameter. The features are fed into random forests regressor.
The best hyperparameters are chosen with ï¬ve-fold cross-validation to ï¬nd the best
conï¬guration. A grid-search is performed on the following hyperparameters:
Random forests n_estimators = [50, 100, 150, 200, 250, 300]
1https://github.com/lucabenedetto/r2de-nlp-to-estimating-irt-parameters
58

--- Page Break ---

7.2. Baselines
Random forests max_depth = [15, 25, 50, 75, 100]
TF-IDFmax_features = [1000, 1200, ..., 4000]
7.2.3 ELMo
Xue et al. [70] proposed an ELMo based model to predict the diï¬ƒculty and the
response time of MCQ. The text is preprocessed with tokenization, lemmatization,
and stopwords removal. The model is based on ELMo [46], pre-trained on the One
Billion Word Benchmark [10]. An encoding layer is added to learn the sequential
information from the ELMo embedding output. The encoding layer is made of a
Bidirectional LSTM network. This layer allows the extraction of encoding features,
which captures more abstract information than the embeddings alone. A dense layer
then follows the encoding layer to convert the feature vectors to the targets through a
non-linear combination of the feature vectorsâ€™ elements. Figure 7.1 shows the general
structure of the model.
Figure 7.1. Structure of the ELMo-based model presented in [70].
We reproduce the best conï¬guration for predicting the diï¬ƒculty only, referred to
as â€œMethod 1â€; that is, we use the encoding feature and ELMo original. The model
is originally meant to estimate the p-valueof the questions, but, in our case, we use
it to estimate the diï¬ƒculty deï¬ned as in IRT. This does not aï¬€ect the accuracy of
the model since the p-valueis a ï¬‚oating-point number (as the IRTdiï¬ƒculty we are
estimating).
59

--- Page Break ---

Chapter 7. Results
7.3 Evaluation
The models are all trained on the same dataset (Q TRAIN) and compared using the
same test set (Q TEST). We do three diï¬€erent runs for each model, using three diï¬€erent
random seeds. Then, we calculate the average and the standard deviation (SD) of
the results. We also calculate the performance of the models for questions that are
very easy or very hard (i.e., with diï¬ƒculty bgreater than 2 or less than -2) referred
to as â€œextremeâ€ questions. This is done to see how the models perform on diï¬€erent
types of questions and understand whether the accuracy of the estimate depends on
the diï¬ƒculty of the question. We use the diï¬€erent input conï¬gurations presented in
Chapter 6: i) question only ( Q only); ii) question and correct choice(s)( Q+correct);
iii) question and all choices ( Q+all). Furthermore, the Transformers models are
evaluated with and without the additional pre-training on MLM, as explained in
Chapter 4.
7.3.1 ASSISTments
The ASSISTments Q TESTis made up of 2253 questions. For MCQ, only the stem is
available, so the only input conï¬guration that can be tested is Q only. Also, since we
do not have any domain-related documents, such as the Lectures dataset, pre-training
on MLM cannot be performed.
Comparison with the state of the art
Table 7.1 shows the results of the experiments on the ASSISTments dataset. It
indicates (in this order): the model name, the input conï¬guration, if pre-training on
MLM is performed, the MAEandRMSEfor all the questions, and the same metrics
for â€œextremeâ€ questions.
Model Input MLM MAE MAE, |b|>2
Majority - - 1.066 0.000 2.8820.000
R2DE [6] Q only - 0.966 0.001 2.4080.005
ELMo [70] Q only - 0.933 0.013 2.0250.052
DistilBERT Q only N 0.919 0.009 1.8640.059
BERT Q only N 0.9110.0031.8490.074
RMSE RMSE, |b|>2
Majority - - 1.423 0.000 3.0330.000
R2DE [6] Q only - 1.304 0.001 2.7000.003
ELMo [70] Q only - 1.255 0.017 2.3750.036
DistilBERT Q only N 1.239 0.010 2.2590.037
BERT Q only N 1.2280.0032.2430.038
Table 7.1. ASSISTments results.
60

--- Page Break ---

7.3. Evaluation
The BERT-based model has the best performance on all the metrics, reducing
RMSE by 13.76% and MAEby14.53% comparedto the majority baseline. Moreover, it
reduces the MAE by 2.33% and the RMSE by 2.17% compared to the best performing
baseline that is ELMo. This diï¬€erence between BERT and ELMo appears even more
evident if we consider extreme questions (i.e., with |b|>2): the BERT model reduces
by 8.7% the MAE and 5.9% the RMSE. We observe a large standard deviation on
â€œextremeâ€ questions. We think it is due to the fact that these questions are only 13%
of the total and to an instability of the models.
The DistilBERT authors claim to maintain 97% of BERTâ€™s performance, which
in our case is conï¬rmed. Indeed, the performance of BERT and DistilBERT is very
similar, the diï¬€erence is less than 1%. The best conï¬guration of the hyperparameters
for both BERT and DistilBERT is the following:
Sequence length =128;
Batch size =64;
Learning rate =1e-5;
Patience early stopping =10 epochs;
Dropout regressor layer =0.5;
Dropout internal =0.25.
Analysis of the best performing model
We perform an analysis of the best performing model, BERT, and compare it with the
other models. First, we look at the diï¬€erence between the train and test performance.
Then, we analyze the distribution of the predicted diï¬ƒculties. Lastly, we look at what
characteristics of the question text may have inï¬‚uenced the model error.
To better understand the models, it is important to compare the performances
on the test set and those on the training set. Table 7.2 shows the performance of
the various models, indicating the diï¬€erence between the metrics in training and in
the test (averaged over three runs). R2DE has a similar error on training and test
set. However, this also leads to poor results on the test set compared to BERT. This,
and the fact that R2DE then tends to predict always diï¬ƒculties around 0 (as we
will show later on), may suggest an underï¬tting phenomenon. We can see that the
Transformer models, BERT and DistilBERT, have a much lower error on the training
set than the test set. This occurs even though we use higher dropout values (0.5) than
those usually used for BERT ï¬ne-tuning (0.1) and early-stopping to try to reduce
overï¬tting.
61

--- Page Break ---

Chapter 7. Results
Train Test
Model MAE MAE MAE
R2DE [6] 0.916 0.001 0.9660.001 0.050
ELMo [70] 0.805 0.008 0.9330.013 0.128
DistilBERT 0.683 0.062 0.9190.009 0.236
BERT 0.608 0.100 0.9110.003 0.303
RMSE RMSE RMSE
R2DE [6] 1.216 0.001 1.3040.001 0.088
ELMo [70] 1.061 0.007 1.2550.017 0.194
DistilBERT 0.897 0.087 1.2390.010 0.342
BERT 0.798 0.124 1.2280.003 0.439
Table 7.2. ASSISTments, train and test errors.
Figure 7.2 plots the loss on the training and the validation sets over training
epochs. We can see how the two curves decrease up to the 16th epoch, after which the
error on the validation starts to rise (i.e., the model begins to lose generalization). The
16th epoch is chosen by early stopping as it has the smallest error on the validation
set; the training goes on for 10 more epochs (patience).
0 5 10 15 20 25
Epoch0.751.001.251.501.752.002.252.50Loss (MSE)
train
validation
Figure 7.2. ASSISTments, model loss over training epochs.
Figure 7.3 shows the distribution of test diï¬ƒculties predicted by R2DE, ELMo,
DistilBERT, and BERT (the target diï¬ƒculty of the test set is shown in Figure 7.4).
We can immediately notice how R2DE tends to predict diï¬ƒculties around 0 (note
that the plot of R2DE has a diï¬€erent scale on the y-axis). The other models, on the
other hand, have a Gaussian distribution. BERT, compared to other models, has a
lower density around 0.
62

--- Page Break ---

7.3. Evaluation
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.61.21.82.4Densityitems
(a) R2DE.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems (b) ELMo.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems
(c) DistilBERT.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems (d) BERT.
Figure 7.3. Test set, distribution of predicted diï¬ƒculties.
Figure 7.4 shows the distribution of target diï¬ƒculties and the ones predicted by
BERT across all splits of the dataset. We see that the target diï¬ƒculties in the various
splits are distributed similarly. We also note that BERT maintains the Gaussian
distribution of the target but tends to predict diï¬ƒculties closer to 0 compared to the
target.
63

--- Page Break ---

Chapter 7. Results
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems
(a) Train target
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems (b) Train prediction
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems
(c) Validation target
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems (d) Validation prediction
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems
(e) Test target
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.6Densityitems (f) Test prediction
Figure 7.4. Distribution of the target diï¬ƒculties and BERT predictions.
64

--- Page Break ---

7.3. Evaluation
0 20 40 60 80
Input length (num. of words)5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error
(a) R2DE.
0 20 40 60 80
Input length (num. of words)5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error (b) ELMo.
0 20 40 60 80
Input length (num. of words)5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error
(c) DistilBERT.
0 20 40 60 80
Input length (num. of words)5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error (d) BERT.
Figure 7.5. ASSISTments, error depending on the input length and true diï¬ƒculty.
Figure 7.5 shows the dependence between true diï¬ƒculty, the prediction error, and
length of the question (in terms of the number of words) in the test set. We considered
questions with less than 90 words for better visualization, representing 97.5% of the
test setâ€™s questions, and we calculate the error of a single point as | b-^b|, wherebis
the true diï¬ƒculty and ^bis the predicted diï¬ƒculty. We can note that regardless of the
length of the question, R2DE always tends to predict diï¬ƒculties around the average,
i.e., equal to 0. All the other models show diï¬€erent behavior from R2DE. Even in
BERT, the best performing model, the error increases when the true diï¬ƒculty is far
from the average, but not as much as in R2DE. There is no clear relationship between
question length and prediction error.
65

--- Page Break ---

Chapter 7. Results
0.0 0.1 0.2 0.3 0.4
Percentage of digits5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error
(a) R2DE.
0.0 0.1 0.2 0.3 0.4
Percentage of digits5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error (b) ELMo.
0.0 0.1 0.2 0.3 0.4
Percentage of digits5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error
(c) DistilBERT.
0.0 0.1 0.2 0.3 0.4
Percentage of digits5.0
2.5
0.02.55.0Difficulty
0.00.51.01.52.02.53.0
Error (d) BERT.
Figure 7.6. ASSISTments, error depending on percentage of digits in the input text and true
diï¬ƒculty.
Many of the ASSISTments questions are related to mathematics; therefore, some
questions contain numbers. To see the inï¬‚uence of the presence of numbers, we
calculated the percentage of digits as the number of digits divided by the total number
of characters in the question. Figure 7.6 shows us the dependence between true
diï¬ƒculty, the prediction error, and the percentage of digits in the test set. We see
that digits in the questions are quite relevant; indeed, about 20% of the test questions
have a percentage of digits greater than 0.1. Again, we can note that regardless of
the percentage of digits, R2DE always tends to take diï¬ƒculties around the average.
In the other models for true diï¬ƒculties around 0, the error seems to decrease as the
digitsâ€™ percentage increases. The presence of numbers does not seem to have caused
problems for the models.
66

--- Page Break ---

7.3. Evaluation
BERT R2DE DistilBERT ELMo
Digits % Avg. b MAE MAE MAE MAE
Y 80% -0.314 0.869 0.926 0.871 0.889
N 20% -0.442 1.067 1.122 1.058 1.063
RMSE RMSE RMSE RMSE
Y 80% -0.314 1.171 1.238 1.175 1.189
N 20% -0.442 1.433 1.547 1.425 1.453
Table 7.3. Models performance per question with and without digits.
Continuing the previous analysis, Table 7.3 shows the models performance for
questions with or without digits. As we can see, all models follow the same pattern,
i.e., they perform better with questions containing digits. A possible explanation is
that the questions with no digits are only 20%, and therefore the model did not have
enough examples to learn. Furthermore, questions containing numbers can be related
to the same domain, mathematics. In contrast, questions that do not contain numbers
could have completely diï¬€erent domains and, therefore, might be more diï¬ƒcult to
learn due to the lack of training samples.
BERT R2DE DistilBERT ELMo
Type % Avg. b MAE MAE MAE MAE
algebra 46% -0.182 0.852 0.902 0.853 0.872
choose_1 38% -0.687 0.965 1.012 0.952 0.979
ï¬ll_in_1 14% 0.007 0.876 0.967 0.877 0.860
RMSE RMSE RMSE RMSE
algebra 46% -0.182 1.156 1.205 1.156 1.148
choose_1 38% -0.687 1.275 1.377 1.265 1.328
ï¬ll_in_1 14% 0.007 1.211 1.276 1.215 1.170
Table 7.4. Models performance per question type.
Unlike the Cloud Academy dataset, the ASSISTments dataset contains diï¬€erent
types of questions, as described in Chapter 5 (e.g, algebra,choose_1 andï¬ll_in_1 ).
Table 7.4 shows the performance of the models per question type. As we can notice,
in all models, the performance on choose_1 is lower than the others. This can be
motivated by the fact that these problems are MCQ, of which we do not have complete
textual information; indeed, the texts of the answers are not provided. Also, these
67

--- Page Break ---

Chapter 7. Results
questions have a diï¬ƒculty average bfar from 0. R2DE tends to be â€œlazyâ€ and makes
predictions around 0 (i.e., the average of the diï¬ƒculty), explaining why the error is
more signiï¬cant. We can see how ELMo performs slightly better for ï¬ll_in_1 than
foralgebra. While for the other models, the opposite happens. This fact could be due
to ELMoâ€™s diï¬€erent text pre-processing.
BERT R2DE DistilBERT ELMo
Type % Avg. b MAE MAE MAE MAE
With â€œ?â€ 43% -0.312 0.894 0.937 0.911 0.926
Without â€œ?â€ 57% -0.359 0.918 0.986 0.906 0.921
RMSE RMSE RMSE RMSE
With â€œ?â€ 43% -0.312 1.182 1.234 1.194 1.242
Without â€œ?â€ 57% -0.359 1.260 1.356 1.253 1.249
Table 7.5. Models performance per question with and without â€œ?â€.
Another feature that distinguishes the various items is the presence or absence
of the question mark â€œ?â€. Table 7.5 shows us the performance of question patterns
with and without â€œ?â€. BERT and R2DE predict more easily the diï¬ƒculty of questions
with â€œ?â€, while DistilBERT and ELMo have almost the same performance regardless
of the type. We also notice how R2DE performs much worse for questions without â€œ?â€,
even if they have similar average diï¬ƒculty. A possible explanation for the problematic
diï¬ƒculty prediction with questions without â€œ?â€ may derive from the fact that this
type of question often refers to external images.
7.3.2 Cloud Academy
The Cloud Academy Q TESTis made up of 1259 questions. All the questions are
MCQ, and, diï¬€erently from ASSISTments, the text of the possible choices is available.
Thus, we can test the three input conï¬gurations described in Chapter 6. We also
experiment with pre-training the Transformers-based models on MLMusing the
additional Lectures dataset.
Analysis of diï¬€erent conï¬gurations
Table7.6showstheresultsofDistilBERTandBERTwithdiï¬€erentconï¬gurations. The
results with pre-training on MLMreported refer to the training of Transformer-based
model for 24 epochs on the additional Lectures dataset.
68

--- Page Break ---

7.3. Evaluation
Model Input MLM MAE RMSE
DistilBERT Q only N 0.805 0.005 1.0170.005
DistilBERT Q +cor. N 0.799 0.010 1.0190.017
DistilBERT Q +all N 0.794 0.005 1.0130.007
BERT Q only N 0.807 0.015 1.0220.020
BERT Q +cor. N 0.789 0.010 0.9990.017
BERT Q +all N 0.811 0.011 1.0270.013
DistilBERT Q only Y 0.802 0.002 1.0160.002
DistilBERT Q +cor. Y 0.786 0.006 0.9940.009
DistilBERT Q +all Y 0.794 0.004 1.0090.005
BERT Q only Y 0.808 0.010 1.0200.015
BERT Q +cor. Y 0.7730.0100.9780.011
BERT Q +all Y 0.801 0.015 1.0140.015
Table 7.6. Cloud Academy, results of BERT and DistilBERT.
The best performing model is BERT pre-trained on MLMusing as input the
question stem plus the text of the correct choice(s) (i.e., the Q+cor.conï¬guration).
The results show that the input conï¬guration leading to better performance is Q
+correctfor all models except DistilBERT without pre-training, where Q+all
leads to better performance. In general, we can say that the possible choices provide
useful information that leads to an increase in performance. We can notice that the
pre-training on MLM, under the same input conï¬guration, increases the performance
by up to 2% on the MAE and up to 1.9% on the RMSE.
The best conï¬guration of the hyperparameters for both BERT and DistilBERT is
the following:
Sequence length =256 forQ+all, 128 for others;
Batch size =16;
Learning rate =2e-5;
Patience early stopping =10 epochs;
Dropout regressor layer =0.5;
Dropout internal =0.5.
Comparison with the state of the art
Table 7.7 shows the results of the best conï¬gurations of the proposed Transformers-
based models compared to the baselines. We can observe how the Transformers-based
models perform better than the baselines, even without the additional pre-training
69

--- Page Break ---

Chapter 7. Results
onMLM. Our best modelâ€”BERT with pre-training on MLMandQ + correct as
inputâ€”reduces the MAEby 4.9% and the RMSEby 5.4% with respect to the best
baseline, i.e., R2DE. We can see how all models have lower performance if only the
questionâ€™s stem is used. R2DE, unlike BERT and ELMo, performs better using the
question and all the possible choices.
Model Input MLM MAE MAE, |b|>2
Majority - - 0.845 0.000 2.5270.000
R2DE [6] Q only - 0.826 0.001 2.3970.004
R2DE [6] Q +cor. - 0.819 0.001 2.3200.005
R2DE [6] Q +all - 0.813 0.001 2.3310.008
ELMo [70] Q only - 0.833 0.002 2.2860.032
ELMo [70] Q +cor. - 0.831 0.008 2.1840.033
ELMo [70] Q +all - 0.839 0.004 2.2130.025
DistilBERT Q +all N 0.794 0.005 2.2030.044
BERT Q +cor. N 0.789 0.010 2.1180.130
DistilBERT Q +cor. Y 0.785 0.007 2.0780.065
BERT Q +cor. Y 0.7730.0102.0440.143
RMSE RMSE, |b|>2
Majority - - 1.069 0.000 2.5680.000
R2DE [6] Q only - 1.051 0.001 2.4680.005
R2DE [6] Q +cor. - 1.033 0.002 2.3910.005
R2DE [6] Q +all - 1.034 0.001 2.4050.008
ELMo [70] Q only - 1.053 0.002 2.3730.025
ELMo [70] Q +cor. - 1.048 0.010 2.2760.018
ELMo [70] Q +all - 1.057 0.007 2.3080.015
DistilBERT Q +all N 1.013 0.007 2.3090.036
BERT Q +cor. N 0.999 0.017 2.2220.110
DistilBERT Q +cor. Y 0.994 0.009 2.1920.058
BERT Q +cor. Y 0.9780.0112.1390.121
Table 7.7. Cloud Academy results.
Analysis of the best performing model
We perform an analysis of the best performing model, BERT with pre-training on
MLM, and compare it with the other models. First, we look at the diï¬€erence between
70

--- Page Break ---

7.3. Evaluation
the train and test performance. Then, we analyze the distribution of the predicted
diï¬ƒculties. Lastly, we look at what characteristics of the question text may have
inï¬‚uenced the model error.
0 5 10 15 20
Epoch0.20.40.60.81.01.21.41.6Loss (MSE)train
validation
(a) Without further pre-training.
0 5 10 15 20
Epoch0.20.40.60.81.01.21.41.6Loss (MSE)
train
validation (b) With further pre-training.
Figure 7.7. Cloud Academy, model loss over training epochs.
Figure 7.7 plots the loss on the training and validation sets over training epochs
of the BERT model and the BERT model with further pre-training respectively. As
we can observe, the model with further pre-training shows a more stable validation
loss, and also the training loss goes down faster. In these two training runs , the
epoch with the lowest validation error is the same (but itâ€™s not guaranteed to always
happen).
Train Test
Model MAE MAE MAE
R2DE [6] 0.366 0.002 0.8130.001 0.366
ELMo [70] 0.727 0.013 0.8310.008 0.104
DistilBERT 0.643 0.062 0.7850.007 0.142
BERT 0.364 0.249 0.7730.010 0.409
RMSE RMSE RMSE
R2DE [6] 0.448 0.002 1.0340.001 0.586
ELMo [70] 1.061 0.007 1.0480.010 0.013
DistilBERT 0.897 0.087 0.9940.009 0.097
BERT 0.547 0.311 0.9780.011 0.521
Table 7.8. Train and test errors.
As we have already done for the ASSISTments dataset, we compare the models
performance on the test set and on the training set (averaged over three runs). Table
7.8 shows the performance of the various models, indicating the diï¬€erence between
71

--- Page Break ---

Chapter 7. Results
the metrics on the training set and test set. We can see that BERT and R2DE have a
much lower error on the training set than the test set. Furthermore, we can see a large
standard deviation of BERT on the training set, as the early stopping has stopped
the training at diï¬€erent epochs. This, however, led to similar results on the test set.
Figure 7.8 shows the distribution of test diï¬ƒculties predicted by R2DE, ELMo,
DistilBERT, and BERT (the target diï¬ƒculty of the test set is shown in the Figure
7.9). All the predictions have a Gaussian distribution; however, we can notice how
the distributions of R2DE, ELMo, and DistilBERT have a lower variance than that of
BERT.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.20.40.60.81.01.21.4Densityitems
(a) R2DE.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.20.40.60.81.01.21.4Densityitems (b) ELMo.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.20.40.60.81.01.21.4Densityitems
(c) DistilBERT.
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.20.40.60.81.01.21.4Densityitems (d) BERT.
Figure 7.8. Test set, distribution of predicted diï¬ƒculties.
Figure 7.9 shows the distribution of the target diï¬ƒculties and the ones predicted by
BERT across all splits of the dataset. We see that the target diï¬ƒculties in the various
splits are distributed similarly. We also note that BERT maintains the Gaussian
distribution of the target but tends to predict diï¬ƒculties closer to 0 compared to the
target in the validation and test sets.
72

--- Page Break ---

7.3. Evaluation
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.60.70.8Densityitems
(a) Train target
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.60.70.8Densityitems (b) Train prediction
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.60.70.8Densityitems
(c) Validation target
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.60.70.8Densityitems (d) Validation prediction
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.60.70.8Densityitems
(e) Test target
5.0
 2.5
 0.0 2.5 5.0
Predicted difficulty0.00.10.20.30.40.50.60.70.8Densityitems (f) Test prediction
Figure 7.9. Distribution of the target diï¬ƒculties and BERT predictions.
73

--- Page Break ---

Chapter 7. Results
0 20 40 60 80 100
Input length (num. of words)3
2
1
0123Difficulty
0.00.51.01.52.02.53.0
Error
(a) R2DE.
0 20 40 60 80 100
Input length (num. of words)3
2
1
0123Difficulty
0.00.51.01.52.02.53.0
Error (b) ELMo.
0 20 40 60 80 100
Input length (num. of words)3
2
1
0123Difficulty
0.00.51.01.52.02.53.0
Error
(c) DistilBERT.
0 20 40 60 80 100
Input length (num. of words)3
2
1
0123Difficulty
0.00.51.01.52.02.53.0
Error (d) BERT.
Figure 7.10. Cloud Academy, error depending on the input length and true diï¬ƒculty.
Figure 7.10 shows the dependence between true diï¬ƒculty, the prediction error, and
length of the questions (in terms of the number of words) in the test set. We consider
questions with less than 100 words and target diï¬ƒculty between -3 and 3 for better
visualization (1067 samples over 1259 in the test set). For calculating the number of
words, we use the best input conï¬guration for each model: Q + all for R2DE and Q
+ correct for the others. For all models, the error increases as the target diï¬ƒculty
moves away from zero, showing how models tend to make predictions around zero.
The models have at their disposal many examples of questions with diï¬ƒculty around
zero, and few for extreme diï¬ƒculties. The graphs do not show an inï¬‚uence of the text
length on the prediction error.
74

--- Page Break ---

7.3. Evaluation
BERT R2DE DistilBERT ELMo
Cloze % Avg. b MAE MAE MAE MAE
Y 18% -0.144 0.809 0.892 0.836 0.891
N 82% 0.166 0.756 0.795 0.772 0.819
RMSE RMSE RMSE RMSE
Y 18% -0.144 1.034 1.149 1.076 1.142
N 82% 0.166 0.958 1.007 0.976 1.019
Table 7.9. Models performance per question type.
In the Cloud Academy dataset, we have two types of questions: i) cloze questions,
where the correct choice goes in place of an underscore in the stem of the question,
ii) questions with a question mark at the end. The diï¬ƒculty of cloze questions on
average is lower than those with a question mark. 18% of the questions in the test
set are cloze questions. Table 7.9 shows the performance of the models by type of
question (cloze, or not cloze). It can be seen that all models show a more signiï¬cant
error on cloze-type questions. This can be due to several factors, including the lower
number of samples. Another explanation could be the fact that these items are not
close to the concept of â€œquestionâ€ in natural language.
BERT R2DE DistilBERT ELMo
Digits % Avg. b MAE MAE MAE MAE
Y 50% 0.191 0.758 0.796 0.783 0.830
N 50% 0.032 0.770 0.828 0.784 0.834
RMSE RMSE RMSE RMSE
Y 50% 0.191 0.976 1.027 1.004 1.051
N 50% 0.032 0.967 1.040 0.986 1.033
Table 7.10. Models performance per digits.
Table 7.10 shows the performance of the models based on the presence or absence
of digits in the question text. Unlike the ASSISTments dataset, about half of the
questions do not contain any numbers (for this reason, we report only the binary table
and not the scatter plot with the percentage of digits per question). There is no clear
pattern if the presence of numbers aï¬€ects the prediction error.
75

--- Page Break ---

Chapter 7. Results
BERT R2DE DistilBERT ELMo
# correct % Avg. b MAE MAE MAE MAE
1 88% 0.099 0.773 0.823 0.788 0.836
> 1 12% 0.200 0.705 0.739 0.751 0.801
RMSE RMSE RMSE RMSE
1 88% 0.099 0.984 1.049 1.003 1.051
> 1 12% 0.200 0.879 0.920 0.927 0.980
Table 7.11. Models performance per number of correct choices.
The Cloud Academy dataset consists of MCQwith one or more correct choices.
Table 7.11 shows us the performance of the models on the test set based on the
number of correct choices to the question, only one or more than one. We can see
that the models more easily predict questions with more than one choice. BERT and
DistilBERT have good performance with questions with multiple choices, suggesting
that the encoding used is not wrong. The encoding of the possible choices certainly
deserves further attention. For instance, to better understand why R2DE obtains
better performance by exploiting all possible choices.
76

--- Page Break ---

Chapter 8
Conclusion
In this work, we have conducted a study about how pre-trained Transformers models,
speciï¬cally BERT and DistilBERT, perform in the task of Question Diï¬ƒculty Esti-
mation ( QDE) from text, and we have proposed a model that outperforms previous
approaches by up to 6.5% on RMSE. We evaluated the models on two real-world
datasets (one public and one private), using as ground truth Item Response The-
ory (IRT) diï¬ƒculties. Transformers have become the de facto standard in several
Natural Language Processing ( NLP) tasks and conï¬rmed their eï¬€ectiveness in QDE.
We explored two approaches to train our model: with and without an additional
pre-training on Masked Language Modeling ( MLM). This further pre-training can
increase the performance of the model by utilizing an additional dataset (e.g., books
or lecture notes) related to the task domain. Previous approaches are either totally
dependent on additional documents (i.e., they cannot work with the text of the
questions only) or cannot leverage such information. Diï¬€erently from them, our
approach can work with or without a supplementary corpus of documents. The
proposed model can outperform the state of the art approaches being trained only
on the questions text and can be further improved if such an additional dataset is
available. Speciï¬cally, we experimented with two pre-trained language models: BERT
and DistilBERT. Both models proved to be better than the baselines, and the best
performing model turned out to be the one based on BERT.
As an outcome of our study, we can say that: i) if an additional dataset if available,
the pre-training on MLMimproves the performance of Transformers model; ii) if the
only available data is the text of the questions, DistilBERT might also be a good
option, as it retains almost the same performance of BERT but at a fraction of the
computational cost; iii) the possible choices help to estimate the diï¬ƒculties as all the
models show a larger error using only the stem of the questions.
We have analyzed which characteristics of the questions aï¬€ect the prediction error
of the models in our experiments. We have noticed that the error naturally increases
as the magnitude of the diï¬ƒculty increases and that R2DE, in particular, tends to
predict more frequently diï¬ƒculty close to zero. This is also due to the Gaussian
distribution with zero mean of the training data as the models cannot see many
questions that are very easy or very diï¬ƒcult. The length of the question does not
seem to aï¬€ect the prediction error of BERT and the other models. It has also been
noticed that BERT has worse performance on cloze questions. One reason could be
that this type of question is further away from natural language. Plus, we had fewer
77

--- Page Break ---

Chapter 8. Conclusion
samples available for these questions than those ending with a question mark.
Our contribution opens up several directions of research that can be explored in
the future. A possible idea might be to improve the performance of BERT on cloze
questions by using a diï¬€erent encoding. In fact, during the MLMtraining, what BERT
tries to do is to predict the hidden word, which is the same objective of the cloze
questions. The [MASK]token has the same function as the underscore. Therefore, the
token could be reused in the ï¬ne-tuning on QDE instead of the underscore.
The model interpretability is very important to provide support to question
creators. Unlike other models, Transformers are based on the Attention mechanism,
which might explain which part of the question aï¬€ects the diï¬ƒculty. However, we are
aware that Transformers models interpretation is not immediate, and there is ongoing
research on it.
Another future experiment could involve using Question Answering as an auxiliary
task: a ï¬rst ï¬ne-tuning of BERT to answer the questions and then a second to perform
QDE. In the literature, an attempt has been made to use response time prediction
as an auxiliary task [70], but no one has used Question Answering. This approach
requires no additional data apart from the question and the correct choice and could
lead to increased performance.
In our case, we used an additional datasetâ€”containing the transcripts of lessons
related to the questionsâ€”to further pre-train the model on MLM. This further pre-
training has led to an increase in performance, but it would be interesting to examine
if it really is the best way to exploit such a dataset. Other approaches might use the
additional dataset to see how many times the topic required by the question appears
in the corpus in order to extract useful Information Retrieval ( IR) features. We could
compare which method between the two is the most eï¬€ective way to leverage this
additional information.
78

--- Page Break ---

Acronyms
CTT Classical Test Theory
IRT Item Response Theory
1PL One-Parameter Logistic
2PL Two-Parameter Logistic
3PL Three-Parameter Logistic
GMAT Graduate Management Admission Test
ICC Item Characteristic Curve
NLP Natural Language Processing
TF-IDF Term Frequencyâ€“Inverse Document Frequency
MLE Maximum Likelihood Estimation
FFNN Feed Forward Neural Network
MLP Multi Layer Perceptron
RNN Recurrent Neural Network
seq2seq Sequence to Sequence Learning
LSTM Long short-term memory
ReLU Rectiï¬ed Linear Unit
BPE Byte Pair Encoding
SVM Support-Vector Machines
BERT Bidirectional Encoder Representations from Transformers
GLUE General Language Understanding Evaluation
SQuAD Stanford Q/A dataset
SVM Support-Vector Machines
CQA Community Question Answering
IR Information Retrieval
MCQ Multiple Choice Questions
LDA Latent Dirichlet Allocation
OJ Online Judge
KT Knowledge Tracing
79

--- Page Break ---

Chapter 8. Conclusion
BKT Bayesian Knowledge Tracing
NSP Next Sentence Prediction
MLM Masked Language Modeling
MSE Mean Squared Error
RMSE Root Mean Square Error
MAE Mean Absolute Error
IRF Item Response Function
TPU Tensor Processing Unit
QDE Question Diï¬ƒculty Estimation
80

--- Page Break ---

Bibliography
[1]Ghodai Abdelrahman and Qing Wang. Knowledge tracing with sequential key-
value memory networks. In Proceedings of the 42nd International ACM SIGIR
Conference on Research and Development in Information Retrieval , pages175â€“184,
2019.
[2]Abir Abyaa, Mohammed Khalidi Idrissi, and Samir Bennani. Learner modelling:
systematic review of the literature from the last 5 years. Educational Technology
Research and Development , 67(5):1105â€“1143, 2019.
[3]Tahani Alsubait, Bijan Parsia, and Ulrike Sattler. A similarity-based theory of
controlling mcq diï¬ƒculty. In 2013 second international conference on e-learning
and e-technologies in education (ICEEE) , pages 283â€“288. IEEE, 2013.
[4]Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model
for scientiï¬c text. arXiv preprint arXiv:1903.10676 , 2019.
[5]Luca Benedetto, Andrea Cappelli, Roberto Turrin, and Paolo Cremonesi. In-
troducing a framework to assess newly created questions with natural language
processing. arXiv preprint arXiv:2004.13530 , 2020.
[6]Luca Benedetto, Andrea Cappelli, Roberto Turrin, and Paolo Cremonesi. R2de:
a nlp approach to estimating irt parameters of newly generated questions. In
Proceedings of the Tenth International Conference on Learning Analytics &
Knowledge , pages 412â€“421, 2020.
[7]Denny Borsboom and Gideon J Mellenbergh. True scores, latent variables, and
constructs: A comment on schmidt and hunter. Intelligence , 30(6):505â€“514, 2002.
[8]Hao Cen, Kenneth Koedinger, and Brian Junker. Learning factors analysisâ€“a
general method for cognitive model evaluation and improvement. In International
Conference on Intelligent Tutoring Systems , pages 164â€“175. Springer, 2006.
[9]Hao Cen, Kenneth Koedinger, and Brian Junker. Comparing two irt models for
conjunctive skills. In International Conference on Intelligent Tutoring Systems ,
pages 796â€“798. Springer, 2008.
[10]Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp
Koehn, and Tony Robinson. One billion word benchmark for measuring progress
in statistical language modeling. arXiv preprint arXiv:1312.3005 , 2013.
81

--- Page Break ---

Bibliography
[11]P. Chen, Y. Lu, V. W. Zheng, and Y. Pian. Prerequisite-driven deep knowledge
tracing. In 2018 IEEE International Conference on Data Mining (ICDM) , pages
39â€“48, 2018.
[12]Albert T Corbett and John R Anderson. Knowledge tracing: Modeling the
acquisition of procedural knowledge. User modeling and user-adapted interaction ,
4(4):253â€“278, 1994.
[13]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. Transformer-xl: Attentive language models beyond a ï¬xed-length
context. arXiv preprint arXiv:1901.02860 , 2019.
[14]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei Fei Li. Imagenet:
a large-scale hierarchical image database. pages 248â€“255, 06 2009.
[15]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 , 2018.
[16]Xinyi Ding and Eric C Larson. Why deep knowledge tracing has less depth than
anticipated. International Educational Data Mining Society , 2019.
[17] William H DuBay. The principles of readability. Online Submission , 2004.
[18]Xitao Fan. Item response theory and classical test theory: An empirical compari-
son of their item/person statistics. Educational and psychological measurement ,
58(3):357â€“381, 1998.
[19]Jiansheng Fang, Wei Zhao, and Dongya Jia. Exercise diï¬ƒculty prediction in
online education systems. In 2019 International Conference on Data Mining
Workshops (ICDMW) , pages 311â€“317. IEEE, 2019.
[20]Mingyu Feng, Neil Heï¬€ernan, and Kenneth Koedinger. Addressing the assessment
challenge with an online system that tutors as it assesses. User Modeling and
User-Adapted Interaction , 19(3):243â€“266, 2009.
[21]Thomas FranÃ§ois and Eleni Miltsakaki. Do nlp and machine learning improve
traditional readability formulas? In Proceedings of the First Workshop on
Predicting and Improving Text Readability for target reader populations , pages
49â€“57, 2012.
[22]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press,
2016. http://www.deeplearningbook.org .
[23]Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu,
Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-speciï¬c language
model pretraining for biomedical natural language processing. arXiv preprint
arXiv:2007.15779 , 2020.
[24]Ronald K Hambleton and Russell W Jones. Comparison of classical test theory
and item response theory and their applications to test development. Educational
measurement: issues and practice , 12(3):38â€“47, 1993.
82

--- Page Break ---

Bibliography
[25]RonaldKHambleton, HariharanSwaminathan, andHJaneRogers. Fundamentals
of item response theory . Sage, 1991.
[26]BA Hanson. Irt parameter estimation using the em algorithm (tech. rep.), 2000.
[27]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 770â€“778, 2016.
[28]Donald O. Hebb. The organization of behavior: A neuropsychological theory .
Wiley, New York, June 1949.
[29]Tin Kam Ho. Random decision forests. In Proceedings of 3rd international
conference on document analysis and recognition , volume 1, pages 278â€“282. IEEE,
1995.
[30]Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural
computation , 9:1735â€“80, 12 1997.
[31]Zhenya Huang, Qi Liu, Enhong Chen, Hongke Zhao, Mingyong Gao, Si Wei,
Yu Su, and Guoping Hu. Question diï¬ƒculty prediction for reading problems in
standard tests. In AAAI, pages 1352â€“1359, 2017.
[32]Chowdhury Md Intisar and Yutaka Watanobe. Cluster analysis to estimate the
diï¬ƒculty of programming problems. In Proceedings of the 3rd International
Conference on Applications in Information Technology , pages 23â€“28, 2018.
[33]Matthew S Johnson et al. Marginal maximum likelihood estimation of item
response models in r. Journal of Statistical Software , 20(10):1â€“24, 2007.
[34]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980 , 2014.
[35]Ghader Kurdi, Bijan Parsia, and Uli Sattler. An experimental evaluation of
automatically generated multiple choice questions from ontologies. In OWL:
Experiences And directionsâ€“reasoner evaluation , pages 24â€“39. Springer, 2016.
[36]Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,
Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language
representationmodelforbiomedicaltextmining. Bioinformatics , 36(4):1234â€“1240,
2020.
[37]Jinseok Lee and Dit-Yan Yeung. Knowledge query network for knowledge tracing:
How knowledge interacts with skills. In Proceedings of the 9th International
Conference on Learning Analytics & Knowledge , pages 491â€“500, 2019.
[38]Jing Liu, Quan Wang, Chin-Yew Lin, and Hsiao-Wuen Hon. Question diï¬ƒculty
estimation in community question answering services. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing , pages 85â€“90,
2013.
83

--- Page Break ---

Bibliography
[39]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 ,
2019.
[40]Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Eï¬€ective
approaches to attention-based neural machine translation. arXiv preprint
arXiv:1508.04025 , 2015.
[41]Ye Mao. Deep learning vs. bayesian knowledge tracing: Student models for
interventions. Journal of educational data mining , 10(2), 2018.
[42]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeï¬€ Dean.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems , pages 3111â€“3119, 2013.
[43]Zachary A Pardos and Anant Dadu. Imputing kcs with representations of problem
content and context. In Proceedings of the 25th Conference on User Modeling,
Adaptation and Personalization , pages 148â€“155, 2017.
[44]Thanaporn Patikorn, David Deisadze, Leo Grande, Ziyang Yu, and Neil Heï¬€ernan.
Generalizability of methods for imputing mathematical skills needed to solve
problems from texts. In International Conference on Artiï¬cial Intelligence in
Education , pages 396â€“405. Springer, 2019.
[45]Radek PelÃ¡nek. Bayesian knowledge tracing, logistic models, and beyond: an
overview of learner modeling techniques. User Modeling and User-Adapted
Interaction , 27(3-5):313â€“350, 2017.
[46]Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representa-
tions.arXiv preprint arXiv:1802.05365 , 2018.
[47]Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami,
Leonidas J Guibas, and Jascha Sohl-Dickstein. Deep knowledge tracing. In
Advances in neural information processing systems , pages 505â€“513, 2015.
[48]Zhaopeng Qiu, Xian Wu, and Wei Fan. Question diï¬ƒculty prediction for multiple
choice problems in medical exams. In Proceedings of the 28th ACM International
Conference on Information and Knowledge Management , pages 139â€“148, 2019.
[49]Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad:
100,000+ questions for machine comprehension of text. arXiv preprint
arXiv:1606.05250 , 2016.
[50]Georg Rasch. Studies in mathematical psychology: I. probabilistic models for
some intelligence and attainment tests. 1960.
[51]Lawrence Rudner. Implementing the Graduate Management Admission Test
Computerized Adaptive Test , pages 151â€“165. 01 2010.
84

--- Page Break ---

Bibliography
[52]Alper Sahin and Duygu Anil. The eï¬€ects of test length and sample size on item
parameters in item response theory. 2017.
[53]Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert,
a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint
arXiv:1910.01108 , 2019.
[54]Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In
2012 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pages 5149â€“5152. IEEE, 2012.
[55]Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation
of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015.
[56]Thomas Sergent, FranÃ§ois Bouchet, and Thibault Carron. Towards temporality-
sensitive recurrent neural networks through enriched traces.
[57]Stefan Slater, Ryan Baker, Ma Victoria Almeda, Alex Bowers, and Neil Heï¬€ernan.
Using correlational topic modeling for automated topic identiï¬cation in intelligent
tutoring systems. In Proceedings of the Seventh International Learning Analytics
& Knowledge Conference , pages 393â€“397, 2017.
[58]Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How to ï¬ne-tune bert for
text classiï¬cation? In China National Conference on Chinese Computational
Linguistics , pages 194â€“206. Springer, 2019.
[59]Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning
with neural networks. In Advances in neural information processing systems ,
pages 3104â€“3112, 2014.
[60]Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chun-
fang Liu. A survey on deep transfer learning. In International conference on
artiï¬cial neural networks , pages 270â€“279. Springer, 2018.
[61]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett, editors, Advances in Neural Information Processing Systems 30 ,
pages 5998â€“6008. Curran Associates, Inc., 2017.
[62]S Vijayarani, Ms J Ilamathi, and Ms Nithya. Preprocessing techniques for text
mining-an overview. International Journal of Computer Science & Communica-
tion Networks , 5(1):7â€“16, 2015.
[63]Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for
natural language understanding. arXiv preprint arXiv:1804.07461 , 2018.
[64]Tianqi Wang, Fenglong Ma, and Jing Gao. Deep hierarchical knowledge tracing.
InProceedings of the 12th International Conference on Educational Data Mining ,
2019.
85

--- Page Break ---

Bibliography
[65]Xiaojing Wang, James O Berger, Donald S Burdick, et al. Bayesian analysis of
dynamic item response models in educational testing. The Annals of Applied
Statistics , 7(1):126â€“153, 2013.
[66] Peter Willett. The porter stemming algorithm: then and now. Program, 2006.
[67]Kevin H Wilson, Yan Karklin, Bojian Han, and Chaitanya Ekanadham. Back to
the basics: Bayesian extensions of irt outperform neural networks for proï¬ciency
estimation. arXiv preprint arXiv:1604.02336 , 2016.
[68]Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In International conference on
machine learning , pages 2048â€“2057, 2015.
[69]Liangbei Xu and Mark A Davenport. Dynamic knowledge embedding and tracing.
arXiv preprint arXiv:2005.09109 , 2020.
[70]Kang Xue, Victoria Yaneva, Christopher Runyon, and Peter Baldwin. Predicting
thediï¬ƒcultyandresponsetimeofmultiplechoicequestionsusingtransferlearning.
InProceedings of the Fifteenth Workshop on Innovative Use of NLP for Building
Educational Applications , pages 193â€“197, 2020.
[71]Victoria Yaneva, Peter Baldwin, Janet Mee, et al. Predicting the diï¬ƒculty
of multiple choice questions in a high-stakes medical exam. In Proceedings of
the Fourteenth Workshop on Innovative Use of NLP for Building Educational
Applications , pages 11â€“20, 2019.
[72]Victoria Yaneva et al. Automatic distractor suggestion for multiple-choice tests
using concept embeddings and information retrieval. In Proceedings of the
thirteenth workshop on innovative use of NLP for building educational applications ,
pages 389â€“398, 2018.
[73]Victoria Yaneva, Constantin OrÄƒsan, Richard Evans, and Omid Rohanian. Com-
bining multiple corpora for readability assessment for people with cognitive
disabilities. Association for Computational Linguistics, 2017.
[74]Chun-Kit Yeung. Deep-irt: Make deep learning based knowledge tracing explain-
able using item response theory. arXiv preprint arXiv:1904.11738 , 2019.
[75]Chun-Kit Yeung and Dit-Yan Yeung. Addressing two problems in deep knowledge
tracing via prediction-consistent regularization. In Proceedings of the Fifth Annual
ACM Conference on Learning at Scale , pages 1â€“10, 2018.
[76]Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung. Dynamic key-value
memory networks for knowledge tracing. In Proceedings of the 26th international
conference on World Wide Web , pages 765â€“774, 2017.
[77]Liang Zhang, Xiaolu Xiong, Siyuan Zhao, Anthony Botelho, and Neil T Heï¬€ernan.
Incorporating rich features into deep knowledge tracing. In Proceedings of the
Fourth (2017) ACM Conference on Learning@ Scale , pages 169â€“172, 2017.
86

--- Page Break ---

Bibliography
[78]Wen Zhang, Taketoshi Yoshida, and Xijin Tang. A comparative study of tf*
idf, lsi and multi-words for text classiï¬cation. Expert Systems with Applications ,
38(3):2758â€“2765, 2011.
[79]Wayne Xin Zhao, Wenhui Zhang, Yulan He, Xing Xie, and Ji-Rong Wen. Auto-
matically learning topics and diï¬ƒculty levels of problems in online judge systems.
ACM Transactions on Information Systems (TOIS) , 36(3):1â€“33, 2018.
87

--- Page Break ---

