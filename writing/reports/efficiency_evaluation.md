# Evaluating the Efficiency of Synthetic Student Data for IRT Difficulty Estimation

This document outlines the process and findings from evaluating the efficiency of using synthetically generated student response data (produced by a neural network model) to estimate Item Response Theory (IRT) difficulty parameters, compared to using traditional real student response data.

## Goal

The primary goal was to quantify the practical value of the synthetic student model. We wanted to determine how many *real* student answers would be needed in a traditional IRT analysis to achieve the same level of difficulty estimation accuracy as our model based on synthetic data. This comparison aims to highlight potential savings in data collection cost and time offered by the synthetic approach.

## Methodology Overview

The core idea is to compare difficulty estimates derived from the synthetic model against estimates derived from varying amounts of real student data.

1.  **Define a Target Set:** Focus on a specific set of questions (the holdout set from the main model training, N=470 questions in this case).
2.  **Establish Ground Truth:** Determine the "true" difficulty for these target questions.
3.  **Calculate Synthetic Model Error:** Estimate difficulty for the target questions using the neural network's synthetic predictions and calculate the error against the ground truth.
4.  **Simulate Real Data Analysis:**
    *   Repeatedly sample increasing amounts of *real* student answers corresponding to the target questions.
    *   For each sample size, estimate IRT difficulty and calculate the average error against the ground truth.
5.  **Plot and Interpret:** Plot Error vs. Number of Real Answers. Draw a line representing the synthetic model's error and find the intersection point to determine the equivalent number of real answers.

## Initial Approach: Using RMSE

Our first attempt used the Root Mean Square Error (RMSE) between the estimated difficulty parameters and the ground truth parameters as the error metric.

### RMSE - Attempt 1: Issues with Ground Truth

*   **Procedure:** We loaded the `01_difficulty.csv` file (generated by `main2.py` using the *entire* dataset) and filtered it for the holdout questions to serve as the ground truth. We then calculated the synthetic model's difficulty RMSE and simulated the RMSE for different real data sample sizes (from the holdout answers).
*   **Result:** The synthetic model showed a very high RMSE (~7.8). The simulation curve showed decreasing RMSE with more real data, but the interpretation was that the synthetic model was only equivalent to using ~258 real answers.
*   **Problem:** We realized that comparing estimates derived *only* from holdout data (synthetic or sampled) against a ground truth derived from the *full dataset* was problematic. IRT parameters are scale-dependent; estimates based on different item/person pools can have different absolute values even if the relative ordering is similar.

### RMSE - Attempt 2: Corrected Ground Truth

*   **Procedure:** We modified the `evaluate_efficiency.py` script to calculate a new ground truth. We ran the `estimate_irt_1pl_difficulty` function *only* on the complete set of real answers available for the holdout questions (`holdout_answers_df`). This ensured all estimates (synthetic, sampled, ground truth) were derived from the same item pool and subsets of the same answer pool.
*   **Result:** The RMSE for the full holdout set simulation now correctly approached zero. However, the synthetic model's RMSE remained very high (~7.7), and the interpretation was still unfavorable (~258 equivalent real answers).
*   **Problem:** While the comparison was now fairer, the high synthetic RMSE persisted. This suggested that although the *same* IRT estimation function (`estimate_irt_1pl_difficulty`) was used for both synthetic and real data, the absolute difficulty *scale* resulting from the synthetic (binarized) predictions differed significantly from the scale resulting from real response patterns. RMSE, being sensitive to absolute differences, penalized this scale mismatch heavily.

## Revised Approach: Using Correlation

Given the issues with RMSE's sensitivity to absolute scale, we switched the error metric to focus on the *relative ordering* of difficulties.

*   **New Metric:** \(1 - \text{Pearson Correlation}\). A value of 0 means perfect positive correlation (perfect ranking), while a value of 2 means perfect negative correlation. Higher correlation leads to lower error.
*   **Procedure:** We modified `evaluate_efficiency.py` to use this new metric in the `calculate_error` function and updated plots and interpretations accordingly.

## Findings with Correlation Metric

*   **Synthetic Model Error:** The error (1 - Corr) for the synthetic model was **0.0364**, indicating a very high Pearson correlation of \(1 - 0.0364 \approx 0.964\) with the ground truth difficulties. This shows the synthetic model accurately captures the *relative* difficulty ranking of the holdout items.
*   **Simulation Error:** The error curve for real data samples behaved as expected, starting higher (lower correlation) for small sample sizes and decreasing towards zero (perfect correlation) as the sample size approached the full holdout set.
    *   258 answers: ~0.62 error (Corr ~0.38)
    *   ...
    *   18,074 answers: ~0.047 error (Corr ~0.953)
    *   20,656 answers: ~0.031 error (Corr ~0.969)
    *   ...
    *   25,821 answers: ~0.000 error (Corr ~1.0)
*   **Interpretation:** By finding where the synthetic model's error (0.0364) intersects the real data simulation curve, we estimated that the synthetic model achieves a difficulty correlation roughly equivalent to using **~19,758 real student answers**.

## Conclusion

Using \(1 - \text{Pearson Correlation}\) as the evaluation metric proved much more effective than RMSE for demonstrating the efficiency of the synthetic student model in this context. While RMSE was sensitive to differences in the absolute scale of IRT parameters derived from synthetic vs. real data, the correlation metric highlighted that the synthetic model accurately reproduces the *relative difficulty ranking* of items.

The final results indicate that the synthetic model provides difficulty estimations whose ranking accuracy is comparable to what would be achieved through a traditional IRT analysis using nearly 20,000 real student responses for the holdout set. This represents a significant potential saving in data collection effort and cost, validating the practical utility of the synthetic approach for accelerating assessment development while maintaining psychometric quality in terms of item ordering. 