{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Knowledge Tracing with ModernBERT\n",
        "\n",
        "This notebook demonstrates our approach to simulating student responses in an educational platform using **Deep Knowledge Tracing (DKT)** with **ModernBERT**. Each student is treated as a unique case, and we fine-tune separate ModernBERT models per student. The key goal is to predict whether a student will answer a given question correctly or incorrectly based on:\n",
        "\n",
        "- The text of the question and its multiple-choice options.\n",
        "- The topic, subject, and other relevant attributes.\n",
        "- The student's past performance (implicitly learned during training).\n",
        "\n",
        "Our broader objective is to create synthetic \"student models\" that can help us estimate question difficulty without requiring costly large-scale field testing with real students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Overview\n",
        "1. **Imports and Environment Setup**: We load environment variables and import the required Python libraries.\n",
        "2. **Data Loading and Preparation**: We read the main dataset, remove duplicates, and prepare it for training.\n",
        "3. **User Selection**: We select the top 50 users based on the number of answers submitted.\n",
        "4. **Model Training Function**: We define a function that fine-tunes a ModernBERT model for a specific user.\n",
        "5. **Batch Training**: We loop over the selected users and fine-tune a separate model for each.\n",
        "6. **Next Steps**: Outline potential next steps, such as evaluating performance, computing difficulty metrics, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# 1) Imports and Environment Setup\n",
        "\n",
        "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"[INFO] Environment variables loaded and libraries imported.\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading\n",
        "In this section, we load the main dataset that contains all the user interactions (student answers to questions). We remove duplicate entries (if any) to ensure that each `answer_id` is unique. This step helps maintain data quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# 2) Data Loading and Preparation\n",
        "\n",
        "# Load the master dataset that has the translated version of all records\n",
        "df_original = pd.read_csv('../data/new/master_translated.csv')\n",
        "print(f\"[INFO] Loaded master dataset with {len(df_original):,} rows.\")\n",
        "\n",
        "# Drop duplicate answer_ids to ensure each answer is only counted once\n",
        "initial_count = len(df_original)\n",
        "df_original.drop_duplicates(subset=['answer_id'], inplace=True)\n",
        "final_count = len(df_original)\n",
        "print(f\"[INFO] Duplicates removed: {initial_count - final_count}\")\n",
        "print(f\"[INFO] Dataset now has {final_count:,} rows.\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### User Selection\n",
        "We next identify the top 50 users based on the number of answers submitted. These top users are chosen for individual model training, which will allow us to capture a variety of response patterns from different individuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# 3) Selecting the top 50 users by answer count\n",
        "\n",
        "# Count how many answers each user has\n",
        "user_answer_counts = df_original['user_id'].value_counts()\n",
        "\n",
        "# Select the top 50\n",
        "top_50_users = user_answer_counts.head(50)\n",
        "\n",
        "# (Optional) Reverse the order, so you process from smallest to largest if you prefer\n",
        "top_50_users = top_50_users.iloc[::-1]\n",
        "\n",
        "print(\"[INFO] Top 50 users by number of answers:\")\n",
        "for u_id, count in top_50_users.items():\n",
        "    print(f\"- User {u_id}: {count:,} answers\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training Setup\n",
        "Below, we install all necessary libraries and define the function that trains a **ModernBERT** model for a single user. We use Hugging Face Transformers to:\n",
        "\n",
        "1. **Tokenize** our question text + multiple-choice options.\n",
        "2. **Fine-tune** a ModernBERT-based binary classifier.\n",
        "3. **Evaluate** performance (using a weighted F1 score, which is important because the dataset can be imbalanced).\n",
        "\n",
        "Key libraries and components:\n",
        "- **Datasets**: For handling train/validation splits in a memory-efficient manner.\n",
        "- **AutoTokenizer** and **AutoModelForSequenceClassification**: Automatically load the correct tokenizer and model configuration.\n",
        "- **Trainer** and **TrainingArguments**: Simplify the training loop, logging, checkpointing, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# 4) Model Definition and Utility Imports\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "print(\"[INFO] Transformers and related libraries loaded.\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train a Model for One User\n",
        "The following function, `train_user_model`, encapsulates all the steps for training a ModernBERT model on data from a single user. The pipeline:\n",
        "\n",
        "1. **Filter** the DataFrame to retrieve only rows belonging to the target user.\n",
        "2. **Combine** the question text, the multiple-choice options, and meta-info (topic, subject, axis) into one string.\n",
        "3. **Split** the user's data into train/test sets.\n",
        "4. **Tokenize** the text for ModernBERT.\n",
        "5. **Initialize** and **train** the model (using the Hugging Face Trainer).\n",
        "6. **Evaluate** using a weighted F1 metric.\n",
        "7. **Save** the best checkpoint.\n",
        "\n",
        "If `push_to_hub=True`, the final model can be automatically uploaded to the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "def train_user_model(\n",
        "    df: pd.DataFrame,\n",
        "    user_id: int,\n",
        "    model_name: str = \"answerdotai/ModernBERT-base\",\n",
        "    output_dir_base: str = \"user_models\",\n",
        "    push_to_hub: bool = False,\n",
        "    hub_repo_org: str = None,\n",
        "    hub_token: str = None,\n",
        "    num_train_epochs: int = 5,\n",
        "    batch_size: int = 4\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a ModernBERT model to predict if a user will answer correctly.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The entire dataset containing rows for all users.\n",
        "        user_id (int): The target user's identifier.\n",
        "        model_name (str): Hugging Face model ID for ModernBERT.\n",
        "        output_dir_base (str): Base directory where user-specific checkpoints will be stored.\n",
        "        push_to_hub (bool): Whether to push the final model to the Hugging Face Hub.\n",
        "        hub_repo_org (str): Org or username for HF Hub repositories.\n",
        "        hub_token (str): Personal access token if required for private repos.\n",
        "        num_train_epochs (int): Number of epochs to train.\n",
        "        batch_size (int): Training batch size.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Filter user data\n",
        "    df_user = df[df[\"user_id\"] == user_id].copy()\n",
        "\n",
        "    # Check if user has any data\n",
        "    if df_user.empty:\n",
        "        print(f\"[User {user_id}] No data. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Convert correctness to integer labels (0/1)\n",
        "    df_user[\"label\"] = df_user[\"is_correct\"].astype(int)\n",
        "\n",
        "    # 2) Combine text: question + choices + topics\n",
        "    def combine_text(row):\n",
        "        q = str(row.get(\"question_title\", \"\"))\n",
        "        a = str(row.get(\"option_a\", \"\"))\n",
        "        b = str(row.get(\"option_b\", \"\"))\n",
        "        c = str(row.get(\"option_c\", \"\"))\n",
        "        d = str(row.get(\"option_d\", \"\"))\n",
        "        e = str(row.get(\"option_e\", \"\"))\n",
        "        topic = str(row.get(\"topic_name\", \"\"))\n",
        "        subj = str(row.get(\"subject_name\", \"\"))\n",
        "        axis = str(row.get(\"axis_name\", \"\"))\n",
        "\n",
        "        return (\n",
        "            f\"Topic: {topic}\\n\"\n",
        "            f\"Subject: {subj}\\n\"\n",
        "            f\"Axis: {axis}\\n\\n\"\n",
        "            f\"Question: {q}\\n\"\n",
        "            f\"A) {a}\\n\"\n",
        "            f\"B) {b}\\n\"\n",
        "            f\"C) {c}\\n\"\n",
        "            f\"D) {d}\\n\"\n",
        "            f\"E) {e}\"\n",
        "        )\n",
        "\n",
        "    df_user[\"text\"] = df_user.apply(combine_text, axis=1)\n",
        "\n",
        "    # 3) Train/Test split\n",
        "    train_df, test_df = train_test_split(\n",
        "        df_user,\n",
        "        test_size=0.2,\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Check if split is valid\n",
        "    if train_df.empty or test_df.empty:\n",
        "        print(f\"[User {user_id}] Not enough data to split. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Build Hugging Face Datasets\n",
        "    train_dataset_hf = Dataset.from_pandas(train_df[[\"text\",\"label\"]].reset_index(drop=True))\n",
        "    test_dataset_hf  = Dataset.from_pandas(test_df[[\"text\",\"label\"]].reset_index(drop=True))\n",
        "\n",
        "    ds_dict = DatasetDict({\n",
        "        \"train\": train_dataset_hf,\n",
        "        \"test\": test_dataset_hf\n",
        "    })\n",
        "\n",
        "    # 4) Tokenize\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "    ds_dict = ds_dict.map(tokenize_fn, batched=True)\n",
        "\n",
        "    # 5) Load Model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2\n",
        "    )\n",
        "    model.config.problem_type = \"single_label_classification\"\n",
        "\n",
        "    # 6) Training Arguments\n",
        "    user_output_dir = os.path.join(output_dir_base, f\"user_{user_id}\")\n",
        "    os.makedirs(user_output_dir, exist_ok=True)\n",
        "\n",
        "    if push_to_hub:\n",
        "        if hub_repo_org:\n",
        "            hub_repo_id = f\"{hub_repo_org}/modernbert-user-{user_id}\"\n",
        "        else:\n",
        "            hub_repo_id = f\"modernbert-user-{user_id}\"\n",
        "    else:\n",
        "        hub_repo_id = None\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=user_output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        use_mps_device=True,          # For Apple Silicon\n",
        "        bf16=True,                    # Use bfloat16 if supported\n",
        "        optim=\"adamw_torch_fused\",   # Fused optimizer for performance\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        save_total_limit=2,\n",
        "        push_to_hub=push_to_hub,\n",
        "        hub_model_id=hub_repo_id,\n",
        "        hub_token=hub_token,\n",
        "        hub_strategy=\"end\",\n",
        "    )\n",
        "\n",
        "    # Define metrics\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        f1_val = f1_score(labels, preds, average=\"weighted\")\n",
        "        return {\"accuracy\": acc, \"f1\": f1_val}\n",
        "\n",
        "    # 7) Create Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=ds_dict[\"train\"],\n",
        "        eval_dataset=ds_dict[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Check if we should resume training\n",
        "    last_checkpoint = None\n",
        "    if os.path.isdir(user_output_dir):\n",
        "        last_checkpoint = get_last_checkpoint(user_output_dir)\n",
        "    if last_checkpoint is not None:\n",
        "        print(f\"[User {user_id}] Resuming from {last_checkpoint}.\")\n",
        "    else:\n",
        "        print(f\"[User {user_id}] Starting training from scratch.\")\n",
        "\n",
        "    # 8) Train\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "\n",
        "    # Evaluate final\n",
        "    eval_metrics = trainer.evaluate()\n",
        "    print(f\"[User {user_id}] Final eval metrics: {eval_metrics}\")\n",
        "\n",
        "    # 9) Save the best checkpoint\n",
        "    trainer.save_model(user_output_dir)\n",
        "    print(f\"[User {user_id}] Done! Best model saved at: {user_output_dir}.\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Models for the Top 50 Users\n",
        "Now we iterate over the selected top 50 users and train a separate ModernBERT model for each. This step can be time-consuming depending on your hardware and the amount of data per user.\n",
        "\n",
        "> **Note**: The loop below is configured with `push_to_hub=True`, so if you set up your Hugging Face credentials and tokens in the environment, the models will be uploaded to your account or organization repository. You can set this to `False` if you only want to train locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# 5) Batch Training for Top Users\n",
        "\n",
        "OUTPUT_DIR_BASE = \"user_models\"  # Local directory for saving user-specific model checkpoints\n",
        "\n",
        "for user_id, count in top_50_users.items():\n",
        "    print(f\"\\n=== Training model for User {user_id} (Answers = {count}) ===\")\n",
        "    train_user_model(\n",
        "        df=df_original,\n",
        "        user_id=user_id,\n",
        "        model_name=\"answerdotai/ModernBERT-base\",\n",
        "        output_dir_base=OUTPUT_DIR_BASE,\n",
        "        push_to_hub=True,  # Set to False if you don't want to push to Hugging Face Hub\n",
        "        hub_token=os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\"),\n",
        "        num_train_epochs=10,\n",
        "        batch_size=4\n",
        "    )"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "1. **Performance Analysis**: Check the distribution of F1 scores across the 50 user models. Evaluate if there's a strong correlation between the number of training samples a user has and the final model performance.\n",
        "2. **Generalization**: Consider how well these models will generalize to new questions or topics not seen in the training set.\n",
        "3. **Difficulty Estimation**: If these models accurately simulate student responses, use them to compute question difficulty by querying each synthetic student.\n",
        "4. **Integration with Real Students**: Compare synthetic responses with real student performance to validate the realism of the simulated data.\n",
        "\n",
        "This completes the core demonstration of training separate ModernBERT models per user."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "mimetype": "text/x-python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
